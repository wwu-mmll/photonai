{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Getting Started 1. Installation You only need two things: Python 3 and your favourite Python IDE to get started. Then simply install via pip. 1 pip install photonai 2. Setup New Analysis Start by importing some utilities and creating a new Hyperpipe instance, naming the analysis and specifying where to save all outputs. 1 2 3 4 5 6 from sklearn.model_selection import ShuffleSplit , KFold from sklearn.datasets import load_breast_cancer from photonai.base import Hyperpipe , PipelineElement , Switch from photonai.optimization import IntegerRange , FloatRange pipe = Hyperpipe ( 'basic_pipe' , project_folder = './' ) 3. Define training, optimization and testing parameters Select parameters to customize the training, hyperparameter optimization and testing procedure. Particularly, you can choose the hyperparameter optimization strategy, set parameters, choose performance metrics and choose the performance metric to minimize or maximize, respectively. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 pipe = Hyperpipe ( 'basic_pipe' , project_folder = './' , # choose hyperparameter optimization strategy optimizer = 'random_grid_search' , # PHOTONAI automatically calculates your preferred metrics metrics = [ 'accuracy' , 'balanced_accuracy' , 'f1_score' ], # this metrics selects the best hyperparameter configuration # in this case mean squared error is minimized best_config_metric = 'f1_score' , # select cross validation strategies outer_cv = ShuffleSplit ( n_splits = 3 , test_size = 0.2 ), inner_cv = KFold ( n_splits = 10 )) 4. Build custom pipeline Select and arrange normalization, dimensionality reduction, feature selection, data augmentation, over- or undersampling algorithms in simple or parallel data streams. You can integrate custom algorithms or choose from our wide range of pre-registered algorithms from established toolboxes. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 pipe += PipelineElement ( 'StandardScaler' ) pipe += PipelineElement ( 'PCA' , hyperparameters = { 'n_components' : FloatRange ( 0.5 , 0.8 , step = 0.1 )}) pipe += PipelineElement ( 'ImbalancedDataTransformer' , hyperparameters = { 'method_name' : [ 'RandomUnderSampler' , 'RandomOverSampler' , 'SMOTE' ]}) or_element = Switch ( 'EstimatorSwitch' ) or_element += PipelineElement ( 'RandomForestClassifier' , hyperparameters = { 'min_samples_split' : IntegerRange ( 2 , 30 )}) or_element += PipelineElement ( 'SVC' , hyperparameters = { 'C' : FloatRange ( 0.5 , 10 ), 'kernel' : [ 'linear' , 'rbf' ]}) pipe += or_element 5. Load Data and Train Load your data and start the (nested-) cross-validated hyperparameter optimization, training and evaluation procedure. You will see an extensive output to monitor the hyperparameter optimization progress, see the results and track the best performances so far. 1 2 X , y = load_breast_cancer ( return_X_y = True ) pipe . fit ( X , y )","title":"Basic Usage"},{"location":"algorithms/algorithms_index/","text":"Algorithms PHOTONAI offers easy access to established machine learning algorithms. The algorithms can be imported by adding a PipelineElement with a specific name, such as \"SVC\" for importing the SupportVectorClassifier from scikit-learn , as shown in the following examples. You can set all parameters of the imported class as usual: e.g. add gamma='auto' to the PipelineElement to set the support vector machine's gamma parameter to 'auto'. In addition, you can specify each parameter as a hyperparameter and define a value range or value list to find the optimal value, such as 'kernel': ['linear', 'rbf'] . To build a custom pipeline, have a look at PHOTONAIs pre-registered processing- and learning algorithms . You can access algorithms for all purposes from several open-source packages. In addition, PHOTONAI offers several utility classes as well, such as linear statistical feature selection or sample pairing algorithms. In addition you can specify hyperparameters as well as their value range in order to be optimized by the hyperparameter optimization strategy. Currently, PHOTONAI offers Grid-Search , Random Search and two frameworks for bayesian optimization. PCA 1 2 3 4 5 6 from photonai.base import PipelineElement PipelineElement ( 'PCA' , hyperparameters = { 'n_components' : IntegerRange ( 5 , 20 )}, test_disabled = True ) # to test if disabling the PipelineElement improves performance, # simply add the test_disabled=True parameter SVC 1 2 3 4 PipelineElement ( 'SVC' , hyperparameters = { 'kernel' : Categorical ([ 'rbf' , 'poly' ]), 'C' : FloatRange ( 0.5 , 2 )}, gamma = 'auto' ) Keras Neural Net 1 2 3 4 5 6 7 8 PipelineElement ( 'KerasDnnRegressor' , hyperparameters = { 'hidden_layer_sizes' : Categorical ([[ 10 , 8 , 4 ], [ 20 , 5 , 3 ]]), 'dropout_rate' : Categorical ([[ 0.5 , 0.2 , 0.1 ], 0.1 ])}, activations = 'relu' , epochs = 5 , batch_size = 32 )","title":"Access pre-registered algorithms"},{"location":"algorithms/algorithms_index/#algorithms","text":"PHOTONAI offers easy access to established machine learning algorithms. The algorithms can be imported by adding a PipelineElement with a specific name, such as \"SVC\" for importing the SupportVectorClassifier from scikit-learn , as shown in the following examples. You can set all parameters of the imported class as usual: e.g. add gamma='auto' to the PipelineElement to set the support vector machine's gamma parameter to 'auto'. In addition, you can specify each parameter as a hyperparameter and define a value range or value list to find the optimal value, such as 'kernel': ['linear', 'rbf'] . To build a custom pipeline, have a look at PHOTONAIs pre-registered processing- and learning algorithms . You can access algorithms for all purposes from several open-source packages. In addition, PHOTONAI offers several utility classes as well, such as linear statistical feature selection or sample pairing algorithms. In addition you can specify hyperparameters as well as their value range in order to be optimized by the hyperparameter optimization strategy. Currently, PHOTONAI offers Grid-Search , Random Search and two frameworks for bayesian optimization.","title":"Algorithms"},{"location":"algorithms/algorithms_index/#pca","text":"1 2 3 4 5 6 from photonai.base import PipelineElement PipelineElement ( 'PCA' , hyperparameters = { 'n_components' : IntegerRange ( 5 , 20 )}, test_disabled = True ) # to test if disabling the PipelineElement improves performance, # simply add the test_disabled=True parameter","title":"PCA"},{"location":"algorithms/algorithms_index/#svc","text":"1 2 3 4 PipelineElement ( 'SVC' , hyperparameters = { 'kernel' : Categorical ([ 'rbf' , 'poly' ]), 'C' : FloatRange ( 0.5 , 2 )}, gamma = 'auto' )","title":"SVC"},{"location":"algorithms/algorithms_index/#keras-neural-net","text":"1 2 3 4 5 6 7 8 PipelineElement ( 'KerasDnnRegressor' , hyperparameters = { 'hidden_layer_sizes' : Categorical ([[ 10 , 8 , 4 ], [ 20 , 5 , 3 ]]), 'dropout_rate' : Categorical ([[ 0.5 , 0.2 , 0.1 ], 0.1 ])}, activations = 'relu' , epochs = 5 , batch_size = 32 )","title":"Keras Neural Net"},{"location":"algorithms/estimators/","text":"Estimator All Classification Regression Linear Estimators Name Class Package ARDRegression sklearn.linear_model.ARDRegression scikit-learn BayesianRidge sklearn.linear_model.BayesianRidge scikit-learn ElasticNet sklearn.linear_model.ElasticNet scikit-learn HuberRegressor sklearn.linear_model.HuberRegressor scikit-learn Lars sklearn.linear_model.Lars scikit-learn Lasso sklearn.linear_model.Lasso scikit-learn LassoLars sklearn.linear_model.LassoLars scikit-learn LinearRegression sklearn.linear_model.LinearRegression scikit-learn LogisticRegression sklearn.linear_model.LogisticRegression scikit-learn PassiveAggressiveClassifier sklearn.linear_model.PassiveAggressiveClassifier scikit-learn Perceptron sklearn.linear_model.Perceptron scikit-learn RANSACRegressor sklearn.linear_model.RANSACRegressor scikit-learn Ridge sklearn.linear_model.Ridge scikit-learn RidgeClassifier sklearn.linear_model.RidgeClassifier scikit-learn SGDClassifier sklearn.linear_model.SGDClassifier scikit-learn SGDRegressor sklearn.linear_model.SGDRegressor scikit-learn TheilSenRegressor sklearn.linear_model.TheilSenRegressor scikit-learn Tree-based Name Class Package ExtraTreesClassifier sklearn.ensemble.ExtraTreesClassifier scikit-learn ExtraTreesRegressor sklearn.ensemble.ExtraTreesRegressor scikit-learn DecisionTreeClassifier sklearn.tree.DecisionTreeClassifier scikit-learn DecisionTreeRegressor sklearn.tree.DecisionTreeRegressor scikit-learn RandomForestClassifier sklearn.ensemble.RandomForestClassifier scikit-learn RandomForestRegressor sklearn.ensemble.RandomForestRegressor scikit-learn Supported Vector Machines Name Class Package LinearSVC sklearn.svm.LinearSVC scikit-learn LinearSVR sklearn.svm.LinearSVR scikit-learn NuSVC sklearn.svm.NuSVC scikit-learn NuSVR sklearn.svm.NuSVR scikit-learn OneClassSVM sklearn.svm.OneClassSVM scikit-learn PhotonOneClassSVM photonai.modelwrapper.PhotonOneClassSVM.PhotonOneClassSVM scikit-learn / PHOTONAI SVC sklearn.svm.SVC scikit-learn SVR sklearn.svm.SVR scikit-learn Neural Networks Name Class Package BernoulliRBM sklearn.neural_network.BernoulliRBM scikit-learn KerasDnnClassifier photonai.modelwrapper.keras_dnn_classifier.KerasDnnClassifier keras / PHOTONAI KerasDnnRegressor photonai.modelwrapper.keras_dnn_regressor.KerasDnnRegressor keras / PHOTONAI MLPClassifier sklearn.neural_network.MLPClassifier scikit-learn MLPRegressor sklearn.neural_network.MLPRegressor scikit-learn PhotonMLPClassifier photonai.modelwrapper.PhotonMLPClassifier.PhotonMLPClassifier scikit-learn / PHOTONAI Ensemble Name Class Package AdaBoostClassifier sklearn.ensemble.AdaBoostClassifier scikit-learn AdaBoostRegressor sklearn.ensemble.AdaBoostRegressor scikit-learn BaggingClassifier sklearn.ensemble.BaggingClassifier scikit-learn BaggingRegressor sklearn.ensemble.BaggingRegressor scikit-learn GradientBoostingClassifier sklearn.ensemble.GradientBoostingClassifier scikit-learn GradientBoostingRegressor sklearn.ensemble.GradientBoostingRegressor scikit-learn Neighour-Based Name Class Package KNeighborsClassifier sklearn.neighbors.KNeighborsClassifier scikit-learn KNeighborsRegressor sklearn.neighbors.KNeighborsRegressor scikit-learn NearestCentroid sklearn.neighbors.NearestCentroid scikit-learn RadiusNeighborsClassifier sklearn.neighbors.RadiusNeighborsClassifier scikit-learn RadiusNeighborsRegressor sklearn.neighbors.RadiusNeighborsRegressor scikit-learn Probabilistic Name Class Package BayesianGaussianMixture sklearn.mixture.BayesianGaussianMixture scikit-learn BernoulliNB sklearn.naive_bayes.BernoulliNB scikit-learn GaussianNB sklearn.naive_bayes.GaussianNB scikit-learn MultinomialNB sklearn.naive_bayes.MultinomialNB scikit-learn GaussianMixture sklearn.mixture.GaussianMixture scikit-learn GaussianProcessClassifier sklearn.gaussian_process.GaussianProcessClassifier scikit-learn GaussianProcessRegressor sklearn.gaussian_process.GaussianProcessRegressor scikit-learn Other Name Class Package DummyClassifier sklearn.dummy.DummyClassifier scikit-learn DummyRegressor sklearn.dummy.DummyRegressor scikit-learn KernelRidge sklearn.kernel_ridge.KernelRidge scikit-learn PhotonVotingClassifier photonai.modelwrapper.Voting.PhotonVotingClassifier PHOTONAI PhotonVotingRegressor photonai.modelwrapper.Voting.PhotonVotingRegressor PHOTONAI function myFunction(filter) { var input, table, tr, td, i, txtValue; if (filter == 'class') { document.getElementById(\"button_all\").classList.remove('md-button--primary'); document.getElementById(\"button_class\").classList.add('md-button--primary'); document.getElementById(\"button_reg\").classList.remove('md-button--primary'); } else if (filter == 'reg') { document.getElementById(\"button_all\").classList.remove('md-button--primary'); document.getElementById(\"button_class\").classList.remove('md-button--primary'); document.getElementById(\"button_reg\").classList.add('md-button--primary'); } else { document.getElementById(\"button_all\").classList.add('md-button--primary'); document.getElementById(\"button_class\").classList.remove('md-button--primary'); document.getElementById(\"button_reg\").classList.remove('md-button--primary'); } alltables = document.querySelectorAll(\"table[data-name=filterTable]\"); alltables.forEach(function(table){ tr = table.getElementsByTagName(\"tr\"); // Loop through all table rows, and hide those who don't match the search query for (i = 0; i < tr.length; i++) { txtValue = tr[i].getAttribute('ml-type'); if (txtValue) { if (txtValue.indexOf(filter) > -1) { tr[i].style.display = \"\"; } else { tr[i].style.display = \"none\"; } } } }); }","title":"Estimators"},{"location":"algorithms/hpos/","text":"Hyperparameter Optimization PHOTONAI offers easy access to several established hyperparameter optimization strategies. Grid Search An exhaustive searching through a manually specified subset of the hyperparameter space. The grid is defined by a finite list for each hyperparameter. 1 2 pipe = Hyperpipe ( \"...\" , optimizer = 'grid_search' ) Random Grid Search Random sampling of a manually specified subset of the hyperparameter space. The grid is defined by a finite list for each hyperparameter. Then, a specified number of random configurations from this grid is tested 1 2 3 4 pipe = Hyperpipe ( \"...\" , optimizer = 'random_grid_search' , optimizer_params = { 'n_configurations' : 30 , 'limit_in_minutes' : 10 }) Random Search A grid-free selection of configurations based on the hyperparameter space. In the case of numerical parameters, decisions are made only on the basis of the interval limits. The creation of configurations is limited by time or a maximum number of runs. 1 2 3 4 pipe = Hyperpipe ( \"...\" , optimizer = 'random_search' , optimizer_params = { 'n_configurations' : 30 , 'limit_in_minutes' : 20 }) If the optimizer_params contain a time and numerical limit, both limits are considered by aborting if either of the limits is met. The default limit for Random Search is n_configurations=10 . Scikit-Optimize Scikit-Optimize, or skopt, is a simple and efficient library to minimize (very) expensive and noisy black-box functions. It implements several methods for sequential model-based optimization. skopt aims to be accessible and easy to use in many contexts. Scikit-optimize usage and implementation details available here . A detailed parameter documentation here. 1 2 3 4 5 6 7 pipe = Hyperpipe ( \"...\" , optimizer = 'sk_opt' , optimizer_params = { 'n_configurations' : 55 , 'n_initial_points' : 15 , 'initial_point_generator' : \"sobol\" , 'acq_func' : 'LCB' , 'acq_func_kwargs' : { 'kappa' : 1.96 }}) Nevergrad Nevergrad is a gradient-free optimization platform. Thus, this package is suitable for optimizing over the hyperparamter space. As a great advantage, evolutionary algorithms are implemented here in addition to Bayesian techniques. Nevergrad usage and implementation details available here . 1 2 3 4 5 6 import nevergrad as ng # list of all available nevergrad optimizer print ( list ( ng . optimizers . registry . values ())) my_pipe = Hyperpipe ( \"...\" , optimizer = 'nevergrad' , optimizer_params = { 'facade' : 'NGO' , 'n_configurations' : 30 }) Smac SMAC (sequential model-based algorithm configuration) is a versatile tool for optimizing algorithm parameters. The main core consists of Bayesian Optimization in combination with an aggressive racing mechanism to efficiently decide which of two configurations performs better. SMAC usage and implementation details available here . 1 2 3 4 5 6 my_pipe = Hyperpipe ( \"...\" , optimizer = 'smac' , optimizer_params = { \"facade\" : \"SMAC4BO\" , \"wallclock_limit\" : 60.0 * 10 , # seconds \"ta_run_limit\" : 100 } # limit of configurations ) Switch Optimizer This optimizer is special, as it uses the strategies above to optimizes the same dataflow for different learning algorithms in a switch (\"OR\") element at the end of the pipeline. For example you can use bayesian optimization for each learning algorithm and select that each of the algorithms gets 25 configurations to be tested. This is different to a global optimization, in which, after an initial exploration phase, computational resources are dedicated to the best performing learning algorithm only. By equally distributing computational ressources to each learning algorithms, better comparability is achieved in-between the algorithms. This can according to the use case be desirable. 1 2 3 pipe = Hyperpipe ( \"...\" , optimizer = \"switch\" , optimizer_params = { 'name' : 'sk_opt' , 'n_configurations' : 25 })","title":"Hyperparameter Optimizers"},{"location":"algorithms/registry/","text":"Registry The PHOTONAI Registry class lets you register your class with a key, so that you can access it conveniently in your PHOTONAI Hyperpipe . setup via the PipelineElement class . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 import os from sklearn.datasets import load_breast_cancer from sklearn.model_selection import KFold from photonai.base import Hyperpipe , PipelineElement , PhotonRegistry from photonai.optimization import IntegerRange # REGISTER ELEMENT base_folder = os . path . dirname ( os . path . abspath ( __file__ )) custom_elements_folder = os . path . join ( base_folder , '../advanced/custom_elements' ) registry = PhotonRegistry ( custom_elements_folder = custom_elements_folder ) registry . register ( photon_name = 'MyCustomEstimator' , class_str = 'custom_estimator.CustomEstimator' , element_type = 'Estimator' ) registry . register ( photon_name = 'MyCustomTransformer' , class_str = 'custom_transformer.CustomTransformer' , element_type = 'Transformer' ) registry . activate () # WE USE THE BREAST CANCER SET FROM SKLEARN X , y = load_breast_cancer ( return_X_y = True ) # DESIGN YOUR PIPELINE my_pipe = Hyperpipe ( 'custom_estimator_pipe' , optimizer = 'random_grid_search' , optimizer_params = { 'n_configurations' : 2 }, metrics = [ 'accuracy' , 'precision' , 'recall' , 'balanced_accuracy' ], best_config_metric = 'accuracy' , outer_cv = KFold ( n_splits = 3 ), inner_cv = KFold ( n_splits = 3 ), verbosity = 1 , project_folder = './tmp/' ) # SHOW WHAT IS POSSIBLE IN THE CONSOLE registry . list_available_elements () # NOW FIND OUT MORE ABOUT A SPECIFIC ELEMENT registry . info ( 'MyCustomEstimator' ) registry . info ( 'MyCustomTransformer' ) my_pipe . add ( PipelineElement ( 'StandardScaler' )) my_pipe += PipelineElement ( 'PCA' , hyperparameters = { 'n_components' : IntegerRange ( 5 , 20 )}, test_disabled = True ) my_pipe += PipelineElement ( 'MyCustomEstimator' ) # NOW TRAIN YOUR PIPELINE my_pipe . fit ( X , y )","title":"Registry"},{"location":"algorithms/transformers/","text":"Transformer Decomposition Name Class Package CCA sklearn.cross_decomposition.CCA scikit-learn DictionaryLearning sklearn.decomposition.DictionaryLearning scikit-learn dict_learning sklearn.decomposition.dict_learning scikit-learn dict_learning_online sklearn.decomposition.dict_learning_online scikit-learn FactorAnalysis sklearn.decomposition.FactorAnalysis scikit-learn FastICA sklearn.decomposition.FastICA scikit-learn IncrementalPCA sklearn.decomposition.IncrementalPCA scikit-learn KernelPCA sklearn.decomposition.KernelPCA scikit-learn LatentDirichletAllocation sklearn.decomposition.LatentDirichletAllocation scikit-learn MiniBatchDictionaryLearning sklearn.decomposition.MiniBatchDictionaryLearning scikit-learn MiniBatchSparsePCA sklearn.decomposition.MiniBatchSparsePCA scikit-learn NMF sklearn.decomposition.NMF scikit-learn PCA sklearn.decomposition.PCA scikit-learn PLSCanonical sklearn.cross_decomposition.PLSCanonical scikit-learn PLSRegression sklearn.cross_decomposition.PLSRegression scikit-learn PLSSVD sklearn.cross_decomposition.PLSSVD scikit-learn SparsePCA sklearn.decomposition.SparsePCA scikit-learn SparseCoder sklearn.decomposition.SparseCoder scikit-learn TruncatedSVD sklearn.decomposition.TruncatedSVD scikit-learn sparse_encode sklearn.decomposition.sparse_encode scikit-learn Feature Selection Name Class Package FClassifSelectPercentile photonai.modelwrapper.FeatureSelection.FClassifSelectPercentile PHOTONAI FRegressionFilterPValue photonai.modelwrapper.FeatureSelection.FRegressionFilterPValue PHOTONAI FRegressionSelectPercentile photonai.modelwrapper.FeatureSelection.FRegressionSelectPercentile PHOTONAI GenericUnivariateSelect sklearn.feature_selection.GenericUnivariateSelect scikit-learn LassoFeatureSelection photonai.modelwrapper.FeatureSelection.LassoFeatureSelection PHOTONAI RFE sklearn.feature_selection.RFE scikit-learn RFECV sklearn.feature_selection.RFECV scikit-learn SelectPercentile sklearn.feature_selection.SelectPercentile scikit-learn SelectKBest sklearn.feature_selection.SelectKBest scikit-learn SelectFpr sklearn.feature_selection.SelectFpr scikit-learn SelectFdr sklearn.feature_selection.SelectFdr scikit-learn SelectFromModel sklearn.feature_selection.SelectFromModel scikit-learn SelectFwe sklearn.feature_selection.SelectFwe scikit-learn VarianceThreshold sklearn.feature_selection.VarianceThreshold scikit-learn Preprocessing Name Class Package Binarizer sklearn.preprocessing.Binarizer scikit-learn FeatureEncoder photonai.modelwrapper.OrdinalEncoder.FeatureEncoder PHOTON FunctionTransformer sklearn.preprocessing.FunctionTransformer scikit-learn KernelCenterer sklearn.preprocessing.KernelCenterer scikit-learn LabelEncoder sklearn.preprocessing.LabelEncoder scikit-learn MaxAbsScaler sklearn.preprocessing.MaxAbsScaler scikit-learn MinMaxScaler sklearn.preprocessing.MinMaxScaler scikit-learn Normalizer sklearn.preprocessing.Normalizer scikit-learn PolynomialFeatures sklearn.preprocessing.PolynomialFeatures scikit-learn QuantileTransformer sklearn.preprocessing.QuantileTransformer scikit-learn RobustScaler sklearn.preprocessing.RobustScaler scikit-learn SimpleImputer sklearn.impute.SimpleImputer scikit-learn StandardScaler sklearn.preprocessing.StandardScaler scikit-learn SourceSplitter photonai.modelwrapper.source_splitter.SourceSplitter PHOTONAI Other Name Class Package ConfounderRemoval photonai.modelwrapper.ConfounderRemoval.ConfounderRemoval PHOTONAI ImbalancedDataTransformer photonai.modelwrapper.imbalanced_data_transformer.ImbalancedDataTransformer imbalanced-learn / PHOTONAI SamplePairingClassification photonai.modelwrapper.SamplePairing.SamplePairingClassification PHOTONAI SamplePairingRegression photonai.modelwrapper.SamplePairing.SamplePairingRegression PHOTONAI","title":"Transformers"},{"location":"api/architecture/","text":"Package Structure The photonai source code is divided in the following folders base : Here reside photonai's core elements such as the Hyperpipe, the pipeline, the pipeline element and all other photonai pipeline specialities. helper : not much to say here modelwrapper : All algorithms shipped with PHOTONAI and wrappers for accessing non-scikit-learn conform algorithms are stored here. optimization : Everything around Hyperparameter Optimization. photonlogger : Special logging logic to make everything as informative and pretty as possible. Also to avoid naming conflicts with loggers from other packages. processing : Here reside all classes that do the actual computing","title":"Overview"},{"location":"api/architecture/#package-structure","text":"The photonai source code is divided in the following folders base : Here reside photonai's core elements such as the Hyperpipe, the pipeline, the pipeline element and all other photonai pipeline specialities. helper : not much to say here modelwrapper : All algorithms shipped with PHOTONAI and wrappers for accessing non-scikit-learn conform algorithms are stored here. optimization : Everything around Hyperparameter Optimization. photonlogger : Special logging logic to make everything as informative and pretty as possible. Also to avoid naming conflicts with loggers from other packages. processing : Here reside all classes that do the actual computing","title":"Package Structure"},{"location":"api/custom_estimator/","text":"Custom Estimator You can combine your own learning algorithm, bet it a neural net or anything else, by simply adhering to the scikit-learn interface as shown below. Then register your class with the Register module and you're done! 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import numpy as np from sklearn.base import BaseEstimator , ClassifierMixin class CustomEstimator ( BaseEstimator , ClassifierMixin ): def __init__ ( self , param1 = 0 , param2 = None ): # it is important that you name your params the same in the constructor # stub as well as in your class variables! self . param1 = param1 self . param2 = param2 def fit ( self , X , y = None , ** kwargs ): \"\"\" Adjust the underlying model or method to the data. Returns ------- IMPORTANT: must return self! \"\"\" return self def predict ( self , X ): \"\"\" Use the learned model to make predictions. \"\"\" return np . random . randint ( 0 , 2 , X . shape [ 0 ])","title":"Custom Estimator"},{"location":"api/custom_transformer/","text":"Custom Transformer You can add your own method, be it preprocessing, feature selection or dimensionality reduction, by simply adhering to the scikit-learn interface as shown below. Then register your class with the Register module and you're good to go. You can then combine it with any optimizer and metric and design your custom pipeline layout. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # we use BaseEstimator as to prepare the transformer for hyperparameter optimization # we inherit the get_params and set_params methods from sklearn.base import BaseEstimator class CustomTransformer ( BaseEstimator ): def __init__ ( self , param1 = 0 , param2 = None ): # it is important that you name your params the same in the constructor # stub as well as in your class variables! self . param1 = param1 self . param2 = param2 def fit ( self , data , targets = None , ** kwargs ): \"\"\" Adjust the underlying model or method to the data. Returns ------- IMPORTANT: must return self! \"\"\" return self def transform ( self , data , targets = None , ** kwargs ): \"\"\" Apply the method's logic to the data. \"\"\" return data","title":"Custom Transformer"},{"location":"api/hyperpipe/","text":"Configuration class that specifies the format in which the results are saved. Results can be saved to a MongoDB or a simple son-file. You can also choose whether to save predictions and/or feature importances. Source code in photonai/base/hyperpipe.py class OutputSettings : \"\"\" Configuration class that specifies the format in which the results are saved. Results can be saved to a MongoDB or a simple son-file. You can also choose whether to save predictions and/or feature importances. \"\"\" def __init__ ( self , mongodb_connect_url : str = None , save_output : bool = True , overwrite_results : bool = False , generate_best_model : bool = True , user_id : str = '' , wizard_object_id : str = '' , wizard_project_name : str = '' , project_folder : str = '' ): \"\"\" Initialize the object. Parameters: mongodb_connect_url: Valid mongodb connection url that specifies a database for storing the results. save_output: Controls the general saving of the results. overwrite_results: Allows overwriting the results folder if it already exists. generate_best_model: Determines whether an optimum_pipe should be created and fitted. If False, no dependent files are created. user_id: The user name of the according PHOTONAI Wizard login. wizard_object_id: The object id to map the designed pipeline in the PHOTONAI Wizard to the results in the PHOTONAI CORE Database. wizard_project_name: How the project is titled in the PHOTONAI Wizard. project_folder: Deprecated Parameter - transferred to Hyperpipe. \"\"\" if project_folder : msg = \"Deprecated: The parameter 'project_folder' was moved to the Hyperpipe. \" \\ \"Please use Hyperpipe(..., project_folder='').\" logger . error ( msg ) raise DeprecationWarning ( msg ) self . mongodb_connect_url = mongodb_connect_url self . overwrite_results = overwrite_results self . user_id = user_id self . wizard_object_id = wizard_object_id self . wizard_project_name = wizard_project_name self . generate_best_model = generate_best_model self . save_output = save_output self . save_predictions_from_best_config_inner_folds = None self . verbosity = 0 self . results_folder = '' self . project_folder = '' self . log_file = '' self . logging_file_handler = None # this is only allowed from hyperpipe def set_project_folder ( self , project_folder ): self . project_folder = project_folder self . initialize_log_file () @property def setup_error_file ( self ): if self . project_folder : return os . path . join ( self . project_folder , 'photon_setup_errors.log' ) else : return \"\" def initialize_log_file ( self ): self . log_file = self . setup_error_file def update_settings ( self , name , timestamp ): if self . save_output : if not os . path . exists ( self . project_folder ): os . makedirs ( self . project_folder ) # Todo: give rights to user if this is done by docker container if self . overwrite_results : self . results_folder = os . path . join ( self . project_folder , name + '_results' ) else : self . results_folder = os . path . join ( self . project_folder , name + '_results_' + timestamp ) logger . info ( \"Output Folder: \" + self . results_folder ) if not os . path . exists ( self . results_folder ): os . makedirs ( self . results_folder ) if os . path . basename ( self . log_file ) == \"photon_setup_errors.log\" : self . log_file = 'photon_output.log' self . log_file = self . _add_timestamp ( self . log_file ) self . set_log_file () # if we made it here, there should be no further setup errors, every error that comes # now can go to the standard logger instance if os . path . isfile ( self . setup_error_file ): os . remove ( self . setup_error_file ) def _add_timestamp ( self , file ): return os . path . join ( self . results_folder , os . path . basename ( file )) def _get_log_level ( self ): if self . verbosity == 0 : level = 25 elif self . verbosity == 1 : level = logging . INFO # 20 elif self . verbosity == 2 : level = logging . DEBUG # 10 else : level = logging . WARN # 30 return level def set_log_file ( self ): logfile_directory = os . path . dirname ( self . log_file ) if not os . path . exists ( logfile_directory ): os . makedirs ( logfile_directory ) if self . logging_file_handler is None : self . logging_file_handler = logging . FileHandler ( self . log_file ) self . logging_file_handler . setLevel ( self . _get_log_level ()) logger . addHandler ( self . logging_file_handler ) else : self . logging_file_handler . close () self . logging_file_handler . baseFilename = self . log_file def set_log_level ( self ): verbose_num = self . _get_log_level () logger . setLevel ( verbose_num ) for handler in logger . handlers : handler . setLevel ( verbose_num ) __init__ ( self , mongodb_connect_url = None , save_output = True , overwrite_results = False , generate_best_model = True , user_id = '' , wizard_object_id = '' , wizard_project_name = '' , project_folder = '' ) special Initialize the object. Parameters: Name Type Description Default mongodb_connect_url str Valid mongodb connection url that specifies a database for storing the results. None save_output bool Controls the general saving of the results. True overwrite_results bool Allows overwriting the results folder if it already exists. False generate_best_model bool Determines whether an optimum_pipe should be created and fitted. If False, no dependent files are created. True user_id str The user name of the according PHOTONAI Wizard login. '' wizard_object_id str The object id to map the designed pipeline in the PHOTONAI Wizard to the results in the PHOTONAI CORE Database. '' wizard_project_name str How the project is titled in the PHOTONAI Wizard. '' project_folder str Deprecated Parameter - transferred to Hyperpipe. '' Source code in photonai/base/hyperpipe.py def __init__ ( self , mongodb_connect_url : str = None , save_output : bool = True , overwrite_results : bool = False , generate_best_model : bool = True , user_id : str = '' , wizard_object_id : str = '' , wizard_project_name : str = '' , project_folder : str = '' ): \"\"\" Initialize the object. Parameters: mongodb_connect_url: Valid mongodb connection url that specifies a database for storing the results. save_output: Controls the general saving of the results. overwrite_results: Allows overwriting the results folder if it already exists. generate_best_model: Determines whether an optimum_pipe should be created and fitted. If False, no dependent files are created. user_id: The user name of the according PHOTONAI Wizard login. wizard_object_id: The object id to map the designed pipeline in the PHOTONAI Wizard to the results in the PHOTONAI CORE Database. wizard_project_name: How the project is titled in the PHOTONAI Wizard. project_folder: Deprecated Parameter - transferred to Hyperpipe. \"\"\" if project_folder : msg = \"Deprecated: The parameter 'project_folder' was moved to the Hyperpipe. \" \\ \"Please use Hyperpipe(..., project_folder='').\" logger . error ( msg ) raise DeprecationWarning ( msg ) self . mongodb_connect_url = mongodb_connect_url self . overwrite_results = overwrite_results self . user_id = user_id self . wizard_object_id = wizard_object_id self . wizard_project_name = wizard_project_name self . generate_best_model = generate_best_model self . save_output = save_output self . save_predictions_from_best_config_inner_folds = None self . verbosity = 0 self . results_folder = '' self . project_folder = '' self . log_file = '' self . logging_file_handler = None","title":"Hyperpipe"},{"location":"api/hyperpipe/#photonai.base.hyperpipe.OutputSettings.__init__","text":"Initialize the object. Parameters: Name Type Description Default mongodb_connect_url str Valid mongodb connection url that specifies a database for storing the results. None save_output bool Controls the general saving of the results. True overwrite_results bool Allows overwriting the results folder if it already exists. False generate_best_model bool Determines whether an optimum_pipe should be created and fitted. If False, no dependent files are created. True user_id str The user name of the according PHOTONAI Wizard login. '' wizard_object_id str The object id to map the designed pipeline in the PHOTONAI Wizard to the results in the PHOTONAI CORE Database. '' wizard_project_name str How the project is titled in the PHOTONAI Wizard. '' project_folder str Deprecated Parameter - transferred to Hyperpipe. '' Source code in photonai/base/hyperpipe.py def __init__ ( self , mongodb_connect_url : str = None , save_output : bool = True , overwrite_results : bool = False , generate_best_model : bool = True , user_id : str = '' , wizard_object_id : str = '' , wizard_project_name : str = '' , project_folder : str = '' ): \"\"\" Initialize the object. Parameters: mongodb_connect_url: Valid mongodb connection url that specifies a database for storing the results. save_output: Controls the general saving of the results. overwrite_results: Allows overwriting the results folder if it already exists. generate_best_model: Determines whether an optimum_pipe should be created and fitted. If False, no dependent files are created. user_id: The user name of the according PHOTONAI Wizard login. wizard_object_id: The object id to map the designed pipeline in the PHOTONAI Wizard to the results in the PHOTONAI CORE Database. wizard_project_name: How the project is titled in the PHOTONAI Wizard. project_folder: Deprecated Parameter - transferred to Hyperpipe. \"\"\" if project_folder : msg = \"Deprecated: The parameter 'project_folder' was moved to the Hyperpipe. \" \\ \"Please use Hyperpipe(..., project_folder='').\" logger . error ( msg ) raise DeprecationWarning ( msg ) self . mongodb_connect_url = mongodb_connect_url self . overwrite_results = overwrite_results self . user_id = user_id self . wizard_object_id = wizard_object_id self . wizard_project_name = wizard_project_name self . generate_best_model = generate_best_model self . save_output = save_output self . save_predictions_from_best_config_inner_folds = None self . verbosity = 0 self . results_folder = '' self . project_folder = '' self . log_file = '' self . logging_file_handler = None","title":"__init__()"},{"location":"api/base/branch/","text":"Documentation for Branch A substream of pipeline elements that is encapsulated, e.g. for parallelization. Examples: 1 2 3 4 5 6 7 8 from photonai.base import Branch from photonai.optimization import IntegerRange tree_qua_branch = Branch ( 'tree_branch' ) tree_qua_branch += PipelineElement ( 'QuantileTransformer' , n_quantiles = 100 ) tree_qua_branch += PipelineElement ( 'DecisionTreeClassifier' , { 'min_samples_split' : IntegerRange ( 2 , 4 )}, criterion = 'gini' ) Source code in photonai/base/photon_elements.py class Branch ( PipelineElement ): \"\"\" A substream of pipeline elements that is encapsulated, e.g. for parallelization. Example: ``` python from photonai.base import Branch from photonai.optimization import IntegerRange tree_qua_branch = Branch('tree_branch') tree_qua_branch += PipelineElement('QuantileTransformer', n_quantiles=100) tree_qua_branch += PipelineElement('DecisionTreeClassifier', {'min_samples_split': IntegerRange(2, 4)}, criterion='gini') ``` \"\"\" def __init__ ( self , name : str , elements : List [ PipelineElement ] = None ): \"\"\" Initialize the object. Parameters: name: Name of the encapsulated item and/or summary of the encapsulated element`s functions. elements: List of PipelineElements added one after another to the Branch. \"\"\" super () . __init__ ( name , {}, test_disabled = False , disabled = False , base_element = True ) # in case any of the children needs y or covariates we need to request them self . needs_y = True self . needs_covariates = True self . elements = [] self . has_hyperparameters = True self . skip_caching = True self . identifier = \"BRANCH:\" # needed for caching on individual level self . fix_fold_id = False self . do_not_delete_cache_folder = False # add elements if elements : for element in elements : self . add ( element ) def fit ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ): \"\"\" Calls the fit function on all underlying base elements. Parameters: X: The array-like input with shape=[N, D], where N is the number of samples and D is the number of features. y: The truth array-like values with shape=[N], where N is the number of samples. **kwargs: Keyword arguments, passed to base_elements fit. Returns: Fitted self. \"\"\" self . base_element = Branch . sanity_check_pipeline ( self . base_element ) return super () . fit ( X , y , ** kwargs ) def transform ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ) -> ( np . ndarray , np . ndarray , dict ): \"\"\" Calls the transform function on all underlying base elements. If _estimator_type is in ['classifier', 'regressor'], predict is called instead. Parameters: X: The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. y: The truth array-like values with shape=[N], where N is the number of samples. **kwargs: Keyword arguments, passed to base_elements predict/transform. Returns: Transformed/Predicted data. \"\"\" if self . _estimator_type == 'classifier' or self . _estimator_type == 'regressor' : return super () . predict ( X ), y , kwargs return super () . transform ( X , y , ** kwargs ) def predict ( self , X : np . ndarray , ** kwargs ) -> np . ndarray : \"\"\" Calls the predict function on underlying base elements. Parameters: X: The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. **kwargs: Keyword arguments, passed to base_elements predict method. Returns: Prediction values. \"\"\" return super () . predict ( X , ** kwargs ) def __iadd__ ( self , pipe_element : PipelineElement ): \"\"\" Add an element to the sub pipeline. Parameters: pipe_element: The PipelineElement to add, being either a transformer or an estimator. \"\"\" super ( Branch , self ) . __iadd__ ( pipe_element ) self . _prepare_pipeline () return self def add ( self , pipe_element : PipelineElement ): \"\"\" Add an element to the sub pipeline. Parameters: pipe_element: The PipelineElement to add, being either a transformer or an estimator. \"\"\" self . __iadd__ ( pipe_element ) @staticmethod def prepare_photon_pipe ( elements ): pipeline_steps = list () for item in elements : pipeline_steps . append (( item . name , item )) return PhotonPipeline ( pipeline_steps ) @staticmethod def sanity_check_pipeline ( pipe ): if isinstance ( pipe . elements [ - 1 ][ 1 ], CallbackElement ): msg = \"Last element of pipeline cannot be callback element, would be mistaken for estimator. Removing it.\" logger . warning ( msg ) warnings . warn ( msg ) del pipe . elements [ - 1 ] return pipe def _prepare_pipeline ( self ): \"\"\" Generates sklearn pipeline with all underlying elements \"\"\" self . _hyperparameters = { item . name : item . hyperparameters for item in self . elements if hasattr ( item , 'hyperparameters' )} if self . has_hyperparameters : self . generate_sklearn_hyperparameters () new_pipe = Branch . prepare_photon_pipe ( self . elements ) new_pipe . _fix_fold_id = self . fix_fold_id new_pipe . _do_not_delete_cache_folder = self . do_not_delete_cache_folder self . base_element = new_pipe def copy_me ( self ): new_copy_of_me = self . __class__ ( self . name ) for item in self . elements : if hasattr ( item , 'copy_me' ): copy_item = item . copy_me () else : copy_item = deepcopy ( item ) new_copy_of_me += copy_item if self . current_config is not None : new_copy_of_me . set_params ( ** self . current_config ) new_copy_of_me . _random_state = self . _random_state return new_copy_of_me @property def hyperparameters ( self ): return self . _hyperparameters @hyperparameters . setter def hyperparameters ( self , value ): \"\"\" Setting hyperparameters does not make sense, only the items that added can be optimized, not the container (self). \"\"\" return @property def _estimator_type ( self ): return getattr ( self . elements [ - 1 ], '_estimator_type' ) def generate_config_grid ( self ): if self . has_hyperparameters : tmp_grid = create_global_config_grid ( self . elements , self . name ) return tmp_grid else : return [] def generate_sklearn_hyperparameters ( self ): \"\"\" Generates a dictionary according to the sklearn convention of element_name__parameter_name: parameter_value \"\"\" self . _hyperparameters = {} for element in self . elements : for attribute , value_list in element . hyperparameters . items (): self . _hyperparameters [ self . name + '__' + attribute ] = value_list def _check_hyper ( self , BaseEstimator ): pass @property def feature_importances_ ( self ): if hasattr ( self . elements [ - 1 ], 'feature_importances_' ): return getattr ( self . elements [ - 1 ], 'feature_importances_' ) __iadd__ ( self , pipe_element ) special Add an element to the sub pipeline. Parameters: Name Type Description Default pipe_element PipelineElement The PipelineElement to add, being either a transformer or an estimator. required Source code in photonai/base/photon_elements.py def __iadd__ ( self , pipe_element : PipelineElement ): \"\"\" Add an element to the sub pipeline. Parameters: pipe_element: The PipelineElement to add, being either a transformer or an estimator. \"\"\" super ( Branch , self ) . __iadd__ ( pipe_element ) self . _prepare_pipeline () return self __init__ ( self , name , elements = None ) special Initialize the object. Parameters: Name Type Description Default name str Name of the encapsulated item and/or summary of the encapsulated element`s functions. required elements List[photonai.base.photon_elements.PipelineElement] List of PipelineElements added one after another to the Branch. None Source code in photonai/base/photon_elements.py def __init__ ( self , name : str , elements : List [ PipelineElement ] = None ): \"\"\" Initialize the object. Parameters: name: Name of the encapsulated item and/or summary of the encapsulated element`s functions. elements: List of PipelineElements added one after another to the Branch. \"\"\" super () . __init__ ( name , {}, test_disabled = False , disabled = False , base_element = True ) # in case any of the children needs y or covariates we need to request them self . needs_y = True self . needs_covariates = True self . elements = [] self . has_hyperparameters = True self . skip_caching = True self . identifier = \"BRANCH:\" # needed for caching on individual level self . fix_fold_id = False self . do_not_delete_cache_folder = False # add elements if elements : for element in elements : self . add ( element ) add ( self , pipe_element ) Add an element to the sub pipeline. Parameters: Name Type Description Default pipe_element PipelineElement The PipelineElement to add, being either a transformer or an estimator. required Source code in photonai/base/photon_elements.py def add ( self , pipe_element : PipelineElement ): \"\"\" Add an element to the sub pipeline. Parameters: pipe_element: The PipelineElement to add, being either a transformer or an estimator. \"\"\" self . __iadd__ ( pipe_element ) fit ( self , X , y = None , ** kwargs ) Calls the fit function on all underlying base elements. Parameters: Name Type Description Default X ndarray The array-like input with shape=[N, D], where N is the number of samples and D is the number of features. required y ndarray The truth array-like values with shape=[N], where N is the number of samples. None **kwargs Keyword arguments, passed to base_elements fit. {} Returns: Type Description Fitted self. Source code in photonai/base/photon_elements.py def fit ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ): \"\"\" Calls the fit function on all underlying base elements. Parameters: X: The array-like input with shape=[N, D], where N is the number of samples and D is the number of features. y: The truth array-like values with shape=[N], where N is the number of samples. **kwargs: Keyword arguments, passed to base_elements fit. Returns: Fitted self. \"\"\" self . base_element = Branch . sanity_check_pipeline ( self . base_element ) return super () . fit ( X , y , ** kwargs ) predict ( self , X , ** kwargs ) Calls the predict function on underlying base elements. Parameters: Name Type Description Default X ndarray The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. required **kwargs Keyword arguments, passed to base_elements predict method. {} Returns: Type Description ndarray Prediction values. Source code in photonai/base/photon_elements.py def predict ( self , X : np . ndarray , ** kwargs ) -> np . ndarray : \"\"\" Calls the predict function on underlying base elements. Parameters: X: The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. **kwargs: Keyword arguments, passed to base_elements predict method. Returns: Prediction values. \"\"\" return super () . predict ( X , ** kwargs ) transform ( self , X , y = None , ** kwargs ) Calls the transform function on all underlying base elements. If _estimator_type is in ['classifier', 'regressor'], predict is called instead. Parameters: Name Type Description Default X ndarray The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. required y ndarray The truth array-like values with shape=[N], where N is the number of samples. None **kwargs Keyword arguments, passed to base_elements predict/transform. {} Returns: Type Description (<class 'numpy.ndarray'>, <class 'numpy.ndarray'>, <class 'dict'>) Transformed/Predicted data. Source code in photonai/base/photon_elements.py def transform ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ) -> ( np . ndarray , np . ndarray , dict ): \"\"\" Calls the transform function on all underlying base elements. If _estimator_type is in ['classifier', 'regressor'], predict is called instead. Parameters: X: The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. y: The truth array-like values with shape=[N], where N is the number of samples. **kwargs: Keyword arguments, passed to base_elements predict/transform. Returns: Transformed/Predicted data. \"\"\" if self . _estimator_type == 'classifier' or self . _estimator_type == 'regressor' : return super () . predict ( X ), y , kwargs return super () . transform ( X , y , ** kwargs )","title":"Branch"},{"location":"api/base/branch/#documentation-for-branch","text":"A substream of pipeline elements that is encapsulated, e.g. for parallelization. Examples: 1 2 3 4 5 6 7 8 from photonai.base import Branch from photonai.optimization import IntegerRange tree_qua_branch = Branch ( 'tree_branch' ) tree_qua_branch += PipelineElement ( 'QuantileTransformer' , n_quantiles = 100 ) tree_qua_branch += PipelineElement ( 'DecisionTreeClassifier' , { 'min_samples_split' : IntegerRange ( 2 , 4 )}, criterion = 'gini' ) Source code in photonai/base/photon_elements.py class Branch ( PipelineElement ): \"\"\" A substream of pipeline elements that is encapsulated, e.g. for parallelization. Example: ``` python from photonai.base import Branch from photonai.optimization import IntegerRange tree_qua_branch = Branch('tree_branch') tree_qua_branch += PipelineElement('QuantileTransformer', n_quantiles=100) tree_qua_branch += PipelineElement('DecisionTreeClassifier', {'min_samples_split': IntegerRange(2, 4)}, criterion='gini') ``` \"\"\" def __init__ ( self , name : str , elements : List [ PipelineElement ] = None ): \"\"\" Initialize the object. Parameters: name: Name of the encapsulated item and/or summary of the encapsulated element`s functions. elements: List of PipelineElements added one after another to the Branch. \"\"\" super () . __init__ ( name , {}, test_disabled = False , disabled = False , base_element = True ) # in case any of the children needs y or covariates we need to request them self . needs_y = True self . needs_covariates = True self . elements = [] self . has_hyperparameters = True self . skip_caching = True self . identifier = \"BRANCH:\" # needed for caching on individual level self . fix_fold_id = False self . do_not_delete_cache_folder = False # add elements if elements : for element in elements : self . add ( element ) def fit ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ): \"\"\" Calls the fit function on all underlying base elements. Parameters: X: The array-like input with shape=[N, D], where N is the number of samples and D is the number of features. y: The truth array-like values with shape=[N], where N is the number of samples. **kwargs: Keyword arguments, passed to base_elements fit. Returns: Fitted self. \"\"\" self . base_element = Branch . sanity_check_pipeline ( self . base_element ) return super () . fit ( X , y , ** kwargs ) def transform ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ) -> ( np . ndarray , np . ndarray , dict ): \"\"\" Calls the transform function on all underlying base elements. If _estimator_type is in ['classifier', 'regressor'], predict is called instead. Parameters: X: The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. y: The truth array-like values with shape=[N], where N is the number of samples. **kwargs: Keyword arguments, passed to base_elements predict/transform. Returns: Transformed/Predicted data. \"\"\" if self . _estimator_type == 'classifier' or self . _estimator_type == 'regressor' : return super () . predict ( X ), y , kwargs return super () . transform ( X , y , ** kwargs ) def predict ( self , X : np . ndarray , ** kwargs ) -> np . ndarray : \"\"\" Calls the predict function on underlying base elements. Parameters: X: The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. **kwargs: Keyword arguments, passed to base_elements predict method. Returns: Prediction values. \"\"\" return super () . predict ( X , ** kwargs ) def __iadd__ ( self , pipe_element : PipelineElement ): \"\"\" Add an element to the sub pipeline. Parameters: pipe_element: The PipelineElement to add, being either a transformer or an estimator. \"\"\" super ( Branch , self ) . __iadd__ ( pipe_element ) self . _prepare_pipeline () return self def add ( self , pipe_element : PipelineElement ): \"\"\" Add an element to the sub pipeline. Parameters: pipe_element: The PipelineElement to add, being either a transformer or an estimator. \"\"\" self . __iadd__ ( pipe_element ) @staticmethod def prepare_photon_pipe ( elements ): pipeline_steps = list () for item in elements : pipeline_steps . append (( item . name , item )) return PhotonPipeline ( pipeline_steps ) @staticmethod def sanity_check_pipeline ( pipe ): if isinstance ( pipe . elements [ - 1 ][ 1 ], CallbackElement ): msg = \"Last element of pipeline cannot be callback element, would be mistaken for estimator. Removing it.\" logger . warning ( msg ) warnings . warn ( msg ) del pipe . elements [ - 1 ] return pipe def _prepare_pipeline ( self ): \"\"\" Generates sklearn pipeline with all underlying elements \"\"\" self . _hyperparameters = { item . name : item . hyperparameters for item in self . elements if hasattr ( item , 'hyperparameters' )} if self . has_hyperparameters : self . generate_sklearn_hyperparameters () new_pipe = Branch . prepare_photon_pipe ( self . elements ) new_pipe . _fix_fold_id = self . fix_fold_id new_pipe . _do_not_delete_cache_folder = self . do_not_delete_cache_folder self . base_element = new_pipe def copy_me ( self ): new_copy_of_me = self . __class__ ( self . name ) for item in self . elements : if hasattr ( item , 'copy_me' ): copy_item = item . copy_me () else : copy_item = deepcopy ( item ) new_copy_of_me += copy_item if self . current_config is not None : new_copy_of_me . set_params ( ** self . current_config ) new_copy_of_me . _random_state = self . _random_state return new_copy_of_me @property def hyperparameters ( self ): return self . _hyperparameters @hyperparameters . setter def hyperparameters ( self , value ): \"\"\" Setting hyperparameters does not make sense, only the items that added can be optimized, not the container (self). \"\"\" return @property def _estimator_type ( self ): return getattr ( self . elements [ - 1 ], '_estimator_type' ) def generate_config_grid ( self ): if self . has_hyperparameters : tmp_grid = create_global_config_grid ( self . elements , self . name ) return tmp_grid else : return [] def generate_sklearn_hyperparameters ( self ): \"\"\" Generates a dictionary according to the sklearn convention of element_name__parameter_name: parameter_value \"\"\" self . _hyperparameters = {} for element in self . elements : for attribute , value_list in element . hyperparameters . items (): self . _hyperparameters [ self . name + '__' + attribute ] = value_list def _check_hyper ( self , BaseEstimator ): pass @property def feature_importances_ ( self ): if hasattr ( self . elements [ - 1 ], 'feature_importances_' ): return getattr ( self . elements [ - 1 ], 'feature_importances_' )","title":"Documentation for Branch"},{"location":"api/base/branch/#photonai.base.photon_elements.Branch.__iadd__","text":"Add an element to the sub pipeline. Parameters: Name Type Description Default pipe_element PipelineElement The PipelineElement to add, being either a transformer or an estimator. required Source code in photonai/base/photon_elements.py def __iadd__ ( self , pipe_element : PipelineElement ): \"\"\" Add an element to the sub pipeline. Parameters: pipe_element: The PipelineElement to add, being either a transformer or an estimator. \"\"\" super ( Branch , self ) . __iadd__ ( pipe_element ) self . _prepare_pipeline () return self","title":"__iadd__()"},{"location":"api/base/branch/#photonai.base.photon_elements.Branch.__init__","text":"Initialize the object. Parameters: Name Type Description Default name str Name of the encapsulated item and/or summary of the encapsulated element`s functions. required elements List[photonai.base.photon_elements.PipelineElement] List of PipelineElements added one after another to the Branch. None Source code in photonai/base/photon_elements.py def __init__ ( self , name : str , elements : List [ PipelineElement ] = None ): \"\"\" Initialize the object. Parameters: name: Name of the encapsulated item and/or summary of the encapsulated element`s functions. elements: List of PipelineElements added one after another to the Branch. \"\"\" super () . __init__ ( name , {}, test_disabled = False , disabled = False , base_element = True ) # in case any of the children needs y or covariates we need to request them self . needs_y = True self . needs_covariates = True self . elements = [] self . has_hyperparameters = True self . skip_caching = True self . identifier = \"BRANCH:\" # needed for caching on individual level self . fix_fold_id = False self . do_not_delete_cache_folder = False # add elements if elements : for element in elements : self . add ( element )","title":"__init__()"},{"location":"api/base/branch/#photonai.base.photon_elements.Branch.add","text":"Add an element to the sub pipeline. Parameters: Name Type Description Default pipe_element PipelineElement The PipelineElement to add, being either a transformer or an estimator. required Source code in photonai/base/photon_elements.py def add ( self , pipe_element : PipelineElement ): \"\"\" Add an element to the sub pipeline. Parameters: pipe_element: The PipelineElement to add, being either a transformer or an estimator. \"\"\" self . __iadd__ ( pipe_element )","title":"add()"},{"location":"api/base/branch/#photonai.base.photon_elements.Branch.fit","text":"Calls the fit function on all underlying base elements. Parameters: Name Type Description Default X ndarray The array-like input with shape=[N, D], where N is the number of samples and D is the number of features. required y ndarray The truth array-like values with shape=[N], where N is the number of samples. None **kwargs Keyword arguments, passed to base_elements fit. {} Returns: Type Description Fitted self. Source code in photonai/base/photon_elements.py def fit ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ): \"\"\" Calls the fit function on all underlying base elements. Parameters: X: The array-like input with shape=[N, D], where N is the number of samples and D is the number of features. y: The truth array-like values with shape=[N], where N is the number of samples. **kwargs: Keyword arguments, passed to base_elements fit. Returns: Fitted self. \"\"\" self . base_element = Branch . sanity_check_pipeline ( self . base_element ) return super () . fit ( X , y , ** kwargs )","title":"fit()"},{"location":"api/base/branch/#photonai.base.photon_elements.Branch.predict","text":"Calls the predict function on underlying base elements. Parameters: Name Type Description Default X ndarray The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. required **kwargs Keyword arguments, passed to base_elements predict method. {} Returns: Type Description ndarray Prediction values. Source code in photonai/base/photon_elements.py def predict ( self , X : np . ndarray , ** kwargs ) -> np . ndarray : \"\"\" Calls the predict function on underlying base elements. Parameters: X: The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. **kwargs: Keyword arguments, passed to base_elements predict method. Returns: Prediction values. \"\"\" return super () . predict ( X , ** kwargs )","title":"predict()"},{"location":"api/base/branch/#photonai.base.photon_elements.Branch.transform","text":"Calls the transform function on all underlying base elements. If _estimator_type is in ['classifier', 'regressor'], predict is called instead. Parameters: Name Type Description Default X ndarray The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. required y ndarray The truth array-like values with shape=[N], where N is the number of samples. None **kwargs Keyword arguments, passed to base_elements predict/transform. {} Returns: Type Description (<class 'numpy.ndarray'>, <class 'numpy.ndarray'>, <class 'dict'>) Transformed/Predicted data. Source code in photonai/base/photon_elements.py def transform ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ) -> ( np . ndarray , np . ndarray , dict ): \"\"\" Calls the transform function on all underlying base elements. If _estimator_type is in ['classifier', 'regressor'], predict is called instead. Parameters: X: The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. y: The truth array-like values with shape=[N], where N is the number of samples. **kwargs: Keyword arguments, passed to base_elements predict/transform. Returns: Transformed/Predicted data. \"\"\" if self . _estimator_type == 'classifier' or self . _estimator_type == 'regressor' : return super () . predict ( X ), y , kwargs return super () . transform ( X , y , ** kwargs )","title":"transform()"},{"location":"api/base/hyperpipe/","text":"Documentation for Hyperpipe The PHOTONAI Hyperpipe class creates a custom machine learning pipeline. In addition it defines the relevant analysis\u2019 parameters such as the cross-validation scheme, the hyperparameter optimization strategy, and the performance metrics of interest. So called PHOTONAI PipelineElements can be added to the Hyperpipe, each of them being a data-processing method or a learning algorithm. By choosing and combining data-processing methods or algorithms, and arranging them with the PHOTONAI classes, simple and complex pipeline architectures can be designed rapidly. The PHOTONAI Hyperpipe automatizes the nested training, test and hyperparameter optimization procedures. The Hyperpipe monitors: the nested-cross-validated training and test procedure, communicates with the hyperparameter optimization strategy, streams information between the pipeline elements, logs all results obtained and evaluates the performance, guides the hyperparameter optimization process by a so-called best config metric which is used to select the best performing hyperparameter configuration. Attributes: Name Type Description optimum_pipe PhotonPipeline An sklearn pipeline object that is fitted to the training data according to the best hyperparameter configuration found. Currently, we don't create an ensemble of all best hyperparameter configs over all folds. We find the best config by comparing the test error across outer folds. The hyperparameter config of the best fold is used as the optimal model and is then trained on the complete set. best_config dict Dictionary containing the hyperparameters of the best configuration. Contains the parameters in the sklearn interface of model_name__parameter_name: parameter value. results MDBHyperpipe Object containing all information about the for the performed hyperparameter search. Holds the training and test metrics for all outer folds, inner folds and configurations, as well as additional information. elements list Contains `all PipelineElement or Hyperpipe objects that are added to the pipeline. Examples: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 from photonai.base import Hyperpipe , PipelineElement from photonai.optimization import FloatRange from sklearn.model_selection import ShuffleSplit , KFold from sklearn.datasets import load_breast_cancer hyperpipe = Hyperpipe ( 'myPipe' , optimizer = 'random_grid_search' , optimizer_params = { 'limit_in_minutes' : 5 }, outer_cv = ShuffleSplit ( test_size = 0.2 , n_splits = 3 ), inner_cv = KFold ( n_splits = 10 , shuffle = True ), metrics = [ 'accuracy' , 'precision' , 'recall' , \"f1_score\" ], best_config_metric = 'accuracy' , eval_final_performance = True , verbosity = 0 ) hyperpipe += PipelineElement ( \"SVC\" , hyperparameters = { \"C\" : FloatRange ( 1 , 100 )}) X , y = load_breast_cancer ( return_X_y = True ) hyperpipe . fit ( X , y ) Source code in photonai/base/hyperpipe.py class Hyperpipe ( BaseEstimator ): \"\"\"The PHOTONAI Hyperpipe class creates a custom machine learning pipeline. In addition it defines the relevant analysis\u2019 parameters such as the cross-validation scheme, the hyperparameter optimization strategy, and the performance metrics of interest. So called PHOTONAI PipelineElements can be added to the Hyperpipe, each of them being a data-processing method or a learning algorithm. By choosing and combining data-processing methods or algorithms, and arranging them with the PHOTONAI classes, simple and complex pipeline architectures can be designed rapidly. The PHOTONAI Hyperpipe automatizes the nested training, test and hyperparameter optimization procedures. The Hyperpipe monitors: - the nested-cross-validated training and test procedure, - communicates with the hyperparameter optimization strategy, - streams information between the pipeline elements, - logs all results obtained and evaluates the performance, - guides the hyperparameter optimization process by a so-called best config metric which is used to select the best performing hyperparameter configuration. Attributes: optimum_pipe (PhotonPipeline): An sklearn pipeline object that is fitted to the training data according to the best hyperparameter configuration found. Currently, we don't create an ensemble of all best hyperparameter configs over all folds. We find the best config by comparing the test error across outer folds. The hyperparameter config of the best fold is used as the optimal model and is then trained on the complete set. best_config (dict): Dictionary containing the hyperparameters of the best configuration. Contains the parameters in the sklearn interface of model_name__parameter_name: parameter value. results (MDBHyperpipe): Object containing all information about the for the performed hyperparameter search. Holds the training and test metrics for all outer folds, inner folds and configurations, as well as additional information. elements (list): Contains `all PipelineElement or Hyperpipe objects that are added to the pipeline. Example: ``` python from photonai.base import Hyperpipe, PipelineElement from photonai.optimization import FloatRange from sklearn.model_selection import ShuffleSplit, KFold from sklearn.datasets import load_breast_cancer hyperpipe = Hyperpipe('myPipe', optimizer='random_grid_search', optimizer_params={'limit_in_minutes': 5}, outer_cv=ShuffleSplit(test_size=0.2, n_splits=3), inner_cv=KFold(n_splits=10, shuffle=True), metrics=['accuracy', 'precision', 'recall', \"f1_score\"], best_config_metric='accuracy', eval_final_performance=True, verbosity=0) hyperpipe += PipelineElement(\"SVC\", hyperparameters={\"C\": FloatRange(1, 100)}) X, y = load_breast_cancer(return_X_y=True) hyperpipe.fit(X, y) ``` \"\"\" def __init__ ( self , name : Optional [ str ], inner_cv : Union [ BaseCrossValidator , BaseShuffleSplit , _RepeatedSplits ] = None , outer_cv : Union [ BaseCrossValidator , BaseShuffleSplit , _RepeatedSplits , None ] = None , optimizer : str = 'grid_search' , optimizer_params : dict = None , metrics : Optional [ List [ Union [ Scorer . Metric_Type , str ]]] = None , best_config_metric : Optional [ Union [ Scorer . Metric_Type , str ]] = None , eval_final_performance : bool = None , use_test_set : bool = True , test_size : float = 0.2 , project_folder : str = '' , calculate_metrics_per_fold : bool = True , calculate_metrics_across_folds : bool = False , random_seed : int = None , verbosity : int = 0 , learning_curves : bool = False , learning_curves_cut : FloatRange = None , output_settings : OutputSettings = None , performance_constraints : list = None , permutation_id : str = None , cache_folder : str = None , nr_of_processes : int = 1 , allow_multidim_targets : bool = False ): \"\"\" Initialize the object. Parameters: name: Name of hyperpipe instance. inner_cv: Cross validation strategy to test hyperparameter configurations, generates the validation set. outer_cv: Cross validation strategy to use for the hyperparameter search itself, generates the test set. optimizer: Hyperparameter optimization algorithm. - In case a string literal is given: - \"grid_search\": Optimizer that iteratively tests all possible hyperparameter combinations. - \"random_grid_search\": A variation of the grid search optimization that randomly picks hyperparameter combinations from all possible hyperparameter combinations. - \"sk_opt\": Scikit-Optimize based on theories of bayesian optimization. - \"random_search\": randomly chooses hyperparameter from grid-free domain. - \"smac\": SMAC based on theories of bayesian optimization. - \"nevergrad\": Nevergrad based on theories of evolutionary learning. - In case an object is given: expects the object to have the following methods: - `ask`: returns a hyperparameter configuration in form of an dictionary containing key->value pairs in the sklearn parameter encoding `model_name__parameter_name: parameter_value` - `prepare`: takes a list of pipeline elements and their particular hyperparameters to prepare the hyperparameter space - `tell`: gets a tested config and the respective performance in order to calculate a smart next configuration to process metrics: Metrics that should be calculated for both training, validation and test set Use the preimported metrics from sklearn and photonai, or register your own - Metrics for `classification`: - `accuracy`: sklearn.metrics.accuracy_score - `matthews_corrcoef`: sklearn.metrics.matthews_corrcoef - `confusion_matrix`: sklearn.metrics.confusion_matrix, - `f1_score`: sklearn.metrics.f1_score - `hamming_loss`: sklearn.metrics.hamming_loss - `log_loss`: sklearn.metrics.log_loss - `precision`: sklearn.metrics.precision_score - `recall`: sklearn.metrics.recall_score - Metrics for `regression`: - `mean_squared_error`: sklearn.metrics.mean_squared_error - `mean_absolute_error`: sklearn.metrics.mean_absolute_error - `explained_variance`: sklearn.metrics.explained_variance_score - `r2`: sklearn.metrics.r2_score - Other metrics - `pearson_correlation`: photon_core.framework.Metrics.pearson_correlation - `variance_explained`: photon_core.framework.Metrics.variance_explained_score - `categorical_accuracy`: photon_core.framework.Metrics.categorical_accuracy_score best_config_metric: The metric that should be maximized or minimized in order to choose the best hyperparameter configuration. eval_final_performance: DEPRECATED! Use \"use_test_set\" instead! use_test_set: If the metrics should be calculated for the test set, otherwise the test set is seperated but not used. project_folder: The output folder in which all files generated by the PHOTONAI project are saved to. test_size: The amount of the data that should be left out if no outer_cv is given and eval_final_performance is set to True. calculate_metrics_per_fold: If True, the metrics are calculated for each inner_fold. If False, calculate_metrics_across_folds must be True. calculate_metrics_across_folds: If True, the metrics are calculated across all inner_fold. If False, calculate_metrics_per_fold must be True. random_seed: Random Seed. verbosity: The level of verbosity, 0 is least talkative and gives only warn and error, 1 gives adds info and 2 adds debug. learning_curves: Enables learning curve procedure. Evaluate learning process over different sizes of input. Depends on learning_curves_cut. learning_curves_cut: The tested relative cuts for data size. performance_constraints: Objects that indicate whether a configuration should be tested further. For example, the inner fold of a config does not perform better than the dummy performance. permutation_id: String identifier for permutation tests. cache_folder: Folder path for multi-processing. nr_of_processes: Determined the amount of simultaneous calculation of outer_folds. allow_multidim_targets: Allows multidimensional targets. \"\"\" self . name = re . sub ( r '\\W+' , '' , name ) if eval_final_performance is not None : depr_warning = \"Hyperpipe parameter eval_final_performance is deprecated. It's called use_test_set now.\" use_test_set = eval_final_performance logger . warning ( depr_warning ) raise DeprecationWarning ( depr_warning ) # ====================== Cross Validation =========================== # check if both calculate_metrics_per_folds and calculate_metrics_across_folds is False if not calculate_metrics_across_folds and not calculate_metrics_per_fold : raise NotImplementedError ( \"Apparently, you've set calculate_metrics_across_folds=False and \" \"calculate_metrics_per_fold=False. In this case PHOTONAI does not calculate \" \"any metrics which doesn't make any sense. Set at least one to True.\" ) if inner_cv is None : msg = \"PHOTONAI requires an inner_cv split. Please enable inner cross-validation. \" \\ \"As exmaple: Hyperpipe(...inner_cv = KFold(n_splits = 3), ...). \" \\ \"Ensure you import the cross_validation object first.\" logger . error ( msg ) raise AttributeError ( msg ) # use default cut 'FloatRange(0, 1, 'range', 0.2)' if learning_curves = True but learning_curves_cut is None if learning_curves and learning_curves_cut is None : learning_curves_cut = FloatRange ( 0 , 1 , 'range' , 0.2 ) elif not learning_curves and learning_curves_cut is not None : learning_curves_cut = None self . cross_validation = Hyperpipe . CrossValidation ( inner_cv = inner_cv , outer_cv = outer_cv , use_test_set = use_test_set , test_size = test_size , calculate_metrics_per_fold = calculate_metrics_per_fold , calculate_metrics_across_folds = calculate_metrics_across_folds , learning_curves = learning_curves , learning_curves_cut = learning_curves_cut ) # ====================== Data =========================== self . data = Hyperpipe . Data ( allow_multidim_targets = allow_multidim_targets ) # ====================== Output Folder and Log File Management =========================== if output_settings : self . output_settings = output_settings else : self . output_settings = OutputSettings () if project_folder == '' : self . project_folder = os . getcwd () else : self . project_folder = project_folder self . output_settings . set_project_folder ( self . project_folder ) # update output options to add pipe name and timestamp to results folder self . _verbosity = 0 self . verbosity = verbosity self . output_settings . set_log_file () # ====================== Result Logging =========================== self . results_handler = None self . results = None self . best_config = None # ====================== Pipeline =========================== self . elements = [] self . _pipe = None self . optimum_pipe = None self . preprocessing = None # ====================== Performance Optimization =========================== if optimizer_params is None : optimizer_params = {} self . optimization = Optimization ( metrics = metrics , best_config_metric = best_config_metric , optimizer_input = optimizer , optimizer_params = optimizer_params , performance_constraints = performance_constraints ) # self.optimization.sanity_check_metrics() # ====================== Caching and Parallelization =========================== self . nr_of_processes = nr_of_processes if cache_folder : self . cache_folder = os . path . join ( cache_folder , self . name ) else : self . cache_folder = None # ====================== Internals =========================== self . permutation_id = permutation_id self . allow_multidim_targets = allow_multidim_targets self . is_final_fit = False # ====================== Random Seed =========================== self . random_state = random_seed if random_seed is not None : import random random . seed ( random_seed ) # =================================================================== # Helper Classes # =================================================================== class CrossValidation : def __init__ ( self , inner_cv , outer_cv , use_test_set , test_size , calculate_metrics_per_fold , calculate_metrics_across_folds , learning_curves , learning_curves_cut ): self . inner_cv = inner_cv self . outer_cv = outer_cv self . use_test_set = use_test_set self . test_size = test_size self . learning_curves = learning_curves self . learning_curves_cut = learning_curves_cut self . calculate_metrics_per_fold = calculate_metrics_per_fold # Todo: if self.outer_cv is LeaveOneOut: Set calculate metrics across folds to True -> Print self . calculate_metrics_across_folds = calculate_metrics_across_folds self . outer_folds = None self . inner_folds = dict () def __str__ ( self ): return \"Hyperpipe {} \" . format ( self . name ) class Data : def __init__ ( self , X = None , y = None , kwargs = None , allow_multidim_targets = False ): self . X = X self . y = y self . kwargs = kwargs self . allow_multidim_targets = allow_multidim_targets def input_data_sanity_checks ( self , data , targets , ** kwargs ): # ==================== SANITY CHECKS =============================== # 1. Make to numpy arrays # 2. erase all Nan targets logger . info ( \"Checking input data...\" ) self . X = data self . y = targets self . kwargs = kwargs try : if self . X is None : raise ValueError ( \"(Input-)data is a NoneType.\" ) if self . y is None : raise ValueError ( \"(Input-)target is a NoneType.\" ) shape_x = np . shape ( self . X ) shape_y = np . shape ( self . y ) if not self . allow_multidim_targets : if len ( shape_y ) != 1 : if len ( np . shape ( np . squeeze ( self . y ))) == 1 : # use np.squeeze for non 1D targets. self . y = np . squeeze ( self . y ) shape_y = np . shape ( self . y ) msg = \"y has been automatically squeezed. If this is not your intention, block this \" \\ \"with Hyperpipe(allow_multidim_targets = True\" logger . warning ( msg ) warnings . warn ( msg ) else : raise ValueError ( \"Target is not one-dimensional. Multidimensional targets can cause problems\" \"with sklearn metrics. Please override with \" \"Hyperpipe(allow_multidim_targets = True).\" ) if not shape_x [ 0 ] == shape_y [ 0 ]: raise IndexError ( \"Size of targets mismatch to size of the data: \" + str ( shape_x [ 0 ]) + \" - \" + str ( shape_y [ 0 ])) except IndexError as ie : logger . error ( \"IndexError: \" + str ( ie )) raise ie except ValueError as ve : logger . error ( \"ValueError: \" + str ( ve )) raise ve except Exception as e : logger . error ( \"Error: \" + str ( e )) raise e # be compatible to list of (image-) files if isinstance ( self . X , list ): self . X = np . asarray ( self . X ) elif isinstance ( self . X , ( pd . DataFrame , pd . Series )): self . X = self . X . to_numpy () if isinstance ( self . y , list ): self . y = np . asarray ( self . y ) elif isinstance ( self . y , pd . Series ) or isinstance ( self . y , pd . DataFrame ): self . y = self . y . to_numpy () # at first first, erase all rows where y is Nan if preprocessing has not done it already try : nans_in_y = np . isnan ( self . y ) nr_of_nans = len ( np . where ( nans_in_y == 1 )[ 0 ]) if nr_of_nans > 0 : logger . info ( \"You have {} Nans in your target vector, \" \"PHOTONAI erases every data item that has a Nan Target\" . format ( str ( nr_of_nans ))) self . X = self . X [ ~ nans_in_y ] self . y = self . y [ ~ nans_in_y ] new_kwargs = dict () for name , element_list in kwargs . items (): new_kwargs [ name ] = element_list [ ~ nans_in_y ] self . kwargs = new_kwargs except Exception as e : # This is only for convenience so if it fails then never mind logger . error ( \"Removing Nans in target vector failed: \" + str ( e )) pass logger . info ( \"Running analysis with \" + str ( self . y . shape [ 0 ]) + \" samples.\" ) # =================================================================== # Properties and Helper # =================================================================== @property def estimation_type ( self ): estimation_type = getattr ( self . elements [ - 1 ], '_estimator_type' ) if estimation_type is None : raise NotImplementedError ( \"Last element in Hyperpipe should be an estimator.\" ) else : return estimation_type @property def verbosity ( self ): return self . _verbosity @verbosity . setter def verbosity ( self , value ): self . _verbosity = value self . output_settings . verbosity = self . _verbosity self . output_settings . set_log_level () @staticmethod def disable_multiprocessing_recursively ( pipe ): if isinstance ( pipe , ( Stack , Branch , Switch , Preprocessing )): if hasattr ( pipe , 'nr_of_processes' ): pipe . nr_of_processes = 1 for child in pipe . elements : if isinstance ( child , Branch ): Hyperpipe . disable_multiprocessing_recursively ( child ) elif hasattr ( child , 'base_element' ): Hyperpipe . disable_multiprocessing_recursively ( child . base_element ) elif isinstance ( pipe , PhotonPipeline ): for name , child in pipe . named_steps . items (): Hyperpipe . disable_multiprocessing_recursively ( child ) else : if hasattr ( pipe , 'nr_of_processes' ): pipe . nr_of_processes = 1 @staticmethod def recursive_cache_folder_propagation ( element , cache_folder , inner_fold_id ): if isinstance ( element , ( Switch , Stack , Preprocessing )): for child in element . elements : Hyperpipe . recursive_cache_folder_propagation ( child , cache_folder , inner_fold_id ) elif isinstance ( element , Branch ): # in case it's a Branch, we create a cache subfolder and propagate it to every child if cache_folder : cache_folder = os . path . join ( cache_folder , element . name ) Hyperpipe . recursive_cache_folder_propagation ( element . base_element , cache_folder , inner_fold_id ) # Hyperpipe.prepare_caching(element.base_element.cache_folder) elif isinstance ( element , PhotonPipeline ): element . fold_id = inner_fold_id element . cache_folder = cache_folder # pipe.caching is automatically set to True or False by .cache_folder setter for name , child in element . named_steps . items (): # we need to check if any element is Branch, Stack or Swtich Hyperpipe . recursive_cache_folder_propagation ( child , cache_folder , inner_fold_id ) # else: if it's a simple PipelineElement, then we just don't do anything # =================================================================== # Pipeline Setup # =================================================================== def __iadd__ ( self , pipe_element : PipelineElement ): \"\"\" Add an element to the machine learning pipeline. Returns self. Parameters: pipe_element: The object to add to the machine learning pipeline, being either a transformer or an estimator. \"\"\" if isinstance ( pipe_element , Preprocessing ): self . preprocessing = pipe_element elif isinstance ( pipe_element , CallbackElement ): pipe_element . needs_y = True self . elements . append ( pipe_element ) else : if isinstance ( pipe_element , PipelineElement ) or issubclass ( type ( pipe_element ), PhotonNative ): self . elements . append ( pipe_element ) else : raise TypeError ( \"Element must be of type Pipeline Element\" ) return self def add ( self , pipe_element : PipelineElement ): \"\"\" Add an element to the machine learning pipeline. Returns self. Parameters: pipe_element: The object to add to the machine learning pipeline, being either a transformer or an estimator. \"\"\" self . __iadd__ ( pipe_element ) # =================================================================== # Workflow Setup # =================================================================== def _prepare_dummy_estimator ( self ): self . results . dummy_estimator = MDBDummyResults () if self . estimation_type == 'regressor' : self . results . dummy_estimator . strategy = 'mean' return DummyRegressor ( strategy = self . results . dummy_estimator . strategy ) elif self . estimation_type == 'classifier' : self . results . dummy_estimator . strategy = 'most_frequent' return DummyClassifier ( strategy = self . results . dummy_estimator . strategy ) else : logger . info ( 'Estimator does not specify whether it is a regressor or classifier. ' 'DummyEstimator step skipped.' ) return def __get_pipeline_structure ( self , pipeline_elements ): element_list = dict () for p_el in pipeline_elements : if not hasattr ( p_el , 'name' ): raise Warning ( 'Strange Pipeline Element found that has no name..? Type: ' . format ( type ( p_el ))) if hasattr ( p_el , 'elements' ): child_list = self . __get_pipeline_structure ( p_el . elements ) identifier = p_el . name if hasattr ( p_el , \"identifier\" ): identifier = p_el . identifier + identifier element_list [ identifier ] = child_list else : if hasattr ( p_el , 'base_element' ): element_list [ p_el . name ] = str ( type ( p_el . base_element )) else : element_list [ p_el . name ] = str ( type ( p_el )) return element_list def _prepare_result_logging ( self , start_time ): self . results = MDBHyperpipe ( name = self . name , version = __version__ ) self . results . hyperpipe_info = MDBHyperpipeInfo () # in case eval final performance is false, we have no outer fold predictions if not self . cross_validation . use_test_set : self . output_settings . save_predictions_from_best_config_inner_folds = True self . results_handler = ResultsHandler ( self . results , self . output_settings ) self . results . computation_start_time = start_time self . results . hyperpipe_info . estimation_type = self . estimation_type self . results . output_folder = self . output_settings . results_folder if self . permutation_id is not None : self . results . permutation_id = self . permutation_id # save wizard information to PHOTONAI db in order to map results to the wizard design object if self . output_settings and hasattr ( self . output_settings , 'wizard_object_id' ): if self . output_settings . wizard_object_id : self . name = self . output_settings . wizard_object_id self . results . name = self . output_settings . wizard_object_id self . results . wizard_object_id = ObjectId ( self . output_settings . wizard_object_id ) self . results . wizard_system_name = self . output_settings . wizard_project_name self . results . user_id = self . output_settings . user_id self . results . outer_folds = [] self . results . hyperpipe_info . elements = self . __get_pipeline_structure ( self . elements ) self . results . hyperpipe_info . eval_final_performance = self . cross_validation . use_test_set self . results . hyperpipe_info . best_config_metric = self . optimization . best_config_metric self . results . hyperpipe_info . metrics = self . optimization . metrics self . results . hyperpipe_info . learning_curves_cut = self . cross_validation . learning_curves_cut self . results . hyperpipe_info . maximize_best_config_metric = self . optimization . maximize_metric # optimization def _format_cross_validation ( cv ): if cv : string = \" {} (\" . format ( cv . __class__ . __name__ ) for key , val in cv . __dict__ . items (): string += \" {} = {} , \" . format ( key , val ) return string [: - 2 ] + \")\" else : return \"None\" self . results . hyperpipe_info . cross_validation = \\ { 'OuterCV' : _format_cross_validation ( self . cross_validation . outer_cv ), 'InnerCV' : _format_cross_validation ( self . cross_validation . inner_cv )} self . results . hyperpipe_info . data = { 'X_shape' : self . data . X . shape , 'y_shape' : self . data . y . shape } self . results . hyperpipe_info . optimization = { 'Optimizer' : self . optimization . optimizer_input_str , 'OptimizerParams' : str ( self . optimization . optimizer_params ), 'BestConfigMetric' : self . optimization . best_config_metric } # add json file of hyperpipe attributes try : json_transformer = JsonTransformer () json_transformer . to_json_file ( self , self . output_settings . results_folder + \"/hyperpipe_config.json\" ) except : msg = \"JsonTransformer was unable to create the .json file.\" logger . warning ( msg ) warnings . warn ( msg ) def _finalize_optimization ( self ): # ==================== EVALUATING RESULTS OF HYPERPARAMETER OPTIMIZATION =============================== # 1. computing average metrics # 2. finding overall best config # 3. training model with best config # 4. persisting best model logger . clean_info ( '' ) logger . stars () logger . photon_system_log ( \"Finished all outer fold computations.\" ) logger . info ( \"Now analysing the final results...\" ) # computer dummy metrics logger . info ( \"Computing dummy metrics...\" ) config_item = MDBConfig () dummy_results = [ outer_fold . dummy_results for outer_fold in self . results . outer_folds ] config_item . inner_folds = [ f for f in dummy_results if f is not None ] if len ( config_item . inner_folds ) > 0 : self . results . dummy_estimator . metrics_train , self . results . dummy_estimator . metrics_test = \\ MDBHelper . aggregate_metrics_for_inner_folds ( config_item . inner_folds , self . optimization . metrics ) logger . info ( \"Computing mean and std for all outer fold metrics...\" ) # Compute all final metrics self . results . metrics_train , self . results . metrics_test = \\ MDBHelper . aggregate_metrics_for_outer_folds ( self . results . outer_folds , self . optimization . metrics ) # Find best config across outer folds logger . info ( \"Find best config across outer folds...\" ) best_config = self . optimization . get_optimum_config_outer_folds ( self . results . outer_folds ) self . best_config = best_config . config_dict self . results . best_config = best_config # save results again self . results . computation_end_time = datetime . datetime . now () self . results . computation_completed = True logger . info ( \"Save final results...\" ) self . results_handler . save () logger . info ( \"Prepare Hyperpipe.optimum pipe with best config..\" ) # set self to best config self . optimum_pipe = self . _pipe self . optimum_pipe . set_params ( ** self . best_config ) if self . output_settings . generate_best_model : logger . info ( \"Fitting best model...\" ) # set self to best config self . optimum_pipe = self . _pipe self . optimum_pipe . set_params ( ** self . best_config ) # set caching # we want caching disabled in general but still want to do single subject caching self . recursive_cache_folder_propagation ( self . optimum_pipe , self . cache_folder , 'fixed_fold_id' ) self . optimum_pipe . caching = False # disable multiprocessing when fitting optimum pipe # (otherwise inverse_transform won't work for BrainAtlas/Mask) self . disable_multiprocessing_recursively ( self . optimum_pipe ) self . optimum_pipe . fit ( self . data . X , self . data . y , ** self . data . kwargs ) # Before saving the optimum pipe, add preprocessing without multiprocessing self . disable_multiprocessing_recursively ( self . preprocessing ) self . optimum_pipe . add_preprocessing ( self . preprocessing ) # Now truly set to no caching (including single_subject_caching) self . recursive_cache_folder_propagation ( self . optimum_pipe , None , None ) if self . output_settings . save_output : try : pretrained_model_filename = os . path . join ( self . output_settings . results_folder , 'photon_best_model.photon' ) PhotonModelPersistor . save_optimum_pipe ( self . optimum_pipe , pretrained_model_filename ) logger . info ( \"Saved best model to file.\" ) except Exception as e : logger . info ( \"Could not save best model to file\" ) logger . error ( str ( e )) # get feature importances of optimum pipe logger . info ( \"Mapping back feature importances...\" ) feature_importances = self . optimum_pipe . feature_importances_ if not feature_importances : logger . info ( \"No feature importances available for {} !\" . format ( self . optimum_pipe . elements [ - 1 ][ 0 ])) else : self . results . best_config_feature_importances = feature_importances # write backmapping file only if optimum_pipes inverse_transform works completely. # restriction: only a faulty inverse_transform is considered, missing ones are further ignored. with warnings . catch_warnings ( record = True ) as w : # get backmapping backmapping , _ , _ = self . optimum_pipe . \\ inverse_transform ( np . array ( feature_importances ) . reshape ( 1 , - 1 ), None ) if not any ( \"The inverse transformation is not possible for\" in s for s in [ e . message . args [ 0 ] for e in w ]): # save backmapping self . results_handler . save_backmapping ( filename = 'optimum_pipe_feature_importances_backmapped' , backmapping = backmapping ) else : logger . info ( 'Could not save feature importance: backmapping NOT successful.' ) # save learning curves if self . cross_validation . learning_curves : self . results_handler . save_all_learning_curves () logger . info ( \"Summarizing results...\" ) logger . info ( \"Write predictions to files...\" ) # write all convenience files (summary, predictions_file and plots) self . results_handler . write_predictions_file () logger . info ( \"Write summary...\" ) logger . stars () logger . photon_system_log ( \"\" ) logger . photon_system_log ( self . results_handler . text_summary ()) def preprocess_data ( self ): # if there is a preprocessing pipeline, we apply it first. if self . preprocessing is not None : logger . info ( \"Applying preprocessing steps...\" ) self . preprocessing . fit ( self . data . X , self . data . y , ** self . data . kwargs ) self . data . X , self . data . y , self . data . kwargs = self . preprocessing . transform ( self . data . X , self . data . y , ** self . data . kwargs ) def _prepare_pipeline ( self ): self . _pipe = Branch . prepare_photon_pipe ( self . elements ) self . _pipe = Branch . sanity_check_pipeline ( self . _pipe ) if self . random_state : self . _pipe . random_state = self . random_state # =================================================================== # sklearn interfaces # =================================================================== @staticmethod def fit_outer_folds ( outer_fold_computer , X , y , kwargs ): outer_fold_computer . fit ( X , y , ** kwargs ) return def fit ( self , data : np . ndarray , targets : np . ndarray , ** kwargs ): \"\"\" Starts the hyperparameter search and/or fits the pipeline to the data and targets. Manages the nested cross validated hyperparameter search: 1. Filters the data according to filter strategy (1) and according to the imbalanced_data_strategy (2) 2. requests new configurations from the hyperparameter search strategy, the optimizer, 3. initializes the testing of a specific configuration, 4. communicates the result to the optimizer, 5. repeats 2-4 until optimizer delivers no more configurations to test 6. finally searches for the best config in all tested configs, 7. trains the pipeline with the best config and evaluates the performance on the test set Parameters: data: The array-like training and test data with shape=[N, D], where N is the number of samples and D is the number of features. targets: The truth array-like values with shape=[N], where N is the number of samples. **kwargs: Keyword arguments, passed to Outer_Fold_Manager.fit. Returns: Fitted Hyperpipe. \"\"\" # switch to result output folder start = datetime . datetime . now () self . output_settings . update_settings ( self . name , start . strftime ( \"%Y-%m- %d _%H-%M-%S\" )) logger . photon_system_log ( '=' * 101 ) logger . photon_system_log ( 'PHOTONAI ANALYSIS: ' + self . name ) logger . photon_system_log ( '=' * 101 ) logger . info ( \"Preparing data and PHOTONAI objects for analysis...\" ) # loop over outer cross validation if self . nr_of_processes > 1 : hyperpipe_client = Client ( threads_per_worker = 1 , n_workers = self . nr_of_processes , processes = False ) try : # check data self . data . input_data_sanity_checks ( data , targets , ** kwargs ) # create photon pipeline self . _prepare_pipeline () # initialize the progress monitors self . _prepare_result_logging ( start ) # apply preprocessing self . preprocess_data () if not self . is_final_fit : # Outer Folds outer_folds = FoldInfo . generate_folds ( self . cross_validation . outer_cv , self . data . X , self . data . y , self . data . kwargs , self . cross_validation . use_test_set , self . cross_validation . test_size ) self . cross_validation . outer_folds = { f . fold_id : f for f in outer_folds } delayed_jobs = [] # Run Dummy Estimator dummy_estimator = self . _prepare_dummy_estimator () if self . cache_folder is not None : logger . info ( \"Removing cache files...\" ) CacheManager . clear_cache_files ( self . cache_folder , force_all = True ) # loop over outer cross validation for i , outer_f in enumerate ( outer_folds ): # 1. generate OuterFolds Object outer_fold = MDBOuterFold ( fold_nr = outer_f . fold_nr ) outer_fold_computer = OuterFoldManager ( self . _pipe , self . optimization , outer_f . fold_id , self . cross_validation , cache_folder = self . cache_folder , cache_updater = self . recursive_cache_folder_propagation , dummy_estimator = dummy_estimator , result_obj = outer_fold ) # 2. monitor outputs self . results . outer_folds . append ( outer_fold ) if self . nr_of_processes > 1 : result = dask . delayed ( Hyperpipe . fit_outer_folds )( outer_fold_computer , self . data . X , self . data . y , self . data . kwargs ) delayed_jobs . append ( result ) else : try : # 3. fit outer_fold_computer . fit ( self . data . X , self . data . y , ** self . data . kwargs ) # 4. save outer fold results self . results_handler . save () finally : # 5. clear cache CacheManager . clear_cache_files ( self . cache_folder ) if self . nr_of_processes > 1 : dask . compute ( * delayed_jobs ) self . results_handler . save () # evaluate hyperparameter optimization results for best config self . _finalize_optimization () # clear complete cache ? use self.cache_folder to delete all subfolders within the parent cache folder # directory CacheManager . clear_cache_files ( self . cache_folder , force_all = True ) ############################################################################################### else : self . preprocess_data () self . _pipe . fit ( self . data . X , self . data . y , ** kwargs ) except Exception as e : logger . error ( e ) logger . error ( traceback . format_exc ()) traceback . print_exc () raise e finally : if self . nr_of_processes > 1 : hyperpipe_client . close () return self def predict ( self , data : np . ndarray , ** kwargs ) -> np . ndarray : \"\"\" Use the optimum pipe to predict the input data. Parameters: data: The array-like prediction data with shape=[M, D], where M is the number of samples and D is the number of features. D must correspond to the number of trained dimensions of the fit method. **kwargs: Keyword arguments, passed to optimum_pipe.predict. Returns: Predicted targets calculated on input data with trained model. \"\"\" # Todo: if local_search = true then use optimized pipe here? if self . _pipe : return self . optimum_pipe . predict ( data , ** kwargs ) def predict_proba ( self , data : np . ndarray , ** kwargs ) -> np . ndarray : \"\"\" Use the optimum pipe to predict the probabilities from the input data. Parameters: data: The array-like prediction data with shape=[M, D], where M is the number of samples and D is the number of features. D must correspond to the number of trained dimensions of the fit method. **kwargs: Keyword arguments, passed to optimum_pipe.predict_proba. Returns: Probabilities calculated from input data on fitted model. \"\"\" if self . _pipe : return self . optimum_pipe . predict_proba ( data , ** kwargs ) def transform ( self , data : np . ndarray , ** kwargs ) -> np . ndarray : \"\"\" Use the optimum pipe to transform the data. Parameters: data: The array-like input data with shape=[M, D], where M is the number of samples and D is the number of features. D must correspond to the number of trained dimensions of the fit method. **kwargs: Keyword arguments, passed to optimum_pipe.transform. Returns: Transformed data. \"\"\" if self . _pipe : X , _ , _ = self . optimum_pipe . transform ( data , y = None , ** kwargs ) return X def score ( self , data : np . ndarray , y : np . ndarray , ** kwargs ) -> np . ndarray : \"\"\" Use the optimum pipe to score the model. Parameters: data: The array-like data with shape=[M, D], where M is the number of samples and D is the number of features. D must correspond to the number of trained dimensions of the fit method. y: The array-like true targets. **kwargs: Keyword arguments, passed to optimum_pipe.predict. Returns: Score data on input data with trained model. \"\"\" if self . _pipe : predictions = self . optimum_pipe . predict ( data , ** kwargs ) scorer = Scorer . create ( self . optimization . best_config_metric ) return scorer ( y , predictions ) def _calculate_permutation_importances ( self , ** kwargs ): \"\"\" extracted function from get_feature_importance to improve unit testing \"\"\" importance_list = { 'mean' : list (), 'std' : list ()} def train_and_get_fimps ( pipeline , train_idx , test_idx , data_X , data_y , data_kwargs , fold_str ): train_X , train_y , train_kwargs = PhotonDataHelper . split_data ( data_X , data_y , data_kwargs , indices = train_idx ) test_X , test_y , test_kwargs = PhotonDataHelper . split_data ( data_X , data_y , data_kwargs , indices = test_idx ) # fit fold's best model (again) -> to obtain that model's feature importances logger . photon_system_log ( \"Permutation Importances: Fitting model for \" + fold_str ) pipeline . fit ( train_X , train_y , ** train_kwargs ) # get feature importances logger . photon_system_log ( \"Permutation Importances: Calculating performances for \" + fold_str ) perm_imps = permutation_importance ( pipeline , test_X , test_y , ** kwargs ) # store into list importance_list [ 'mean' ] . append ( perm_imps [ \"importances_mean\" ]) importance_list [ 'std' ] . append ( perm_imps [ \"importances_std\" ]) return perm_imps for outer_fold in self . results . outer_folds : if outer_fold . best_config is None : raise ValueError ( \"Could not find a best config for outer fold \" + str ( outer_fold . fold_nr )) pipe_copy = self . optimum_pipe . copy_me () # set pipe to config pipe_copy . set_params ( ** outer_fold . best_config . config_dict ) if not self . results . hyperpipe_info . eval_final_performance : no_outer_cv_indices = False if outer_fold . best_config . best_config_score is None : no_outer_cv_indices = True elif outer_fold . best_config . best_config_score . training is None or not outer_fold . best_config . best_config_score . training . indices : no_outer_cv_indices = True if no_outer_cv_indices : data_to_split , y_to_split , kwargs_to_split = self . data . X , self . data . y , self . data . kwargs else : logger . photon_system_log ( \"Permutation Importances: Using inner_cv folds.\" ) # get outer fold data idx = outer_fold . best_config . best_config_score . training . indices data_to_split , y_to_split , kwargs_to_split = PhotonDataHelper . split_data ( self . data . X , self . data . y , self . data . kwargs , indices = idx ) for inner_fold in outer_fold . best_config . inner_folds : train_and_get_fimps ( pipe_copy , inner_fold . training . indices , inner_fold . validation . indices , data_to_split , y_to_split , kwargs_to_split , \"inner fold \" + str ( inner_fold . fold_nr )) else : train_and_get_fimps ( pipe_copy , outer_fold . best_config . best_config_score . training . indices , outer_fold . best_config . best_config_score . validation . indices , self . data . X , self . data . y , self . data . kwargs , \"outer fold \" + str ( outer_fold . fold_nr )) return importance_list def get_permutation_feature_importances ( self , ** kwargs ): \"\"\" Fits a model for the best config of each outer fold (using the training data of that fold). Then calls sklearn.inspection.permutation_importance with the test data and the given kwargs (e.g. n_repeats). Returns mean of \"importances_mean\" and of \"importances_std\" of all outer folds. Parameters: **kwargs: Keyword arguments, passed to sklearn.permutation_importance. Returns: Dictionary with average of \"mean\" and \"std\" for all outer folds, respectively. \"\"\" logger . photon_system_log ( \"\" ) logger . photon_system_log ( \"Computing permutation importances. This may take a while.\" ) logger . stars () if self . optimum_pipe is None : raise ValueError ( \"Cannot calculate permutation importances when optimum_pipe is None (probably the \" \"training and optimization procedure failed)\" ) importance_list = self . _calculate_permutation_importances ( ** kwargs ) mean_importances = np . mean ( np . array ( importance_list [ \"mean\" ]), axis = 0 ) std_importances = np . mean ( np . array ( importance_list [ \"std\" ]), axis = 0 ) logger . stars () return { 'mean' : mean_importances , 'std' : std_importances } def inverse_transform_pipeline ( self , hyperparameters : dict , data : np . ndarray , targets : np . ndarray , data_to_inverse : np . ndarray ) -> np . ndarray : \"\"\" Inverse transform data for a pipeline with specific hyperparameter configuration. 1. Copy Sklearn Pipeline, 2. Set Parameters 3. Fit Pipeline to data and targets 4. Inverse transform data with that pipeline Parameters: hyperparameters: The concrete configuration settings for the pipeline elements. data: The training data to which the pipeline is fitted. targets: The truth values for training. data_to_inverse: The data that should be inversed after training. Returns: Inverse data as array. \"\"\" copied_pipe = self . pipe . copy_me () copied_pipe . set_params ( ** hyperparameters ) copied_pipe . fit ( data , targets ) return copied_pipe . inverse_transform ( data_to_inverse ) # =================================================================== # Copy, Save and Load # =================================================================== def copy_me ( self ): \"\"\" Helper function to copy an entire Hyperpipe Returns: Hyperpipe \"\"\" signature = inspect . getfullargspec ( OutputSettings . __init__ )[ 0 ] settings = OutputSettings () for attr in signature : if hasattr ( self . output_settings , attr ): setattr ( settings , attr , getattr ( self . output_settings , attr )) self . output_settings . initialize_log_file () # create new Hyperpipe instance pipe_copy = Hyperpipe ( name = self . name , inner_cv = deepcopy ( self . cross_validation . inner_cv ), outer_cv = deepcopy ( self . cross_validation . outer_cv ), best_config_metric = self . optimization . best_config_metric , metrics = self . optimization . metrics , optimizer = self . optimization . optimizer_input_str , optimizer_params = self . optimization . optimizer_params , project_folder = self . project_folder , output_settings = settings ) signature = inspect . getfullargspec ( self . __init__ )[ 0 ] for attr in signature : if hasattr ( self , attr ) and attr != 'output_settings' : setattr ( pipe_copy , attr , getattr ( self , attr )) if hasattr ( self , 'preprocessing' ) and self . preprocessing : preprocessing = Preprocessing () for element in self . preprocessing . elements : preprocessing += element . copy_me () pipe_copy += preprocessing if hasattr ( self , 'elements' ): for element in self . elements : pipe_copy += element . copy_me () return pipe_copy def save_optimum_pipe ( self , filename = None , password = None ): if filename is None : filename = \"photon_\" + self . name + \"_best_model.p\" PhotonModelPersistor . save_optimum_pipe ( self , filename , password ) @staticmethod def load_optimum_pipe ( file : str , password : str = None ) -> PhotonPipeline : \"\"\" Load optimum pipe from file. As staticmethod, instantiation is thus not required. Called backend: PhotonModelPersistor.load_optimum_pipe. Parameters: file: File path specifying .photon file to load trained pipeline from zipped file. password: Passcode for read file. Returns: Returns pipeline with all trained PipelineElements. \"\"\" return PhotonModelPersistor . load_optimum_pipe ( file , password ) @staticmethod def reload_hyperpipe ( results_folder , X , y , ** data_kwargs ): res_handler = ResultsHandler () res_handler . load_from_file ( os . path . join ( results_folder , \"photon_result_file.json\" )) loaded_optimum_pipe = Hyperpipe . load_optimum_pipe ( os . path . join ( results_folder , \"photon_best_model.photon\" )) new_hyperpipe = JsonTransformer () . from_json_file ( os . path . join ( results_folder , \"hyperpipe_config.json\" )) new_hyperpipe . results = res_handler . results new_hyperpipe . optimum_pipe = loaded_optimum_pipe new_hyperpipe . data = Hyperpipe . Data ( X , y , data_kwargs ) return new_hyperpipe def __repr__ ( self , ** kwargs ): \"\"\"Overwrite BaseEstimator's function to avoid errors when using Jupyter Notebooks.\"\"\" return \"Hyperpipe(name=' {} ')\" . format ( self . name ) __init__ ( self , name , inner_cv = None , outer_cv = None , optimizer = 'grid_search' , optimizer_params = None , metrics = None , best_config_metric = None , eval_final_performance = None , use_test_set = True , test_size = 0.2 , project_folder = '' , calculate_metrics_per_fold = True , calculate_metrics_across_folds = False , random_seed = None , verbosity = 0 , learning_curves = False , learning_curves_cut = None , output_settings = None , performance_constraints = None , permutation_id = None , cache_folder = None , nr_of_processes = 1 , allow_multidim_targets = False ) special Initialize the object. Parameters: Name Type Description Default name Optional[str] Name of hyperpipe instance. required inner_cv Union[sklearn.model_selection._split.BaseCrossValidator, sklearn.model_selection._split.BaseShuffleSplit, sklearn.model_selection._split._RepeatedSplits] Cross validation strategy to test hyperparameter configurations, generates the validation set. None outer_cv Union[sklearn.model_selection._split.BaseCrossValidator, sklearn.model_selection._split.BaseShuffleSplit, sklearn.model_selection._split._RepeatedSplits] Cross validation strategy to use for the hyperparameter search itself, generates the test set. None optimizer str Hyperparameter optimization algorithm. In case a string literal is given: \"grid_search\": Optimizer that iteratively tests all possible hyperparameter combinations. \"random_grid_search\": A variation of the grid search optimization that randomly picks hyperparameter combinations from all possible hyperparameter combinations. \"sk_opt\": Scikit-Optimize based on theories of bayesian optimization. \"random_search\": randomly chooses hyperparameter from grid-free domain. \"smac\": SMAC based on theories of bayesian optimization. \"nevergrad\": Nevergrad based on theories of evolutionary learning. In case an object is given: expects the object to have the following methods: ask : returns a hyperparameter configuration in form of an dictionary containing key->value pairs in the sklearn parameter encoding model_name__parameter_name: parameter_value prepare : takes a list of pipeline elements and their particular hyperparameters to prepare the hyperparameter space tell : gets a tested config and the respective performance in order to calculate a smart next configuration to process 'grid_search' metrics Optional[List[Union[Callable, keras.metrics.Metric, Type[keras.metrics.Metric], str]]] Metrics that should be calculated for both training, validation and test set Use the preimported metrics from sklearn and photonai, or register your own Metrics for classification : accuracy : sklearn.metrics.accuracy_score matthews_corrcoef : sklearn.metrics.matthews_corrcoef confusion_matrix : sklearn.metrics.confusion_matrix, f1_score : sklearn.metrics.f1_score hamming_loss : sklearn.metrics.hamming_loss log_loss : sklearn.metrics.log_loss precision : sklearn.metrics.precision_score recall : sklearn.metrics.recall_score Metrics for regression : mean_squared_error : sklearn.metrics.mean_squared_error mean_absolute_error : sklearn.metrics.mean_absolute_error explained_variance : sklearn.metrics.explained_variance_score r2 : sklearn.metrics.r2_score Other metrics pearson_correlation : photon_core.framework.Metrics.pearson_correlation variance_explained : photon_core.framework.Metrics.variance_explained_score categorical_accuracy : photon_core.framework.Metrics.categorical_accuracy_score None best_config_metric Union[Callable, keras.metrics.Metric, Type[keras.metrics.Metric], str] The metric that should be maximized or minimized in order to choose the best hyperparameter configuration. None eval_final_performance bool DEPRECATED! Use \"use_test_set\" instead! None use_test_set bool If the metrics should be calculated for the test set, otherwise the test set is seperated but not used. True project_folder str The output folder in which all files generated by the PHOTONAI project are saved to. '' test_size float The amount of the data that should be left out if no outer_cv is given and eval_final_performance is set to True. 0.2 calculate_metrics_per_fold bool If True, the metrics are calculated for each inner_fold. If False, calculate_metrics_across_folds must be True. True calculate_metrics_across_folds bool If True, the metrics are calculated across all inner_fold. If False, calculate_metrics_per_fold must be True. False random_seed int Random Seed. None verbosity int The level of verbosity, 0 is least talkative and gives only warn and error, 1 gives adds info and 2 adds debug. 0 learning_curves bool Enables learning curve procedure. Evaluate learning process over different sizes of input. Depends on learning_curves_cut. False learning_curves_cut FloatRange The tested relative cuts for data size. None performance_constraints list Objects that indicate whether a configuration should be tested further. For example, the inner fold of a config does not perform better than the dummy performance. None permutation_id str String identifier for permutation tests. None cache_folder str Folder path for multi-processing. None nr_of_processes int Determined the amount of simultaneous calculation of outer_folds. 1 allow_multidim_targets bool Allows multidimensional targets. False Source code in photonai/base/hyperpipe.py def __init__ ( self , name : Optional [ str ], inner_cv : Union [ BaseCrossValidator , BaseShuffleSplit , _RepeatedSplits ] = None , outer_cv : Union [ BaseCrossValidator , BaseShuffleSplit , _RepeatedSplits , None ] = None , optimizer : str = 'grid_search' , optimizer_params : dict = None , metrics : Optional [ List [ Union [ Scorer . Metric_Type , str ]]] = None , best_config_metric : Optional [ Union [ Scorer . Metric_Type , str ]] = None , eval_final_performance : bool = None , use_test_set : bool = True , test_size : float = 0.2 , project_folder : str = '' , calculate_metrics_per_fold : bool = True , calculate_metrics_across_folds : bool = False , random_seed : int = None , verbosity : int = 0 , learning_curves : bool = False , learning_curves_cut : FloatRange = None , output_settings : OutputSettings = None , performance_constraints : list = None , permutation_id : str = None , cache_folder : str = None , nr_of_processes : int = 1 , allow_multidim_targets : bool = False ): \"\"\" Initialize the object. Parameters: name: Name of hyperpipe instance. inner_cv: Cross validation strategy to test hyperparameter configurations, generates the validation set. outer_cv: Cross validation strategy to use for the hyperparameter search itself, generates the test set. optimizer: Hyperparameter optimization algorithm. - In case a string literal is given: - \"grid_search\": Optimizer that iteratively tests all possible hyperparameter combinations. - \"random_grid_search\": A variation of the grid search optimization that randomly picks hyperparameter combinations from all possible hyperparameter combinations. - \"sk_opt\": Scikit-Optimize based on theories of bayesian optimization. - \"random_search\": randomly chooses hyperparameter from grid-free domain. - \"smac\": SMAC based on theories of bayesian optimization. - \"nevergrad\": Nevergrad based on theories of evolutionary learning. - In case an object is given: expects the object to have the following methods: - `ask`: returns a hyperparameter configuration in form of an dictionary containing key->value pairs in the sklearn parameter encoding `model_name__parameter_name: parameter_value` - `prepare`: takes a list of pipeline elements and their particular hyperparameters to prepare the hyperparameter space - `tell`: gets a tested config and the respective performance in order to calculate a smart next configuration to process metrics: Metrics that should be calculated for both training, validation and test set Use the preimported metrics from sklearn and photonai, or register your own - Metrics for `classification`: - `accuracy`: sklearn.metrics.accuracy_score - `matthews_corrcoef`: sklearn.metrics.matthews_corrcoef - `confusion_matrix`: sklearn.metrics.confusion_matrix, - `f1_score`: sklearn.metrics.f1_score - `hamming_loss`: sklearn.metrics.hamming_loss - `log_loss`: sklearn.metrics.log_loss - `precision`: sklearn.metrics.precision_score - `recall`: sklearn.metrics.recall_score - Metrics for `regression`: - `mean_squared_error`: sklearn.metrics.mean_squared_error - `mean_absolute_error`: sklearn.metrics.mean_absolute_error - `explained_variance`: sklearn.metrics.explained_variance_score - `r2`: sklearn.metrics.r2_score - Other metrics - `pearson_correlation`: photon_core.framework.Metrics.pearson_correlation - `variance_explained`: photon_core.framework.Metrics.variance_explained_score - `categorical_accuracy`: photon_core.framework.Metrics.categorical_accuracy_score best_config_metric: The metric that should be maximized or minimized in order to choose the best hyperparameter configuration. eval_final_performance: DEPRECATED! Use \"use_test_set\" instead! use_test_set: If the metrics should be calculated for the test set, otherwise the test set is seperated but not used. project_folder: The output folder in which all files generated by the PHOTONAI project are saved to. test_size: The amount of the data that should be left out if no outer_cv is given and eval_final_performance is set to True. calculate_metrics_per_fold: If True, the metrics are calculated for each inner_fold. If False, calculate_metrics_across_folds must be True. calculate_metrics_across_folds: If True, the metrics are calculated across all inner_fold. If False, calculate_metrics_per_fold must be True. random_seed: Random Seed. verbosity: The level of verbosity, 0 is least talkative and gives only warn and error, 1 gives adds info and 2 adds debug. learning_curves: Enables learning curve procedure. Evaluate learning process over different sizes of input. Depends on learning_curves_cut. learning_curves_cut: The tested relative cuts for data size. performance_constraints: Objects that indicate whether a configuration should be tested further. For example, the inner fold of a config does not perform better than the dummy performance. permutation_id: String identifier for permutation tests. cache_folder: Folder path for multi-processing. nr_of_processes: Determined the amount of simultaneous calculation of outer_folds. allow_multidim_targets: Allows multidimensional targets. \"\"\" self . name = re . sub ( r '\\W+' , '' , name ) if eval_final_performance is not None : depr_warning = \"Hyperpipe parameter eval_final_performance is deprecated. It's called use_test_set now.\" use_test_set = eval_final_performance logger . warning ( depr_warning ) raise DeprecationWarning ( depr_warning ) # ====================== Cross Validation =========================== # check if both calculate_metrics_per_folds and calculate_metrics_across_folds is False if not calculate_metrics_across_folds and not calculate_metrics_per_fold : raise NotImplementedError ( \"Apparently, you've set calculate_metrics_across_folds=False and \" \"calculate_metrics_per_fold=False. In this case PHOTONAI does not calculate \" \"any metrics which doesn't make any sense. Set at least one to True.\" ) if inner_cv is None : msg = \"PHOTONAI requires an inner_cv split. Please enable inner cross-validation. \" \\ \"As exmaple: Hyperpipe(...inner_cv = KFold(n_splits = 3), ...). \" \\ \"Ensure you import the cross_validation object first.\" logger . error ( msg ) raise AttributeError ( msg ) # use default cut 'FloatRange(0, 1, 'range', 0.2)' if learning_curves = True but learning_curves_cut is None if learning_curves and learning_curves_cut is None : learning_curves_cut = FloatRange ( 0 , 1 , 'range' , 0.2 ) elif not learning_curves and learning_curves_cut is not None : learning_curves_cut = None self . cross_validation = Hyperpipe . CrossValidation ( inner_cv = inner_cv , outer_cv = outer_cv , use_test_set = use_test_set , test_size = test_size , calculate_metrics_per_fold = calculate_metrics_per_fold , calculate_metrics_across_folds = calculate_metrics_across_folds , learning_curves = learning_curves , learning_curves_cut = learning_curves_cut ) # ====================== Data =========================== self . data = Hyperpipe . Data ( allow_multidim_targets = allow_multidim_targets ) # ====================== Output Folder and Log File Management =========================== if output_settings : self . output_settings = output_settings else : self . output_settings = OutputSettings () if project_folder == '' : self . project_folder = os . getcwd () else : self . project_folder = project_folder self . output_settings . set_project_folder ( self . project_folder ) # update output options to add pipe name and timestamp to results folder self . _verbosity = 0 self . verbosity = verbosity self . output_settings . set_log_file () # ====================== Result Logging =========================== self . results_handler = None self . results = None self . best_config = None # ====================== Pipeline =========================== self . elements = [] self . _pipe = None self . optimum_pipe = None self . preprocessing = None # ====================== Performance Optimization =========================== if optimizer_params is None : optimizer_params = {} self . optimization = Optimization ( metrics = metrics , best_config_metric = best_config_metric , optimizer_input = optimizer , optimizer_params = optimizer_params , performance_constraints = performance_constraints ) # self.optimization.sanity_check_metrics() # ====================== Caching and Parallelization =========================== self . nr_of_processes = nr_of_processes if cache_folder : self . cache_folder = os . path . join ( cache_folder , self . name ) else : self . cache_folder = None # ====================== Internals =========================== self . permutation_id = permutation_id self . allow_multidim_targets = allow_multidim_targets self . is_final_fit = False # ====================== Random Seed =========================== self . random_state = random_seed if random_seed is not None : import random random . seed ( random_seed ) add ( self , pipe_element ) Add an element to the machine learning pipeline. Returns self. Parameters: Name Type Description Default pipe_element PipelineElement The object to add to the machine learning pipeline, being either a transformer or an estimator. required Source code in photonai/base/hyperpipe.py def add ( self , pipe_element : PipelineElement ): \"\"\" Add an element to the machine learning pipeline. Returns self. Parameters: pipe_element: The object to add to the machine learning pipeline, being either a transformer or an estimator. \"\"\" self . __iadd__ ( pipe_element ) copy_me ( self ) Helper function to copy an entire Hyperpipe Returns: Type Description Hyperpipe Source code in photonai/base/hyperpipe.py def copy_me ( self ): \"\"\" Helper function to copy an entire Hyperpipe Returns: Hyperpipe \"\"\" signature = inspect . getfullargspec ( OutputSettings . __init__ )[ 0 ] settings = OutputSettings () for attr in signature : if hasattr ( self . output_settings , attr ): setattr ( settings , attr , getattr ( self . output_settings , attr )) self . output_settings . initialize_log_file () # create new Hyperpipe instance pipe_copy = Hyperpipe ( name = self . name , inner_cv = deepcopy ( self . cross_validation . inner_cv ), outer_cv = deepcopy ( self . cross_validation . outer_cv ), best_config_metric = self . optimization . best_config_metric , metrics = self . optimization . metrics , optimizer = self . optimization . optimizer_input_str , optimizer_params = self . optimization . optimizer_params , project_folder = self . project_folder , output_settings = settings ) signature = inspect . getfullargspec ( self . __init__ )[ 0 ] for attr in signature : if hasattr ( self , attr ) and attr != 'output_settings' : setattr ( pipe_copy , attr , getattr ( self , attr )) if hasattr ( self , 'preprocessing' ) and self . preprocessing : preprocessing = Preprocessing () for element in self . preprocessing . elements : preprocessing += element . copy_me () pipe_copy += preprocessing if hasattr ( self , 'elements' ): for element in self . elements : pipe_copy += element . copy_me () return pipe_copy fit ( self , data , targets , ** kwargs ) Starts the hyperparameter search and/or fits the pipeline to the data and targets. Manages the nested cross validated hyperparameter search: Filters the data according to filter strategy (1) and according to the imbalanced_data_strategy (2) requests new configurations from the hyperparameter search strategy, the optimizer, initializes the testing of a specific configuration, communicates the result to the optimizer, repeats 2-4 until optimizer delivers no more configurations to test finally searches for the best config in all tested configs, trains the pipeline with the best config and evaluates the performance on the test set Parameters: Name Type Description Default data ndarray The array-like training and test data with shape=[N, D], where N is the number of samples and D is the number of features. required targets ndarray The truth array-like values with shape=[N], where N is the number of samples. required **kwargs Keyword arguments, passed to Outer_Fold_Manager.fit. {} Returns: Type Description Fitted Hyperpipe. Source code in photonai/base/hyperpipe.py def fit ( self , data : np . ndarray , targets : np . ndarray , ** kwargs ): \"\"\" Starts the hyperparameter search and/or fits the pipeline to the data and targets. Manages the nested cross validated hyperparameter search: 1. Filters the data according to filter strategy (1) and according to the imbalanced_data_strategy (2) 2. requests new configurations from the hyperparameter search strategy, the optimizer, 3. initializes the testing of a specific configuration, 4. communicates the result to the optimizer, 5. repeats 2-4 until optimizer delivers no more configurations to test 6. finally searches for the best config in all tested configs, 7. trains the pipeline with the best config and evaluates the performance on the test set Parameters: data: The array-like training and test data with shape=[N, D], where N is the number of samples and D is the number of features. targets: The truth array-like values with shape=[N], where N is the number of samples. **kwargs: Keyword arguments, passed to Outer_Fold_Manager.fit. Returns: Fitted Hyperpipe. \"\"\" # switch to result output folder start = datetime . datetime . now () self . output_settings . update_settings ( self . name , start . strftime ( \"%Y-%m- %d _%H-%M-%S\" )) logger . photon_system_log ( '=' * 101 ) logger . photon_system_log ( 'PHOTONAI ANALYSIS: ' + self . name ) logger . photon_system_log ( '=' * 101 ) logger . info ( \"Preparing data and PHOTONAI objects for analysis...\" ) # loop over outer cross validation if self . nr_of_processes > 1 : hyperpipe_client = Client ( threads_per_worker = 1 , n_workers = self . nr_of_processes , processes = False ) try : # check data self . data . input_data_sanity_checks ( data , targets , ** kwargs ) # create photon pipeline self . _prepare_pipeline () # initialize the progress monitors self . _prepare_result_logging ( start ) # apply preprocessing self . preprocess_data () if not self . is_final_fit : # Outer Folds outer_folds = FoldInfo . generate_folds ( self . cross_validation . outer_cv , self . data . X , self . data . y , self . data . kwargs , self . cross_validation . use_test_set , self . cross_validation . test_size ) self . cross_validation . outer_folds = { f . fold_id : f for f in outer_folds } delayed_jobs = [] # Run Dummy Estimator dummy_estimator = self . _prepare_dummy_estimator () if self . cache_folder is not None : logger . info ( \"Removing cache files...\" ) CacheManager . clear_cache_files ( self . cache_folder , force_all = True ) # loop over outer cross validation for i , outer_f in enumerate ( outer_folds ): # 1. generate OuterFolds Object outer_fold = MDBOuterFold ( fold_nr = outer_f . fold_nr ) outer_fold_computer = OuterFoldManager ( self . _pipe , self . optimization , outer_f . fold_id , self . cross_validation , cache_folder = self . cache_folder , cache_updater = self . recursive_cache_folder_propagation , dummy_estimator = dummy_estimator , result_obj = outer_fold ) # 2. monitor outputs self . results . outer_folds . append ( outer_fold ) if self . nr_of_processes > 1 : result = dask . delayed ( Hyperpipe . fit_outer_folds )( outer_fold_computer , self . data . X , self . data . y , self . data . kwargs ) delayed_jobs . append ( result ) else : try : # 3. fit outer_fold_computer . fit ( self . data . X , self . data . y , ** self . data . kwargs ) # 4. save outer fold results self . results_handler . save () finally : # 5. clear cache CacheManager . clear_cache_files ( self . cache_folder ) if self . nr_of_processes > 1 : dask . compute ( * delayed_jobs ) self . results_handler . save () # evaluate hyperparameter optimization results for best config self . _finalize_optimization () # clear complete cache ? use self.cache_folder to delete all subfolders within the parent cache folder # directory CacheManager . clear_cache_files ( self . cache_folder , force_all = True ) ############################################################################################### else : self . preprocess_data () self . _pipe . fit ( self . data . X , self . data . y , ** kwargs ) except Exception as e : logger . error ( e ) logger . error ( traceback . format_exc ()) traceback . print_exc () raise e finally : if self . nr_of_processes > 1 : hyperpipe_client . close () return self get_permutation_feature_importances ( self , ** kwargs ) Fits a model for the best config of each outer fold (using the training data of that fold). Then calls sklearn.inspection.permutation_importance with the test data and the given kwargs (e.g. n_repeats). Returns mean of \"importances_mean\" and of \"importances_std\" of all outer folds. Parameters: Name Type Description Default **kwargs Keyword arguments, passed to sklearn.permutation_importance. {} Returns: Type Description Dictionary with average of \"mean\" and \"std\" for all outer folds, respectively. Source code in photonai/base/hyperpipe.py def get_permutation_feature_importances ( self , ** kwargs ): \"\"\" Fits a model for the best config of each outer fold (using the training data of that fold). Then calls sklearn.inspection.permutation_importance with the test data and the given kwargs (e.g. n_repeats). Returns mean of \"importances_mean\" and of \"importances_std\" of all outer folds. Parameters: **kwargs: Keyword arguments, passed to sklearn.permutation_importance. Returns: Dictionary with average of \"mean\" and \"std\" for all outer folds, respectively. \"\"\" logger . photon_system_log ( \"\" ) logger . photon_system_log ( \"Computing permutation importances. This may take a while.\" ) logger . stars () if self . optimum_pipe is None : raise ValueError ( \"Cannot calculate permutation importances when optimum_pipe is None (probably the \" \"training and optimization procedure failed)\" ) importance_list = self . _calculate_permutation_importances ( ** kwargs ) mean_importances = np . mean ( np . array ( importance_list [ \"mean\" ]), axis = 0 ) std_importances = np . mean ( np . array ( importance_list [ \"std\" ]), axis = 0 ) logger . stars () return { 'mean' : mean_importances , 'std' : std_importances } inverse_transform_pipeline ( self , hyperparameters , data , targets , data_to_inverse ) Inverse transform data for a pipeline with specific hyperparameter configuration. Copy Sklearn Pipeline, Set Parameters Fit Pipeline to data and targets Inverse transform data with that pipeline Parameters: Name Type Description Default hyperparameters dict The concrete configuration settings for the pipeline elements. required data ndarray The training data to which the pipeline is fitted. required targets ndarray The truth values for training. required data_to_inverse ndarray The data that should be inversed after training. required Returns: Type Description ndarray Inverse data as array. Source code in photonai/base/hyperpipe.py def inverse_transform_pipeline ( self , hyperparameters : dict , data : np . ndarray , targets : np . ndarray , data_to_inverse : np . ndarray ) -> np . ndarray : \"\"\" Inverse transform data for a pipeline with specific hyperparameter configuration. 1. Copy Sklearn Pipeline, 2. Set Parameters 3. Fit Pipeline to data and targets 4. Inverse transform data with that pipeline Parameters: hyperparameters: The concrete configuration settings for the pipeline elements. data: The training data to which the pipeline is fitted. targets: The truth values for training. data_to_inverse: The data that should be inversed after training. Returns: Inverse data as array. \"\"\" copied_pipe = self . pipe . copy_me () copied_pipe . set_params ( ** hyperparameters ) copied_pipe . fit ( data , targets ) return copied_pipe . inverse_transform ( data_to_inverse ) load_optimum_pipe ( file , password = None ) staticmethod Load optimum pipe from file. As staticmethod, instantiation is thus not required. Called backend: PhotonModelPersistor.load_optimum_pipe. Parameters: Name Type Description Default file str File path specifying .photon file to load trained pipeline from zipped file. required password str Passcode for read file. None Returns: Type Description PhotonPipeline Returns pipeline with all trained PipelineElements. Source code in photonai/base/hyperpipe.py @staticmethod def load_optimum_pipe ( file : str , password : str = None ) -> PhotonPipeline : \"\"\" Load optimum pipe from file. As staticmethod, instantiation is thus not required. Called backend: PhotonModelPersistor.load_optimum_pipe. Parameters: file: File path specifying .photon file to load trained pipeline from zipped file. password: Passcode for read file. Returns: Returns pipeline with all trained PipelineElements. \"\"\" return PhotonModelPersistor . load_optimum_pipe ( file , password ) predict ( self , data , ** kwargs ) Use the optimum pipe to predict the input data. Parameters: Name Type Description Default data ndarray The array-like prediction data with shape=[M, D], where M is the number of samples and D is the number of features. D must correspond to the number of trained dimensions of the fit method. required **kwargs Keyword arguments, passed to optimum_pipe.predict. {} Returns: Type Description ndarray Predicted targets calculated on input data with trained model. Source code in photonai/base/hyperpipe.py def predict ( self , data : np . ndarray , ** kwargs ) -> np . ndarray : \"\"\" Use the optimum pipe to predict the input data. Parameters: data: The array-like prediction data with shape=[M, D], where M is the number of samples and D is the number of features. D must correspond to the number of trained dimensions of the fit method. **kwargs: Keyword arguments, passed to optimum_pipe.predict. Returns: Predicted targets calculated on input data with trained model. \"\"\" # Todo: if local_search = true then use optimized pipe here? if self . _pipe : return self . optimum_pipe . predict ( data , ** kwargs ) predict_proba ( self , data , ** kwargs ) Use the optimum pipe to predict the probabilities from the input data. Parameters: Name Type Description Default data ndarray The array-like prediction data with shape=[M, D], where M is the number of samples and D is the number of features. D must correspond to the number of trained dimensions of the fit method. required **kwargs Keyword arguments, passed to optimum_pipe.predict_proba. {} Returns: Type Description ndarray Probabilities calculated from input data on fitted model. Source code in photonai/base/hyperpipe.py def predict_proba ( self , data : np . ndarray , ** kwargs ) -> np . ndarray : \"\"\" Use the optimum pipe to predict the probabilities from the input data. Parameters: data: The array-like prediction data with shape=[M, D], where M is the number of samples and D is the number of features. D must correspond to the number of trained dimensions of the fit method. **kwargs: Keyword arguments, passed to optimum_pipe.predict_proba. Returns: Probabilities calculated from input data on fitted model. \"\"\" if self . _pipe : return self . optimum_pipe . predict_proba ( data , ** kwargs )","title":"Hyperpipe"},{"location":"api/base/hyperpipe/#documentation-for-hyperpipe","text":"The PHOTONAI Hyperpipe class creates a custom machine learning pipeline. In addition it defines the relevant analysis\u2019 parameters such as the cross-validation scheme, the hyperparameter optimization strategy, and the performance metrics of interest. So called PHOTONAI PipelineElements can be added to the Hyperpipe, each of them being a data-processing method or a learning algorithm. By choosing and combining data-processing methods or algorithms, and arranging them with the PHOTONAI classes, simple and complex pipeline architectures can be designed rapidly. The PHOTONAI Hyperpipe automatizes the nested training, test and hyperparameter optimization procedures. The Hyperpipe monitors: the nested-cross-validated training and test procedure, communicates with the hyperparameter optimization strategy, streams information between the pipeline elements, logs all results obtained and evaluates the performance, guides the hyperparameter optimization process by a so-called best config metric which is used to select the best performing hyperparameter configuration. Attributes: Name Type Description optimum_pipe PhotonPipeline An sklearn pipeline object that is fitted to the training data according to the best hyperparameter configuration found. Currently, we don't create an ensemble of all best hyperparameter configs over all folds. We find the best config by comparing the test error across outer folds. The hyperparameter config of the best fold is used as the optimal model and is then trained on the complete set. best_config dict Dictionary containing the hyperparameters of the best configuration. Contains the parameters in the sklearn interface of model_name__parameter_name: parameter value. results MDBHyperpipe Object containing all information about the for the performed hyperparameter search. Holds the training and test metrics for all outer folds, inner folds and configurations, as well as additional information. elements list Contains `all PipelineElement or Hyperpipe objects that are added to the pipeline. Examples: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 from photonai.base import Hyperpipe , PipelineElement from photonai.optimization import FloatRange from sklearn.model_selection import ShuffleSplit , KFold from sklearn.datasets import load_breast_cancer hyperpipe = Hyperpipe ( 'myPipe' , optimizer = 'random_grid_search' , optimizer_params = { 'limit_in_minutes' : 5 }, outer_cv = ShuffleSplit ( test_size = 0.2 , n_splits = 3 ), inner_cv = KFold ( n_splits = 10 , shuffle = True ), metrics = [ 'accuracy' , 'precision' , 'recall' , \"f1_score\" ], best_config_metric = 'accuracy' , eval_final_performance = True , verbosity = 0 ) hyperpipe += PipelineElement ( \"SVC\" , hyperparameters = { \"C\" : FloatRange ( 1 , 100 )}) X , y = load_breast_cancer ( return_X_y = True ) hyperpipe . fit ( X , y ) Source code in photonai/base/hyperpipe.py class Hyperpipe ( BaseEstimator ): \"\"\"The PHOTONAI Hyperpipe class creates a custom machine learning pipeline. In addition it defines the relevant analysis\u2019 parameters such as the cross-validation scheme, the hyperparameter optimization strategy, and the performance metrics of interest. So called PHOTONAI PipelineElements can be added to the Hyperpipe, each of them being a data-processing method or a learning algorithm. By choosing and combining data-processing methods or algorithms, and arranging them with the PHOTONAI classes, simple and complex pipeline architectures can be designed rapidly. The PHOTONAI Hyperpipe automatizes the nested training, test and hyperparameter optimization procedures. The Hyperpipe monitors: - the nested-cross-validated training and test procedure, - communicates with the hyperparameter optimization strategy, - streams information between the pipeline elements, - logs all results obtained and evaluates the performance, - guides the hyperparameter optimization process by a so-called best config metric which is used to select the best performing hyperparameter configuration. Attributes: optimum_pipe (PhotonPipeline): An sklearn pipeline object that is fitted to the training data according to the best hyperparameter configuration found. Currently, we don't create an ensemble of all best hyperparameter configs over all folds. We find the best config by comparing the test error across outer folds. The hyperparameter config of the best fold is used as the optimal model and is then trained on the complete set. best_config (dict): Dictionary containing the hyperparameters of the best configuration. Contains the parameters in the sklearn interface of model_name__parameter_name: parameter value. results (MDBHyperpipe): Object containing all information about the for the performed hyperparameter search. Holds the training and test metrics for all outer folds, inner folds and configurations, as well as additional information. elements (list): Contains `all PipelineElement or Hyperpipe objects that are added to the pipeline. Example: ``` python from photonai.base import Hyperpipe, PipelineElement from photonai.optimization import FloatRange from sklearn.model_selection import ShuffleSplit, KFold from sklearn.datasets import load_breast_cancer hyperpipe = Hyperpipe('myPipe', optimizer='random_grid_search', optimizer_params={'limit_in_minutes': 5}, outer_cv=ShuffleSplit(test_size=0.2, n_splits=3), inner_cv=KFold(n_splits=10, shuffle=True), metrics=['accuracy', 'precision', 'recall', \"f1_score\"], best_config_metric='accuracy', eval_final_performance=True, verbosity=0) hyperpipe += PipelineElement(\"SVC\", hyperparameters={\"C\": FloatRange(1, 100)}) X, y = load_breast_cancer(return_X_y=True) hyperpipe.fit(X, y) ``` \"\"\" def __init__ ( self , name : Optional [ str ], inner_cv : Union [ BaseCrossValidator , BaseShuffleSplit , _RepeatedSplits ] = None , outer_cv : Union [ BaseCrossValidator , BaseShuffleSplit , _RepeatedSplits , None ] = None , optimizer : str = 'grid_search' , optimizer_params : dict = None , metrics : Optional [ List [ Union [ Scorer . Metric_Type , str ]]] = None , best_config_metric : Optional [ Union [ Scorer . Metric_Type , str ]] = None , eval_final_performance : bool = None , use_test_set : bool = True , test_size : float = 0.2 , project_folder : str = '' , calculate_metrics_per_fold : bool = True , calculate_metrics_across_folds : bool = False , random_seed : int = None , verbosity : int = 0 , learning_curves : bool = False , learning_curves_cut : FloatRange = None , output_settings : OutputSettings = None , performance_constraints : list = None , permutation_id : str = None , cache_folder : str = None , nr_of_processes : int = 1 , allow_multidim_targets : bool = False ): \"\"\" Initialize the object. Parameters: name: Name of hyperpipe instance. inner_cv: Cross validation strategy to test hyperparameter configurations, generates the validation set. outer_cv: Cross validation strategy to use for the hyperparameter search itself, generates the test set. optimizer: Hyperparameter optimization algorithm. - In case a string literal is given: - \"grid_search\": Optimizer that iteratively tests all possible hyperparameter combinations. - \"random_grid_search\": A variation of the grid search optimization that randomly picks hyperparameter combinations from all possible hyperparameter combinations. - \"sk_opt\": Scikit-Optimize based on theories of bayesian optimization. - \"random_search\": randomly chooses hyperparameter from grid-free domain. - \"smac\": SMAC based on theories of bayesian optimization. - \"nevergrad\": Nevergrad based on theories of evolutionary learning. - In case an object is given: expects the object to have the following methods: - `ask`: returns a hyperparameter configuration in form of an dictionary containing key->value pairs in the sklearn parameter encoding `model_name__parameter_name: parameter_value` - `prepare`: takes a list of pipeline elements and their particular hyperparameters to prepare the hyperparameter space - `tell`: gets a tested config and the respective performance in order to calculate a smart next configuration to process metrics: Metrics that should be calculated for both training, validation and test set Use the preimported metrics from sklearn and photonai, or register your own - Metrics for `classification`: - `accuracy`: sklearn.metrics.accuracy_score - `matthews_corrcoef`: sklearn.metrics.matthews_corrcoef - `confusion_matrix`: sklearn.metrics.confusion_matrix, - `f1_score`: sklearn.metrics.f1_score - `hamming_loss`: sklearn.metrics.hamming_loss - `log_loss`: sklearn.metrics.log_loss - `precision`: sklearn.metrics.precision_score - `recall`: sklearn.metrics.recall_score - Metrics for `regression`: - `mean_squared_error`: sklearn.metrics.mean_squared_error - `mean_absolute_error`: sklearn.metrics.mean_absolute_error - `explained_variance`: sklearn.metrics.explained_variance_score - `r2`: sklearn.metrics.r2_score - Other metrics - `pearson_correlation`: photon_core.framework.Metrics.pearson_correlation - `variance_explained`: photon_core.framework.Metrics.variance_explained_score - `categorical_accuracy`: photon_core.framework.Metrics.categorical_accuracy_score best_config_metric: The metric that should be maximized or minimized in order to choose the best hyperparameter configuration. eval_final_performance: DEPRECATED! Use \"use_test_set\" instead! use_test_set: If the metrics should be calculated for the test set, otherwise the test set is seperated but not used. project_folder: The output folder in which all files generated by the PHOTONAI project are saved to. test_size: The amount of the data that should be left out if no outer_cv is given and eval_final_performance is set to True. calculate_metrics_per_fold: If True, the metrics are calculated for each inner_fold. If False, calculate_metrics_across_folds must be True. calculate_metrics_across_folds: If True, the metrics are calculated across all inner_fold. If False, calculate_metrics_per_fold must be True. random_seed: Random Seed. verbosity: The level of verbosity, 0 is least talkative and gives only warn and error, 1 gives adds info and 2 adds debug. learning_curves: Enables learning curve procedure. Evaluate learning process over different sizes of input. Depends on learning_curves_cut. learning_curves_cut: The tested relative cuts for data size. performance_constraints: Objects that indicate whether a configuration should be tested further. For example, the inner fold of a config does not perform better than the dummy performance. permutation_id: String identifier for permutation tests. cache_folder: Folder path for multi-processing. nr_of_processes: Determined the amount of simultaneous calculation of outer_folds. allow_multidim_targets: Allows multidimensional targets. \"\"\" self . name = re . sub ( r '\\W+' , '' , name ) if eval_final_performance is not None : depr_warning = \"Hyperpipe parameter eval_final_performance is deprecated. It's called use_test_set now.\" use_test_set = eval_final_performance logger . warning ( depr_warning ) raise DeprecationWarning ( depr_warning ) # ====================== Cross Validation =========================== # check if both calculate_metrics_per_folds and calculate_metrics_across_folds is False if not calculate_metrics_across_folds and not calculate_metrics_per_fold : raise NotImplementedError ( \"Apparently, you've set calculate_metrics_across_folds=False and \" \"calculate_metrics_per_fold=False. In this case PHOTONAI does not calculate \" \"any metrics which doesn't make any sense. Set at least one to True.\" ) if inner_cv is None : msg = \"PHOTONAI requires an inner_cv split. Please enable inner cross-validation. \" \\ \"As exmaple: Hyperpipe(...inner_cv = KFold(n_splits = 3), ...). \" \\ \"Ensure you import the cross_validation object first.\" logger . error ( msg ) raise AttributeError ( msg ) # use default cut 'FloatRange(0, 1, 'range', 0.2)' if learning_curves = True but learning_curves_cut is None if learning_curves and learning_curves_cut is None : learning_curves_cut = FloatRange ( 0 , 1 , 'range' , 0.2 ) elif not learning_curves and learning_curves_cut is not None : learning_curves_cut = None self . cross_validation = Hyperpipe . CrossValidation ( inner_cv = inner_cv , outer_cv = outer_cv , use_test_set = use_test_set , test_size = test_size , calculate_metrics_per_fold = calculate_metrics_per_fold , calculate_metrics_across_folds = calculate_metrics_across_folds , learning_curves = learning_curves , learning_curves_cut = learning_curves_cut ) # ====================== Data =========================== self . data = Hyperpipe . Data ( allow_multidim_targets = allow_multidim_targets ) # ====================== Output Folder and Log File Management =========================== if output_settings : self . output_settings = output_settings else : self . output_settings = OutputSettings () if project_folder == '' : self . project_folder = os . getcwd () else : self . project_folder = project_folder self . output_settings . set_project_folder ( self . project_folder ) # update output options to add pipe name and timestamp to results folder self . _verbosity = 0 self . verbosity = verbosity self . output_settings . set_log_file () # ====================== Result Logging =========================== self . results_handler = None self . results = None self . best_config = None # ====================== Pipeline =========================== self . elements = [] self . _pipe = None self . optimum_pipe = None self . preprocessing = None # ====================== Performance Optimization =========================== if optimizer_params is None : optimizer_params = {} self . optimization = Optimization ( metrics = metrics , best_config_metric = best_config_metric , optimizer_input = optimizer , optimizer_params = optimizer_params , performance_constraints = performance_constraints ) # self.optimization.sanity_check_metrics() # ====================== Caching and Parallelization =========================== self . nr_of_processes = nr_of_processes if cache_folder : self . cache_folder = os . path . join ( cache_folder , self . name ) else : self . cache_folder = None # ====================== Internals =========================== self . permutation_id = permutation_id self . allow_multidim_targets = allow_multidim_targets self . is_final_fit = False # ====================== Random Seed =========================== self . random_state = random_seed if random_seed is not None : import random random . seed ( random_seed ) # =================================================================== # Helper Classes # =================================================================== class CrossValidation : def __init__ ( self , inner_cv , outer_cv , use_test_set , test_size , calculate_metrics_per_fold , calculate_metrics_across_folds , learning_curves , learning_curves_cut ): self . inner_cv = inner_cv self . outer_cv = outer_cv self . use_test_set = use_test_set self . test_size = test_size self . learning_curves = learning_curves self . learning_curves_cut = learning_curves_cut self . calculate_metrics_per_fold = calculate_metrics_per_fold # Todo: if self.outer_cv is LeaveOneOut: Set calculate metrics across folds to True -> Print self . calculate_metrics_across_folds = calculate_metrics_across_folds self . outer_folds = None self . inner_folds = dict () def __str__ ( self ): return \"Hyperpipe {} \" . format ( self . name ) class Data : def __init__ ( self , X = None , y = None , kwargs = None , allow_multidim_targets = False ): self . X = X self . y = y self . kwargs = kwargs self . allow_multidim_targets = allow_multidim_targets def input_data_sanity_checks ( self , data , targets , ** kwargs ): # ==================== SANITY CHECKS =============================== # 1. Make to numpy arrays # 2. erase all Nan targets logger . info ( \"Checking input data...\" ) self . X = data self . y = targets self . kwargs = kwargs try : if self . X is None : raise ValueError ( \"(Input-)data is a NoneType.\" ) if self . y is None : raise ValueError ( \"(Input-)target is a NoneType.\" ) shape_x = np . shape ( self . X ) shape_y = np . shape ( self . y ) if not self . allow_multidim_targets : if len ( shape_y ) != 1 : if len ( np . shape ( np . squeeze ( self . y ))) == 1 : # use np.squeeze for non 1D targets. self . y = np . squeeze ( self . y ) shape_y = np . shape ( self . y ) msg = \"y has been automatically squeezed. If this is not your intention, block this \" \\ \"with Hyperpipe(allow_multidim_targets = True\" logger . warning ( msg ) warnings . warn ( msg ) else : raise ValueError ( \"Target is not one-dimensional. Multidimensional targets can cause problems\" \"with sklearn metrics. Please override with \" \"Hyperpipe(allow_multidim_targets = True).\" ) if not shape_x [ 0 ] == shape_y [ 0 ]: raise IndexError ( \"Size of targets mismatch to size of the data: \" + str ( shape_x [ 0 ]) + \" - \" + str ( shape_y [ 0 ])) except IndexError as ie : logger . error ( \"IndexError: \" + str ( ie )) raise ie except ValueError as ve : logger . error ( \"ValueError: \" + str ( ve )) raise ve except Exception as e : logger . error ( \"Error: \" + str ( e )) raise e # be compatible to list of (image-) files if isinstance ( self . X , list ): self . X = np . asarray ( self . X ) elif isinstance ( self . X , ( pd . DataFrame , pd . Series )): self . X = self . X . to_numpy () if isinstance ( self . y , list ): self . y = np . asarray ( self . y ) elif isinstance ( self . y , pd . Series ) or isinstance ( self . y , pd . DataFrame ): self . y = self . y . to_numpy () # at first first, erase all rows where y is Nan if preprocessing has not done it already try : nans_in_y = np . isnan ( self . y ) nr_of_nans = len ( np . where ( nans_in_y == 1 )[ 0 ]) if nr_of_nans > 0 : logger . info ( \"You have {} Nans in your target vector, \" \"PHOTONAI erases every data item that has a Nan Target\" . format ( str ( nr_of_nans ))) self . X = self . X [ ~ nans_in_y ] self . y = self . y [ ~ nans_in_y ] new_kwargs = dict () for name , element_list in kwargs . items (): new_kwargs [ name ] = element_list [ ~ nans_in_y ] self . kwargs = new_kwargs except Exception as e : # This is only for convenience so if it fails then never mind logger . error ( \"Removing Nans in target vector failed: \" + str ( e )) pass logger . info ( \"Running analysis with \" + str ( self . y . shape [ 0 ]) + \" samples.\" ) # =================================================================== # Properties and Helper # =================================================================== @property def estimation_type ( self ): estimation_type = getattr ( self . elements [ - 1 ], '_estimator_type' ) if estimation_type is None : raise NotImplementedError ( \"Last element in Hyperpipe should be an estimator.\" ) else : return estimation_type @property def verbosity ( self ): return self . _verbosity @verbosity . setter def verbosity ( self , value ): self . _verbosity = value self . output_settings . verbosity = self . _verbosity self . output_settings . set_log_level () @staticmethod def disable_multiprocessing_recursively ( pipe ): if isinstance ( pipe , ( Stack , Branch , Switch , Preprocessing )): if hasattr ( pipe , 'nr_of_processes' ): pipe . nr_of_processes = 1 for child in pipe . elements : if isinstance ( child , Branch ): Hyperpipe . disable_multiprocessing_recursively ( child ) elif hasattr ( child , 'base_element' ): Hyperpipe . disable_multiprocessing_recursively ( child . base_element ) elif isinstance ( pipe , PhotonPipeline ): for name , child in pipe . named_steps . items (): Hyperpipe . disable_multiprocessing_recursively ( child ) else : if hasattr ( pipe , 'nr_of_processes' ): pipe . nr_of_processes = 1 @staticmethod def recursive_cache_folder_propagation ( element , cache_folder , inner_fold_id ): if isinstance ( element , ( Switch , Stack , Preprocessing )): for child in element . elements : Hyperpipe . recursive_cache_folder_propagation ( child , cache_folder , inner_fold_id ) elif isinstance ( element , Branch ): # in case it's a Branch, we create a cache subfolder and propagate it to every child if cache_folder : cache_folder = os . path . join ( cache_folder , element . name ) Hyperpipe . recursive_cache_folder_propagation ( element . base_element , cache_folder , inner_fold_id ) # Hyperpipe.prepare_caching(element.base_element.cache_folder) elif isinstance ( element , PhotonPipeline ): element . fold_id = inner_fold_id element . cache_folder = cache_folder # pipe.caching is automatically set to True or False by .cache_folder setter for name , child in element . named_steps . items (): # we need to check if any element is Branch, Stack or Swtich Hyperpipe . recursive_cache_folder_propagation ( child , cache_folder , inner_fold_id ) # else: if it's a simple PipelineElement, then we just don't do anything # =================================================================== # Pipeline Setup # =================================================================== def __iadd__ ( self , pipe_element : PipelineElement ): \"\"\" Add an element to the machine learning pipeline. Returns self. Parameters: pipe_element: The object to add to the machine learning pipeline, being either a transformer or an estimator. \"\"\" if isinstance ( pipe_element , Preprocessing ): self . preprocessing = pipe_element elif isinstance ( pipe_element , CallbackElement ): pipe_element . needs_y = True self . elements . append ( pipe_element ) else : if isinstance ( pipe_element , PipelineElement ) or issubclass ( type ( pipe_element ), PhotonNative ): self . elements . append ( pipe_element ) else : raise TypeError ( \"Element must be of type Pipeline Element\" ) return self def add ( self , pipe_element : PipelineElement ): \"\"\" Add an element to the machine learning pipeline. Returns self. Parameters: pipe_element: The object to add to the machine learning pipeline, being either a transformer or an estimator. \"\"\" self . __iadd__ ( pipe_element ) # =================================================================== # Workflow Setup # =================================================================== def _prepare_dummy_estimator ( self ): self . results . dummy_estimator = MDBDummyResults () if self . estimation_type == 'regressor' : self . results . dummy_estimator . strategy = 'mean' return DummyRegressor ( strategy = self . results . dummy_estimator . strategy ) elif self . estimation_type == 'classifier' : self . results . dummy_estimator . strategy = 'most_frequent' return DummyClassifier ( strategy = self . results . dummy_estimator . strategy ) else : logger . info ( 'Estimator does not specify whether it is a regressor or classifier. ' 'DummyEstimator step skipped.' ) return def __get_pipeline_structure ( self , pipeline_elements ): element_list = dict () for p_el in pipeline_elements : if not hasattr ( p_el , 'name' ): raise Warning ( 'Strange Pipeline Element found that has no name..? Type: ' . format ( type ( p_el ))) if hasattr ( p_el , 'elements' ): child_list = self . __get_pipeline_structure ( p_el . elements ) identifier = p_el . name if hasattr ( p_el , \"identifier\" ): identifier = p_el . identifier + identifier element_list [ identifier ] = child_list else : if hasattr ( p_el , 'base_element' ): element_list [ p_el . name ] = str ( type ( p_el . base_element )) else : element_list [ p_el . name ] = str ( type ( p_el )) return element_list def _prepare_result_logging ( self , start_time ): self . results = MDBHyperpipe ( name = self . name , version = __version__ ) self . results . hyperpipe_info = MDBHyperpipeInfo () # in case eval final performance is false, we have no outer fold predictions if not self . cross_validation . use_test_set : self . output_settings . save_predictions_from_best_config_inner_folds = True self . results_handler = ResultsHandler ( self . results , self . output_settings ) self . results . computation_start_time = start_time self . results . hyperpipe_info . estimation_type = self . estimation_type self . results . output_folder = self . output_settings . results_folder if self . permutation_id is not None : self . results . permutation_id = self . permutation_id # save wizard information to PHOTONAI db in order to map results to the wizard design object if self . output_settings and hasattr ( self . output_settings , 'wizard_object_id' ): if self . output_settings . wizard_object_id : self . name = self . output_settings . wizard_object_id self . results . name = self . output_settings . wizard_object_id self . results . wizard_object_id = ObjectId ( self . output_settings . wizard_object_id ) self . results . wizard_system_name = self . output_settings . wizard_project_name self . results . user_id = self . output_settings . user_id self . results . outer_folds = [] self . results . hyperpipe_info . elements = self . __get_pipeline_structure ( self . elements ) self . results . hyperpipe_info . eval_final_performance = self . cross_validation . use_test_set self . results . hyperpipe_info . best_config_metric = self . optimization . best_config_metric self . results . hyperpipe_info . metrics = self . optimization . metrics self . results . hyperpipe_info . learning_curves_cut = self . cross_validation . learning_curves_cut self . results . hyperpipe_info . maximize_best_config_metric = self . optimization . maximize_metric # optimization def _format_cross_validation ( cv ): if cv : string = \" {} (\" . format ( cv . __class__ . __name__ ) for key , val in cv . __dict__ . items (): string += \" {} = {} , \" . format ( key , val ) return string [: - 2 ] + \")\" else : return \"None\" self . results . hyperpipe_info . cross_validation = \\ { 'OuterCV' : _format_cross_validation ( self . cross_validation . outer_cv ), 'InnerCV' : _format_cross_validation ( self . cross_validation . inner_cv )} self . results . hyperpipe_info . data = { 'X_shape' : self . data . X . shape , 'y_shape' : self . data . y . shape } self . results . hyperpipe_info . optimization = { 'Optimizer' : self . optimization . optimizer_input_str , 'OptimizerParams' : str ( self . optimization . optimizer_params ), 'BestConfigMetric' : self . optimization . best_config_metric } # add json file of hyperpipe attributes try : json_transformer = JsonTransformer () json_transformer . to_json_file ( self , self . output_settings . results_folder + \"/hyperpipe_config.json\" ) except : msg = \"JsonTransformer was unable to create the .json file.\" logger . warning ( msg ) warnings . warn ( msg ) def _finalize_optimization ( self ): # ==================== EVALUATING RESULTS OF HYPERPARAMETER OPTIMIZATION =============================== # 1. computing average metrics # 2. finding overall best config # 3. training model with best config # 4. persisting best model logger . clean_info ( '' ) logger . stars () logger . photon_system_log ( \"Finished all outer fold computations.\" ) logger . info ( \"Now analysing the final results...\" ) # computer dummy metrics logger . info ( \"Computing dummy metrics...\" ) config_item = MDBConfig () dummy_results = [ outer_fold . dummy_results for outer_fold in self . results . outer_folds ] config_item . inner_folds = [ f for f in dummy_results if f is not None ] if len ( config_item . inner_folds ) > 0 : self . results . dummy_estimator . metrics_train , self . results . dummy_estimator . metrics_test = \\ MDBHelper . aggregate_metrics_for_inner_folds ( config_item . inner_folds , self . optimization . metrics ) logger . info ( \"Computing mean and std for all outer fold metrics...\" ) # Compute all final metrics self . results . metrics_train , self . results . metrics_test = \\ MDBHelper . aggregate_metrics_for_outer_folds ( self . results . outer_folds , self . optimization . metrics ) # Find best config across outer folds logger . info ( \"Find best config across outer folds...\" ) best_config = self . optimization . get_optimum_config_outer_folds ( self . results . outer_folds ) self . best_config = best_config . config_dict self . results . best_config = best_config # save results again self . results . computation_end_time = datetime . datetime . now () self . results . computation_completed = True logger . info ( \"Save final results...\" ) self . results_handler . save () logger . info ( \"Prepare Hyperpipe.optimum pipe with best config..\" ) # set self to best config self . optimum_pipe = self . _pipe self . optimum_pipe . set_params ( ** self . best_config ) if self . output_settings . generate_best_model : logger . info ( \"Fitting best model...\" ) # set self to best config self . optimum_pipe = self . _pipe self . optimum_pipe . set_params ( ** self . best_config ) # set caching # we want caching disabled in general but still want to do single subject caching self . recursive_cache_folder_propagation ( self . optimum_pipe , self . cache_folder , 'fixed_fold_id' ) self . optimum_pipe . caching = False # disable multiprocessing when fitting optimum pipe # (otherwise inverse_transform won't work for BrainAtlas/Mask) self . disable_multiprocessing_recursively ( self . optimum_pipe ) self . optimum_pipe . fit ( self . data . X , self . data . y , ** self . data . kwargs ) # Before saving the optimum pipe, add preprocessing without multiprocessing self . disable_multiprocessing_recursively ( self . preprocessing ) self . optimum_pipe . add_preprocessing ( self . preprocessing ) # Now truly set to no caching (including single_subject_caching) self . recursive_cache_folder_propagation ( self . optimum_pipe , None , None ) if self . output_settings . save_output : try : pretrained_model_filename = os . path . join ( self . output_settings . results_folder , 'photon_best_model.photon' ) PhotonModelPersistor . save_optimum_pipe ( self . optimum_pipe , pretrained_model_filename ) logger . info ( \"Saved best model to file.\" ) except Exception as e : logger . info ( \"Could not save best model to file\" ) logger . error ( str ( e )) # get feature importances of optimum pipe logger . info ( \"Mapping back feature importances...\" ) feature_importances = self . optimum_pipe . feature_importances_ if not feature_importances : logger . info ( \"No feature importances available for {} !\" . format ( self . optimum_pipe . elements [ - 1 ][ 0 ])) else : self . results . best_config_feature_importances = feature_importances # write backmapping file only if optimum_pipes inverse_transform works completely. # restriction: only a faulty inverse_transform is considered, missing ones are further ignored. with warnings . catch_warnings ( record = True ) as w : # get backmapping backmapping , _ , _ = self . optimum_pipe . \\ inverse_transform ( np . array ( feature_importances ) . reshape ( 1 , - 1 ), None ) if not any ( \"The inverse transformation is not possible for\" in s for s in [ e . message . args [ 0 ] for e in w ]): # save backmapping self . results_handler . save_backmapping ( filename = 'optimum_pipe_feature_importances_backmapped' , backmapping = backmapping ) else : logger . info ( 'Could not save feature importance: backmapping NOT successful.' ) # save learning curves if self . cross_validation . learning_curves : self . results_handler . save_all_learning_curves () logger . info ( \"Summarizing results...\" ) logger . info ( \"Write predictions to files...\" ) # write all convenience files (summary, predictions_file and plots) self . results_handler . write_predictions_file () logger . info ( \"Write summary...\" ) logger . stars () logger . photon_system_log ( \"\" ) logger . photon_system_log ( self . results_handler . text_summary ()) def preprocess_data ( self ): # if there is a preprocessing pipeline, we apply it first. if self . preprocessing is not None : logger . info ( \"Applying preprocessing steps...\" ) self . preprocessing . fit ( self . data . X , self . data . y , ** self . data . kwargs ) self . data . X , self . data . y , self . data . kwargs = self . preprocessing . transform ( self . data . X , self . data . y , ** self . data . kwargs ) def _prepare_pipeline ( self ): self . _pipe = Branch . prepare_photon_pipe ( self . elements ) self . _pipe = Branch . sanity_check_pipeline ( self . _pipe ) if self . random_state : self . _pipe . random_state = self . random_state # =================================================================== # sklearn interfaces # =================================================================== @staticmethod def fit_outer_folds ( outer_fold_computer , X , y , kwargs ): outer_fold_computer . fit ( X , y , ** kwargs ) return def fit ( self , data : np . ndarray , targets : np . ndarray , ** kwargs ): \"\"\" Starts the hyperparameter search and/or fits the pipeline to the data and targets. Manages the nested cross validated hyperparameter search: 1. Filters the data according to filter strategy (1) and according to the imbalanced_data_strategy (2) 2. requests new configurations from the hyperparameter search strategy, the optimizer, 3. initializes the testing of a specific configuration, 4. communicates the result to the optimizer, 5. repeats 2-4 until optimizer delivers no more configurations to test 6. finally searches for the best config in all tested configs, 7. trains the pipeline with the best config and evaluates the performance on the test set Parameters: data: The array-like training and test data with shape=[N, D], where N is the number of samples and D is the number of features. targets: The truth array-like values with shape=[N], where N is the number of samples. **kwargs: Keyword arguments, passed to Outer_Fold_Manager.fit. Returns: Fitted Hyperpipe. \"\"\" # switch to result output folder start = datetime . datetime . now () self . output_settings . update_settings ( self . name , start . strftime ( \"%Y-%m- %d _%H-%M-%S\" )) logger . photon_system_log ( '=' * 101 ) logger . photon_system_log ( 'PHOTONAI ANALYSIS: ' + self . name ) logger . photon_system_log ( '=' * 101 ) logger . info ( \"Preparing data and PHOTONAI objects for analysis...\" ) # loop over outer cross validation if self . nr_of_processes > 1 : hyperpipe_client = Client ( threads_per_worker = 1 , n_workers = self . nr_of_processes , processes = False ) try : # check data self . data . input_data_sanity_checks ( data , targets , ** kwargs ) # create photon pipeline self . _prepare_pipeline () # initialize the progress monitors self . _prepare_result_logging ( start ) # apply preprocessing self . preprocess_data () if not self . is_final_fit : # Outer Folds outer_folds = FoldInfo . generate_folds ( self . cross_validation . outer_cv , self . data . X , self . data . y , self . data . kwargs , self . cross_validation . use_test_set , self . cross_validation . test_size ) self . cross_validation . outer_folds = { f . fold_id : f for f in outer_folds } delayed_jobs = [] # Run Dummy Estimator dummy_estimator = self . _prepare_dummy_estimator () if self . cache_folder is not None : logger . info ( \"Removing cache files...\" ) CacheManager . clear_cache_files ( self . cache_folder , force_all = True ) # loop over outer cross validation for i , outer_f in enumerate ( outer_folds ): # 1. generate OuterFolds Object outer_fold = MDBOuterFold ( fold_nr = outer_f . fold_nr ) outer_fold_computer = OuterFoldManager ( self . _pipe , self . optimization , outer_f . fold_id , self . cross_validation , cache_folder = self . cache_folder , cache_updater = self . recursive_cache_folder_propagation , dummy_estimator = dummy_estimator , result_obj = outer_fold ) # 2. monitor outputs self . results . outer_folds . append ( outer_fold ) if self . nr_of_processes > 1 : result = dask . delayed ( Hyperpipe . fit_outer_folds )( outer_fold_computer , self . data . X , self . data . y , self . data . kwargs ) delayed_jobs . append ( result ) else : try : # 3. fit outer_fold_computer . fit ( self . data . X , self . data . y , ** self . data . kwargs ) # 4. save outer fold results self . results_handler . save () finally : # 5. clear cache CacheManager . clear_cache_files ( self . cache_folder ) if self . nr_of_processes > 1 : dask . compute ( * delayed_jobs ) self . results_handler . save () # evaluate hyperparameter optimization results for best config self . _finalize_optimization () # clear complete cache ? use self.cache_folder to delete all subfolders within the parent cache folder # directory CacheManager . clear_cache_files ( self . cache_folder , force_all = True ) ############################################################################################### else : self . preprocess_data () self . _pipe . fit ( self . data . X , self . data . y , ** kwargs ) except Exception as e : logger . error ( e ) logger . error ( traceback . format_exc ()) traceback . print_exc () raise e finally : if self . nr_of_processes > 1 : hyperpipe_client . close () return self def predict ( self , data : np . ndarray , ** kwargs ) -> np . ndarray : \"\"\" Use the optimum pipe to predict the input data. Parameters: data: The array-like prediction data with shape=[M, D], where M is the number of samples and D is the number of features. D must correspond to the number of trained dimensions of the fit method. **kwargs: Keyword arguments, passed to optimum_pipe.predict. Returns: Predicted targets calculated on input data with trained model. \"\"\" # Todo: if local_search = true then use optimized pipe here? if self . _pipe : return self . optimum_pipe . predict ( data , ** kwargs ) def predict_proba ( self , data : np . ndarray , ** kwargs ) -> np . ndarray : \"\"\" Use the optimum pipe to predict the probabilities from the input data. Parameters: data: The array-like prediction data with shape=[M, D], where M is the number of samples and D is the number of features. D must correspond to the number of trained dimensions of the fit method. **kwargs: Keyword arguments, passed to optimum_pipe.predict_proba. Returns: Probabilities calculated from input data on fitted model. \"\"\" if self . _pipe : return self . optimum_pipe . predict_proba ( data , ** kwargs ) def transform ( self , data : np . ndarray , ** kwargs ) -> np . ndarray : \"\"\" Use the optimum pipe to transform the data. Parameters: data: The array-like input data with shape=[M, D], where M is the number of samples and D is the number of features. D must correspond to the number of trained dimensions of the fit method. **kwargs: Keyword arguments, passed to optimum_pipe.transform. Returns: Transformed data. \"\"\" if self . _pipe : X , _ , _ = self . optimum_pipe . transform ( data , y = None , ** kwargs ) return X def score ( self , data : np . ndarray , y : np . ndarray , ** kwargs ) -> np . ndarray : \"\"\" Use the optimum pipe to score the model. Parameters: data: The array-like data with shape=[M, D], where M is the number of samples and D is the number of features. D must correspond to the number of trained dimensions of the fit method. y: The array-like true targets. **kwargs: Keyword arguments, passed to optimum_pipe.predict. Returns: Score data on input data with trained model. \"\"\" if self . _pipe : predictions = self . optimum_pipe . predict ( data , ** kwargs ) scorer = Scorer . create ( self . optimization . best_config_metric ) return scorer ( y , predictions ) def _calculate_permutation_importances ( self , ** kwargs ): \"\"\" extracted function from get_feature_importance to improve unit testing \"\"\" importance_list = { 'mean' : list (), 'std' : list ()} def train_and_get_fimps ( pipeline , train_idx , test_idx , data_X , data_y , data_kwargs , fold_str ): train_X , train_y , train_kwargs = PhotonDataHelper . split_data ( data_X , data_y , data_kwargs , indices = train_idx ) test_X , test_y , test_kwargs = PhotonDataHelper . split_data ( data_X , data_y , data_kwargs , indices = test_idx ) # fit fold's best model (again) -> to obtain that model's feature importances logger . photon_system_log ( \"Permutation Importances: Fitting model for \" + fold_str ) pipeline . fit ( train_X , train_y , ** train_kwargs ) # get feature importances logger . photon_system_log ( \"Permutation Importances: Calculating performances for \" + fold_str ) perm_imps = permutation_importance ( pipeline , test_X , test_y , ** kwargs ) # store into list importance_list [ 'mean' ] . append ( perm_imps [ \"importances_mean\" ]) importance_list [ 'std' ] . append ( perm_imps [ \"importances_std\" ]) return perm_imps for outer_fold in self . results . outer_folds : if outer_fold . best_config is None : raise ValueError ( \"Could not find a best config for outer fold \" + str ( outer_fold . fold_nr )) pipe_copy = self . optimum_pipe . copy_me () # set pipe to config pipe_copy . set_params ( ** outer_fold . best_config . config_dict ) if not self . results . hyperpipe_info . eval_final_performance : no_outer_cv_indices = False if outer_fold . best_config . best_config_score is None : no_outer_cv_indices = True elif outer_fold . best_config . best_config_score . training is None or not outer_fold . best_config . best_config_score . training . indices : no_outer_cv_indices = True if no_outer_cv_indices : data_to_split , y_to_split , kwargs_to_split = self . data . X , self . data . y , self . data . kwargs else : logger . photon_system_log ( \"Permutation Importances: Using inner_cv folds.\" ) # get outer fold data idx = outer_fold . best_config . best_config_score . training . indices data_to_split , y_to_split , kwargs_to_split = PhotonDataHelper . split_data ( self . data . X , self . data . y , self . data . kwargs , indices = idx ) for inner_fold in outer_fold . best_config . inner_folds : train_and_get_fimps ( pipe_copy , inner_fold . training . indices , inner_fold . validation . indices , data_to_split , y_to_split , kwargs_to_split , \"inner fold \" + str ( inner_fold . fold_nr )) else : train_and_get_fimps ( pipe_copy , outer_fold . best_config . best_config_score . training . indices , outer_fold . best_config . best_config_score . validation . indices , self . data . X , self . data . y , self . data . kwargs , \"outer fold \" + str ( outer_fold . fold_nr )) return importance_list def get_permutation_feature_importances ( self , ** kwargs ): \"\"\" Fits a model for the best config of each outer fold (using the training data of that fold). Then calls sklearn.inspection.permutation_importance with the test data and the given kwargs (e.g. n_repeats). Returns mean of \"importances_mean\" and of \"importances_std\" of all outer folds. Parameters: **kwargs: Keyword arguments, passed to sklearn.permutation_importance. Returns: Dictionary with average of \"mean\" and \"std\" for all outer folds, respectively. \"\"\" logger . photon_system_log ( \"\" ) logger . photon_system_log ( \"Computing permutation importances. This may take a while.\" ) logger . stars () if self . optimum_pipe is None : raise ValueError ( \"Cannot calculate permutation importances when optimum_pipe is None (probably the \" \"training and optimization procedure failed)\" ) importance_list = self . _calculate_permutation_importances ( ** kwargs ) mean_importances = np . mean ( np . array ( importance_list [ \"mean\" ]), axis = 0 ) std_importances = np . mean ( np . array ( importance_list [ \"std\" ]), axis = 0 ) logger . stars () return { 'mean' : mean_importances , 'std' : std_importances } def inverse_transform_pipeline ( self , hyperparameters : dict , data : np . ndarray , targets : np . ndarray , data_to_inverse : np . ndarray ) -> np . ndarray : \"\"\" Inverse transform data for a pipeline with specific hyperparameter configuration. 1. Copy Sklearn Pipeline, 2. Set Parameters 3. Fit Pipeline to data and targets 4. Inverse transform data with that pipeline Parameters: hyperparameters: The concrete configuration settings for the pipeline elements. data: The training data to which the pipeline is fitted. targets: The truth values for training. data_to_inverse: The data that should be inversed after training. Returns: Inverse data as array. \"\"\" copied_pipe = self . pipe . copy_me () copied_pipe . set_params ( ** hyperparameters ) copied_pipe . fit ( data , targets ) return copied_pipe . inverse_transform ( data_to_inverse ) # =================================================================== # Copy, Save and Load # =================================================================== def copy_me ( self ): \"\"\" Helper function to copy an entire Hyperpipe Returns: Hyperpipe \"\"\" signature = inspect . getfullargspec ( OutputSettings . __init__ )[ 0 ] settings = OutputSettings () for attr in signature : if hasattr ( self . output_settings , attr ): setattr ( settings , attr , getattr ( self . output_settings , attr )) self . output_settings . initialize_log_file () # create new Hyperpipe instance pipe_copy = Hyperpipe ( name = self . name , inner_cv = deepcopy ( self . cross_validation . inner_cv ), outer_cv = deepcopy ( self . cross_validation . outer_cv ), best_config_metric = self . optimization . best_config_metric , metrics = self . optimization . metrics , optimizer = self . optimization . optimizer_input_str , optimizer_params = self . optimization . optimizer_params , project_folder = self . project_folder , output_settings = settings ) signature = inspect . getfullargspec ( self . __init__ )[ 0 ] for attr in signature : if hasattr ( self , attr ) and attr != 'output_settings' : setattr ( pipe_copy , attr , getattr ( self , attr )) if hasattr ( self , 'preprocessing' ) and self . preprocessing : preprocessing = Preprocessing () for element in self . preprocessing . elements : preprocessing += element . copy_me () pipe_copy += preprocessing if hasattr ( self , 'elements' ): for element in self . elements : pipe_copy += element . copy_me () return pipe_copy def save_optimum_pipe ( self , filename = None , password = None ): if filename is None : filename = \"photon_\" + self . name + \"_best_model.p\" PhotonModelPersistor . save_optimum_pipe ( self , filename , password ) @staticmethod def load_optimum_pipe ( file : str , password : str = None ) -> PhotonPipeline : \"\"\" Load optimum pipe from file. As staticmethod, instantiation is thus not required. Called backend: PhotonModelPersistor.load_optimum_pipe. Parameters: file: File path specifying .photon file to load trained pipeline from zipped file. password: Passcode for read file. Returns: Returns pipeline with all trained PipelineElements. \"\"\" return PhotonModelPersistor . load_optimum_pipe ( file , password ) @staticmethod def reload_hyperpipe ( results_folder , X , y , ** data_kwargs ): res_handler = ResultsHandler () res_handler . load_from_file ( os . path . join ( results_folder , \"photon_result_file.json\" )) loaded_optimum_pipe = Hyperpipe . load_optimum_pipe ( os . path . join ( results_folder , \"photon_best_model.photon\" )) new_hyperpipe = JsonTransformer () . from_json_file ( os . path . join ( results_folder , \"hyperpipe_config.json\" )) new_hyperpipe . results = res_handler . results new_hyperpipe . optimum_pipe = loaded_optimum_pipe new_hyperpipe . data = Hyperpipe . Data ( X , y , data_kwargs ) return new_hyperpipe def __repr__ ( self , ** kwargs ): \"\"\"Overwrite BaseEstimator's function to avoid errors when using Jupyter Notebooks.\"\"\" return \"Hyperpipe(name=' {} ')\" . format ( self . name )","title":"Documentation for Hyperpipe"},{"location":"api/base/hyperpipe/#photonai.base.hyperpipe.Hyperpipe.__init__","text":"Initialize the object. Parameters: Name Type Description Default name Optional[str] Name of hyperpipe instance. required inner_cv Union[sklearn.model_selection._split.BaseCrossValidator, sklearn.model_selection._split.BaseShuffleSplit, sklearn.model_selection._split._RepeatedSplits] Cross validation strategy to test hyperparameter configurations, generates the validation set. None outer_cv Union[sklearn.model_selection._split.BaseCrossValidator, sklearn.model_selection._split.BaseShuffleSplit, sklearn.model_selection._split._RepeatedSplits] Cross validation strategy to use for the hyperparameter search itself, generates the test set. None optimizer str Hyperparameter optimization algorithm. In case a string literal is given: \"grid_search\": Optimizer that iteratively tests all possible hyperparameter combinations. \"random_grid_search\": A variation of the grid search optimization that randomly picks hyperparameter combinations from all possible hyperparameter combinations. \"sk_opt\": Scikit-Optimize based on theories of bayesian optimization. \"random_search\": randomly chooses hyperparameter from grid-free domain. \"smac\": SMAC based on theories of bayesian optimization. \"nevergrad\": Nevergrad based on theories of evolutionary learning. In case an object is given: expects the object to have the following methods: ask : returns a hyperparameter configuration in form of an dictionary containing key->value pairs in the sklearn parameter encoding model_name__parameter_name: parameter_value prepare : takes a list of pipeline elements and their particular hyperparameters to prepare the hyperparameter space tell : gets a tested config and the respective performance in order to calculate a smart next configuration to process 'grid_search' metrics Optional[List[Union[Callable, keras.metrics.Metric, Type[keras.metrics.Metric], str]]] Metrics that should be calculated for both training, validation and test set Use the preimported metrics from sklearn and photonai, or register your own Metrics for classification : accuracy : sklearn.metrics.accuracy_score matthews_corrcoef : sklearn.metrics.matthews_corrcoef confusion_matrix : sklearn.metrics.confusion_matrix, f1_score : sklearn.metrics.f1_score hamming_loss : sklearn.metrics.hamming_loss log_loss : sklearn.metrics.log_loss precision : sklearn.metrics.precision_score recall : sklearn.metrics.recall_score Metrics for regression : mean_squared_error : sklearn.metrics.mean_squared_error mean_absolute_error : sklearn.metrics.mean_absolute_error explained_variance : sklearn.metrics.explained_variance_score r2 : sklearn.metrics.r2_score Other metrics pearson_correlation : photon_core.framework.Metrics.pearson_correlation variance_explained : photon_core.framework.Metrics.variance_explained_score categorical_accuracy : photon_core.framework.Metrics.categorical_accuracy_score None best_config_metric Union[Callable, keras.metrics.Metric, Type[keras.metrics.Metric], str] The metric that should be maximized or minimized in order to choose the best hyperparameter configuration. None eval_final_performance bool DEPRECATED! Use \"use_test_set\" instead! None use_test_set bool If the metrics should be calculated for the test set, otherwise the test set is seperated but not used. True project_folder str The output folder in which all files generated by the PHOTONAI project are saved to. '' test_size float The amount of the data that should be left out if no outer_cv is given and eval_final_performance is set to True. 0.2 calculate_metrics_per_fold bool If True, the metrics are calculated for each inner_fold. If False, calculate_metrics_across_folds must be True. True calculate_metrics_across_folds bool If True, the metrics are calculated across all inner_fold. If False, calculate_metrics_per_fold must be True. False random_seed int Random Seed. None verbosity int The level of verbosity, 0 is least talkative and gives only warn and error, 1 gives adds info and 2 adds debug. 0 learning_curves bool Enables learning curve procedure. Evaluate learning process over different sizes of input. Depends on learning_curves_cut. False learning_curves_cut FloatRange The tested relative cuts for data size. None performance_constraints list Objects that indicate whether a configuration should be tested further. For example, the inner fold of a config does not perform better than the dummy performance. None permutation_id str String identifier for permutation tests. None cache_folder str Folder path for multi-processing. None nr_of_processes int Determined the amount of simultaneous calculation of outer_folds. 1 allow_multidim_targets bool Allows multidimensional targets. False Source code in photonai/base/hyperpipe.py def __init__ ( self , name : Optional [ str ], inner_cv : Union [ BaseCrossValidator , BaseShuffleSplit , _RepeatedSplits ] = None , outer_cv : Union [ BaseCrossValidator , BaseShuffleSplit , _RepeatedSplits , None ] = None , optimizer : str = 'grid_search' , optimizer_params : dict = None , metrics : Optional [ List [ Union [ Scorer . Metric_Type , str ]]] = None , best_config_metric : Optional [ Union [ Scorer . Metric_Type , str ]] = None , eval_final_performance : bool = None , use_test_set : bool = True , test_size : float = 0.2 , project_folder : str = '' , calculate_metrics_per_fold : bool = True , calculate_metrics_across_folds : bool = False , random_seed : int = None , verbosity : int = 0 , learning_curves : bool = False , learning_curves_cut : FloatRange = None , output_settings : OutputSettings = None , performance_constraints : list = None , permutation_id : str = None , cache_folder : str = None , nr_of_processes : int = 1 , allow_multidim_targets : bool = False ): \"\"\" Initialize the object. Parameters: name: Name of hyperpipe instance. inner_cv: Cross validation strategy to test hyperparameter configurations, generates the validation set. outer_cv: Cross validation strategy to use for the hyperparameter search itself, generates the test set. optimizer: Hyperparameter optimization algorithm. - In case a string literal is given: - \"grid_search\": Optimizer that iteratively tests all possible hyperparameter combinations. - \"random_grid_search\": A variation of the grid search optimization that randomly picks hyperparameter combinations from all possible hyperparameter combinations. - \"sk_opt\": Scikit-Optimize based on theories of bayesian optimization. - \"random_search\": randomly chooses hyperparameter from grid-free domain. - \"smac\": SMAC based on theories of bayesian optimization. - \"nevergrad\": Nevergrad based on theories of evolutionary learning. - In case an object is given: expects the object to have the following methods: - `ask`: returns a hyperparameter configuration in form of an dictionary containing key->value pairs in the sklearn parameter encoding `model_name__parameter_name: parameter_value` - `prepare`: takes a list of pipeline elements and their particular hyperparameters to prepare the hyperparameter space - `tell`: gets a tested config and the respective performance in order to calculate a smart next configuration to process metrics: Metrics that should be calculated for both training, validation and test set Use the preimported metrics from sklearn and photonai, or register your own - Metrics for `classification`: - `accuracy`: sklearn.metrics.accuracy_score - `matthews_corrcoef`: sklearn.metrics.matthews_corrcoef - `confusion_matrix`: sklearn.metrics.confusion_matrix, - `f1_score`: sklearn.metrics.f1_score - `hamming_loss`: sklearn.metrics.hamming_loss - `log_loss`: sklearn.metrics.log_loss - `precision`: sklearn.metrics.precision_score - `recall`: sklearn.metrics.recall_score - Metrics for `regression`: - `mean_squared_error`: sklearn.metrics.mean_squared_error - `mean_absolute_error`: sklearn.metrics.mean_absolute_error - `explained_variance`: sklearn.metrics.explained_variance_score - `r2`: sklearn.metrics.r2_score - Other metrics - `pearson_correlation`: photon_core.framework.Metrics.pearson_correlation - `variance_explained`: photon_core.framework.Metrics.variance_explained_score - `categorical_accuracy`: photon_core.framework.Metrics.categorical_accuracy_score best_config_metric: The metric that should be maximized or minimized in order to choose the best hyperparameter configuration. eval_final_performance: DEPRECATED! Use \"use_test_set\" instead! use_test_set: If the metrics should be calculated for the test set, otherwise the test set is seperated but not used. project_folder: The output folder in which all files generated by the PHOTONAI project are saved to. test_size: The amount of the data that should be left out if no outer_cv is given and eval_final_performance is set to True. calculate_metrics_per_fold: If True, the metrics are calculated for each inner_fold. If False, calculate_metrics_across_folds must be True. calculate_metrics_across_folds: If True, the metrics are calculated across all inner_fold. If False, calculate_metrics_per_fold must be True. random_seed: Random Seed. verbosity: The level of verbosity, 0 is least talkative and gives only warn and error, 1 gives adds info and 2 adds debug. learning_curves: Enables learning curve procedure. Evaluate learning process over different sizes of input. Depends on learning_curves_cut. learning_curves_cut: The tested relative cuts for data size. performance_constraints: Objects that indicate whether a configuration should be tested further. For example, the inner fold of a config does not perform better than the dummy performance. permutation_id: String identifier for permutation tests. cache_folder: Folder path for multi-processing. nr_of_processes: Determined the amount of simultaneous calculation of outer_folds. allow_multidim_targets: Allows multidimensional targets. \"\"\" self . name = re . sub ( r '\\W+' , '' , name ) if eval_final_performance is not None : depr_warning = \"Hyperpipe parameter eval_final_performance is deprecated. It's called use_test_set now.\" use_test_set = eval_final_performance logger . warning ( depr_warning ) raise DeprecationWarning ( depr_warning ) # ====================== Cross Validation =========================== # check if both calculate_metrics_per_folds and calculate_metrics_across_folds is False if not calculate_metrics_across_folds and not calculate_metrics_per_fold : raise NotImplementedError ( \"Apparently, you've set calculate_metrics_across_folds=False and \" \"calculate_metrics_per_fold=False. In this case PHOTONAI does not calculate \" \"any metrics which doesn't make any sense. Set at least one to True.\" ) if inner_cv is None : msg = \"PHOTONAI requires an inner_cv split. Please enable inner cross-validation. \" \\ \"As exmaple: Hyperpipe(...inner_cv = KFold(n_splits = 3), ...). \" \\ \"Ensure you import the cross_validation object first.\" logger . error ( msg ) raise AttributeError ( msg ) # use default cut 'FloatRange(0, 1, 'range', 0.2)' if learning_curves = True but learning_curves_cut is None if learning_curves and learning_curves_cut is None : learning_curves_cut = FloatRange ( 0 , 1 , 'range' , 0.2 ) elif not learning_curves and learning_curves_cut is not None : learning_curves_cut = None self . cross_validation = Hyperpipe . CrossValidation ( inner_cv = inner_cv , outer_cv = outer_cv , use_test_set = use_test_set , test_size = test_size , calculate_metrics_per_fold = calculate_metrics_per_fold , calculate_metrics_across_folds = calculate_metrics_across_folds , learning_curves = learning_curves , learning_curves_cut = learning_curves_cut ) # ====================== Data =========================== self . data = Hyperpipe . Data ( allow_multidim_targets = allow_multidim_targets ) # ====================== Output Folder and Log File Management =========================== if output_settings : self . output_settings = output_settings else : self . output_settings = OutputSettings () if project_folder == '' : self . project_folder = os . getcwd () else : self . project_folder = project_folder self . output_settings . set_project_folder ( self . project_folder ) # update output options to add pipe name and timestamp to results folder self . _verbosity = 0 self . verbosity = verbosity self . output_settings . set_log_file () # ====================== Result Logging =========================== self . results_handler = None self . results = None self . best_config = None # ====================== Pipeline =========================== self . elements = [] self . _pipe = None self . optimum_pipe = None self . preprocessing = None # ====================== Performance Optimization =========================== if optimizer_params is None : optimizer_params = {} self . optimization = Optimization ( metrics = metrics , best_config_metric = best_config_metric , optimizer_input = optimizer , optimizer_params = optimizer_params , performance_constraints = performance_constraints ) # self.optimization.sanity_check_metrics() # ====================== Caching and Parallelization =========================== self . nr_of_processes = nr_of_processes if cache_folder : self . cache_folder = os . path . join ( cache_folder , self . name ) else : self . cache_folder = None # ====================== Internals =========================== self . permutation_id = permutation_id self . allow_multidim_targets = allow_multidim_targets self . is_final_fit = False # ====================== Random Seed =========================== self . random_state = random_seed if random_seed is not None : import random random . seed ( random_seed )","title":"__init__()"},{"location":"api/base/hyperpipe/#photonai.base.hyperpipe.Hyperpipe.add","text":"Add an element to the machine learning pipeline. Returns self. Parameters: Name Type Description Default pipe_element PipelineElement The object to add to the machine learning pipeline, being either a transformer or an estimator. required Source code in photonai/base/hyperpipe.py def add ( self , pipe_element : PipelineElement ): \"\"\" Add an element to the machine learning pipeline. Returns self. Parameters: pipe_element: The object to add to the machine learning pipeline, being either a transformer or an estimator. \"\"\" self . __iadd__ ( pipe_element )","title":"add()"},{"location":"api/base/hyperpipe/#photonai.base.hyperpipe.Hyperpipe.copy_me","text":"Helper function to copy an entire Hyperpipe Returns: Type Description Hyperpipe Source code in photonai/base/hyperpipe.py def copy_me ( self ): \"\"\" Helper function to copy an entire Hyperpipe Returns: Hyperpipe \"\"\" signature = inspect . getfullargspec ( OutputSettings . __init__ )[ 0 ] settings = OutputSettings () for attr in signature : if hasattr ( self . output_settings , attr ): setattr ( settings , attr , getattr ( self . output_settings , attr )) self . output_settings . initialize_log_file () # create new Hyperpipe instance pipe_copy = Hyperpipe ( name = self . name , inner_cv = deepcopy ( self . cross_validation . inner_cv ), outer_cv = deepcopy ( self . cross_validation . outer_cv ), best_config_metric = self . optimization . best_config_metric , metrics = self . optimization . metrics , optimizer = self . optimization . optimizer_input_str , optimizer_params = self . optimization . optimizer_params , project_folder = self . project_folder , output_settings = settings ) signature = inspect . getfullargspec ( self . __init__ )[ 0 ] for attr in signature : if hasattr ( self , attr ) and attr != 'output_settings' : setattr ( pipe_copy , attr , getattr ( self , attr )) if hasattr ( self , 'preprocessing' ) and self . preprocessing : preprocessing = Preprocessing () for element in self . preprocessing . elements : preprocessing += element . copy_me () pipe_copy += preprocessing if hasattr ( self , 'elements' ): for element in self . elements : pipe_copy += element . copy_me () return pipe_copy","title":"copy_me()"},{"location":"api/base/hyperpipe/#photonai.base.hyperpipe.Hyperpipe.fit","text":"Starts the hyperparameter search and/or fits the pipeline to the data and targets. Manages the nested cross validated hyperparameter search: Filters the data according to filter strategy (1) and according to the imbalanced_data_strategy (2) requests new configurations from the hyperparameter search strategy, the optimizer, initializes the testing of a specific configuration, communicates the result to the optimizer, repeats 2-4 until optimizer delivers no more configurations to test finally searches for the best config in all tested configs, trains the pipeline with the best config and evaluates the performance on the test set Parameters: Name Type Description Default data ndarray The array-like training and test data with shape=[N, D], where N is the number of samples and D is the number of features. required targets ndarray The truth array-like values with shape=[N], where N is the number of samples. required **kwargs Keyword arguments, passed to Outer_Fold_Manager.fit. {} Returns: Type Description Fitted Hyperpipe. Source code in photonai/base/hyperpipe.py def fit ( self , data : np . ndarray , targets : np . ndarray , ** kwargs ): \"\"\" Starts the hyperparameter search and/or fits the pipeline to the data and targets. Manages the nested cross validated hyperparameter search: 1. Filters the data according to filter strategy (1) and according to the imbalanced_data_strategy (2) 2. requests new configurations from the hyperparameter search strategy, the optimizer, 3. initializes the testing of a specific configuration, 4. communicates the result to the optimizer, 5. repeats 2-4 until optimizer delivers no more configurations to test 6. finally searches for the best config in all tested configs, 7. trains the pipeline with the best config and evaluates the performance on the test set Parameters: data: The array-like training and test data with shape=[N, D], where N is the number of samples and D is the number of features. targets: The truth array-like values with shape=[N], where N is the number of samples. **kwargs: Keyword arguments, passed to Outer_Fold_Manager.fit. Returns: Fitted Hyperpipe. \"\"\" # switch to result output folder start = datetime . datetime . now () self . output_settings . update_settings ( self . name , start . strftime ( \"%Y-%m- %d _%H-%M-%S\" )) logger . photon_system_log ( '=' * 101 ) logger . photon_system_log ( 'PHOTONAI ANALYSIS: ' + self . name ) logger . photon_system_log ( '=' * 101 ) logger . info ( \"Preparing data and PHOTONAI objects for analysis...\" ) # loop over outer cross validation if self . nr_of_processes > 1 : hyperpipe_client = Client ( threads_per_worker = 1 , n_workers = self . nr_of_processes , processes = False ) try : # check data self . data . input_data_sanity_checks ( data , targets , ** kwargs ) # create photon pipeline self . _prepare_pipeline () # initialize the progress monitors self . _prepare_result_logging ( start ) # apply preprocessing self . preprocess_data () if not self . is_final_fit : # Outer Folds outer_folds = FoldInfo . generate_folds ( self . cross_validation . outer_cv , self . data . X , self . data . y , self . data . kwargs , self . cross_validation . use_test_set , self . cross_validation . test_size ) self . cross_validation . outer_folds = { f . fold_id : f for f in outer_folds } delayed_jobs = [] # Run Dummy Estimator dummy_estimator = self . _prepare_dummy_estimator () if self . cache_folder is not None : logger . info ( \"Removing cache files...\" ) CacheManager . clear_cache_files ( self . cache_folder , force_all = True ) # loop over outer cross validation for i , outer_f in enumerate ( outer_folds ): # 1. generate OuterFolds Object outer_fold = MDBOuterFold ( fold_nr = outer_f . fold_nr ) outer_fold_computer = OuterFoldManager ( self . _pipe , self . optimization , outer_f . fold_id , self . cross_validation , cache_folder = self . cache_folder , cache_updater = self . recursive_cache_folder_propagation , dummy_estimator = dummy_estimator , result_obj = outer_fold ) # 2. monitor outputs self . results . outer_folds . append ( outer_fold ) if self . nr_of_processes > 1 : result = dask . delayed ( Hyperpipe . fit_outer_folds )( outer_fold_computer , self . data . X , self . data . y , self . data . kwargs ) delayed_jobs . append ( result ) else : try : # 3. fit outer_fold_computer . fit ( self . data . X , self . data . y , ** self . data . kwargs ) # 4. save outer fold results self . results_handler . save () finally : # 5. clear cache CacheManager . clear_cache_files ( self . cache_folder ) if self . nr_of_processes > 1 : dask . compute ( * delayed_jobs ) self . results_handler . save () # evaluate hyperparameter optimization results for best config self . _finalize_optimization () # clear complete cache ? use self.cache_folder to delete all subfolders within the parent cache folder # directory CacheManager . clear_cache_files ( self . cache_folder , force_all = True ) ############################################################################################### else : self . preprocess_data () self . _pipe . fit ( self . data . X , self . data . y , ** kwargs ) except Exception as e : logger . error ( e ) logger . error ( traceback . format_exc ()) traceback . print_exc () raise e finally : if self . nr_of_processes > 1 : hyperpipe_client . close () return self","title":"fit()"},{"location":"api/base/hyperpipe/#photonai.base.hyperpipe.Hyperpipe.get_permutation_feature_importances","text":"Fits a model for the best config of each outer fold (using the training data of that fold). Then calls sklearn.inspection.permutation_importance with the test data and the given kwargs (e.g. n_repeats). Returns mean of \"importances_mean\" and of \"importances_std\" of all outer folds. Parameters: Name Type Description Default **kwargs Keyword arguments, passed to sklearn.permutation_importance. {} Returns: Type Description Dictionary with average of \"mean\" and \"std\" for all outer folds, respectively. Source code in photonai/base/hyperpipe.py def get_permutation_feature_importances ( self , ** kwargs ): \"\"\" Fits a model for the best config of each outer fold (using the training data of that fold). Then calls sklearn.inspection.permutation_importance with the test data and the given kwargs (e.g. n_repeats). Returns mean of \"importances_mean\" and of \"importances_std\" of all outer folds. Parameters: **kwargs: Keyword arguments, passed to sklearn.permutation_importance. Returns: Dictionary with average of \"mean\" and \"std\" for all outer folds, respectively. \"\"\" logger . photon_system_log ( \"\" ) logger . photon_system_log ( \"Computing permutation importances. This may take a while.\" ) logger . stars () if self . optimum_pipe is None : raise ValueError ( \"Cannot calculate permutation importances when optimum_pipe is None (probably the \" \"training and optimization procedure failed)\" ) importance_list = self . _calculate_permutation_importances ( ** kwargs ) mean_importances = np . mean ( np . array ( importance_list [ \"mean\" ]), axis = 0 ) std_importances = np . mean ( np . array ( importance_list [ \"std\" ]), axis = 0 ) logger . stars () return { 'mean' : mean_importances , 'std' : std_importances }","title":"get_permutation_feature_importances()"},{"location":"api/base/hyperpipe/#photonai.base.hyperpipe.Hyperpipe.inverse_transform_pipeline","text":"Inverse transform data for a pipeline with specific hyperparameter configuration. Copy Sklearn Pipeline, Set Parameters Fit Pipeline to data and targets Inverse transform data with that pipeline Parameters: Name Type Description Default hyperparameters dict The concrete configuration settings for the pipeline elements. required data ndarray The training data to which the pipeline is fitted. required targets ndarray The truth values for training. required data_to_inverse ndarray The data that should be inversed after training. required Returns: Type Description ndarray Inverse data as array. Source code in photonai/base/hyperpipe.py def inverse_transform_pipeline ( self , hyperparameters : dict , data : np . ndarray , targets : np . ndarray , data_to_inverse : np . ndarray ) -> np . ndarray : \"\"\" Inverse transform data for a pipeline with specific hyperparameter configuration. 1. Copy Sklearn Pipeline, 2. Set Parameters 3. Fit Pipeline to data and targets 4. Inverse transform data with that pipeline Parameters: hyperparameters: The concrete configuration settings for the pipeline elements. data: The training data to which the pipeline is fitted. targets: The truth values for training. data_to_inverse: The data that should be inversed after training. Returns: Inverse data as array. \"\"\" copied_pipe = self . pipe . copy_me () copied_pipe . set_params ( ** hyperparameters ) copied_pipe . fit ( data , targets ) return copied_pipe . inverse_transform ( data_to_inverse )","title":"inverse_transform_pipeline()"},{"location":"api/base/hyperpipe/#photonai.base.hyperpipe.Hyperpipe.load_optimum_pipe","text":"Load optimum pipe from file. As staticmethod, instantiation is thus not required. Called backend: PhotonModelPersistor.load_optimum_pipe. Parameters: Name Type Description Default file str File path specifying .photon file to load trained pipeline from zipped file. required password str Passcode for read file. None Returns: Type Description PhotonPipeline Returns pipeline with all trained PipelineElements. Source code in photonai/base/hyperpipe.py @staticmethod def load_optimum_pipe ( file : str , password : str = None ) -> PhotonPipeline : \"\"\" Load optimum pipe from file. As staticmethod, instantiation is thus not required. Called backend: PhotonModelPersistor.load_optimum_pipe. Parameters: file: File path specifying .photon file to load trained pipeline from zipped file. password: Passcode for read file. Returns: Returns pipeline with all trained PipelineElements. \"\"\" return PhotonModelPersistor . load_optimum_pipe ( file , password )","title":"load_optimum_pipe()"},{"location":"api/base/hyperpipe/#photonai.base.hyperpipe.Hyperpipe.predict","text":"Use the optimum pipe to predict the input data. Parameters: Name Type Description Default data ndarray The array-like prediction data with shape=[M, D], where M is the number of samples and D is the number of features. D must correspond to the number of trained dimensions of the fit method. required **kwargs Keyword arguments, passed to optimum_pipe.predict. {} Returns: Type Description ndarray Predicted targets calculated on input data with trained model. Source code in photonai/base/hyperpipe.py def predict ( self , data : np . ndarray , ** kwargs ) -> np . ndarray : \"\"\" Use the optimum pipe to predict the input data. Parameters: data: The array-like prediction data with shape=[M, D], where M is the number of samples and D is the number of features. D must correspond to the number of trained dimensions of the fit method. **kwargs: Keyword arguments, passed to optimum_pipe.predict. Returns: Predicted targets calculated on input data with trained model. \"\"\" # Todo: if local_search = true then use optimized pipe here? if self . _pipe : return self . optimum_pipe . predict ( data , ** kwargs )","title":"predict()"},{"location":"api/base/hyperpipe/#photonai.base.hyperpipe.Hyperpipe.predict_proba","text":"Use the optimum pipe to predict the probabilities from the input data. Parameters: Name Type Description Default data ndarray The array-like prediction data with shape=[M, D], where M is the number of samples and D is the number of features. D must correspond to the number of trained dimensions of the fit method. required **kwargs Keyword arguments, passed to optimum_pipe.predict_proba. {} Returns: Type Description ndarray Probabilities calculated from input data on fitted model. Source code in photonai/base/hyperpipe.py def predict_proba ( self , data : np . ndarray , ** kwargs ) -> np . ndarray : \"\"\" Use the optimum pipe to predict the probabilities from the input data. Parameters: data: The array-like prediction data with shape=[M, D], where M is the number of samples and D is the number of features. D must correspond to the number of trained dimensions of the fit method. **kwargs: Keyword arguments, passed to optimum_pipe.predict_proba. Returns: Probabilities calculated from input data on fitted model. \"\"\" if self . _pipe : return self . optimum_pipe . predict_proba ( data , ** kwargs )","title":"predict_proba()"},{"location":"api/base/output_settings/","text":"Documentation for OutputSettings Configuration class that specifies the format in which the results are saved. Results can be saved to a MongoDB or a simple son-file. You can also choose whether to save predictions and/or feature importances. Source code in photonai/base/hyperpipe.py class OutputSettings : \"\"\" Configuration class that specifies the format in which the results are saved. Results can be saved to a MongoDB or a simple son-file. You can also choose whether to save predictions and/or feature importances. \"\"\" def __init__ ( self , mongodb_connect_url : str = None , save_output : bool = True , overwrite_results : bool = False , generate_best_model : bool = True , user_id : str = '' , wizard_object_id : str = '' , wizard_project_name : str = '' , project_folder : str = '' ): \"\"\" Initialize the object. Parameters: mongodb_connect_url: Valid mongodb connection url that specifies a database for storing the results. save_output: Controls the general saving of the results. overwrite_results: Allows overwriting the results folder if it already exists. generate_best_model: Determines whether an optimum_pipe should be created and fitted. If False, no dependent files are created. user_id: The user name of the according PHOTONAI Wizard login. wizard_object_id: The object id to map the designed pipeline in the PHOTONAI Wizard to the results in the PHOTONAI CORE Database. wizard_project_name: How the project is titled in the PHOTONAI Wizard. project_folder: Deprecated Parameter - transferred to Hyperpipe. \"\"\" if project_folder : msg = \"Deprecated: The parameter 'project_folder' was moved to the Hyperpipe. \" \\ \"Please use Hyperpipe(..., project_folder='').\" logger . error ( msg ) raise DeprecationWarning ( msg ) self . mongodb_connect_url = mongodb_connect_url self . overwrite_results = overwrite_results self . user_id = user_id self . wizard_object_id = wizard_object_id self . wizard_project_name = wizard_project_name self . generate_best_model = generate_best_model self . save_output = save_output self . save_predictions_from_best_config_inner_folds = None self . verbosity = 0 self . results_folder = '' self . project_folder = '' self . log_file = '' self . logging_file_handler = None # this is only allowed from hyperpipe def set_project_folder ( self , project_folder ): self . project_folder = project_folder self . initialize_log_file () @property def setup_error_file ( self ): if self . project_folder : return os . path . join ( self . project_folder , 'photon_setup_errors.log' ) else : return \"\" def initialize_log_file ( self ): self . log_file = self . setup_error_file def update_settings ( self , name , timestamp ): if self . save_output : if not os . path . exists ( self . project_folder ): os . makedirs ( self . project_folder ) # Todo: give rights to user if this is done by docker container if self . overwrite_results : self . results_folder = os . path . join ( self . project_folder , name + '_results' ) else : self . results_folder = os . path . join ( self . project_folder , name + '_results_' + timestamp ) logger . info ( \"Output Folder: \" + self . results_folder ) if not os . path . exists ( self . results_folder ): os . makedirs ( self . results_folder ) if os . path . basename ( self . log_file ) == \"photon_setup_errors.log\" : self . log_file = 'photon_output.log' self . log_file = self . _add_timestamp ( self . log_file ) self . set_log_file () # if we made it here, there should be no further setup errors, every error that comes # now can go to the standard logger instance if os . path . isfile ( self . setup_error_file ): os . remove ( self . setup_error_file ) def _add_timestamp ( self , file ): return os . path . join ( self . results_folder , os . path . basename ( file )) def _get_log_level ( self ): if self . verbosity == 0 : level = 25 elif self . verbosity == 1 : level = logging . INFO # 20 elif self . verbosity == 2 : level = logging . DEBUG # 10 else : level = logging . WARN # 30 return level def set_log_file ( self ): logfile_directory = os . path . dirname ( self . log_file ) if not os . path . exists ( logfile_directory ): os . makedirs ( logfile_directory ) if self . logging_file_handler is None : self . logging_file_handler = logging . FileHandler ( self . log_file ) self . logging_file_handler . setLevel ( self . _get_log_level ()) logger . addHandler ( self . logging_file_handler ) else : self . logging_file_handler . close () self . logging_file_handler . baseFilename = self . log_file def set_log_level ( self ): verbose_num = self . _get_log_level () logger . setLevel ( verbose_num ) for handler in logger . handlers : handler . setLevel ( verbose_num ) __init__ ( self , mongodb_connect_url = None , save_output = True , overwrite_results = False , generate_best_model = True , user_id = '' , wizard_object_id = '' , wizard_project_name = '' , project_folder = '' ) special Initialize the object. Parameters: Name Type Description Default mongodb_connect_url str Valid mongodb connection url that specifies a database for storing the results. None save_output bool Controls the general saving of the results. True overwrite_results bool Allows overwriting the results folder if it already exists. False generate_best_model bool Determines whether an optimum_pipe should be created and fitted. If False, no dependent files are created. True user_id str The user name of the according PHOTONAI Wizard login. '' wizard_object_id str The object id to map the designed pipeline in the PHOTONAI Wizard to the results in the PHOTONAI CORE Database. '' wizard_project_name str How the project is titled in the PHOTONAI Wizard. '' project_folder str Deprecated Parameter - transferred to Hyperpipe. '' Source code in photonai/base/hyperpipe.py def __init__ ( self , mongodb_connect_url : str = None , save_output : bool = True , overwrite_results : bool = False , generate_best_model : bool = True , user_id : str = '' , wizard_object_id : str = '' , wizard_project_name : str = '' , project_folder : str = '' ): \"\"\" Initialize the object. Parameters: mongodb_connect_url: Valid mongodb connection url that specifies a database for storing the results. save_output: Controls the general saving of the results. overwrite_results: Allows overwriting the results folder if it already exists. generate_best_model: Determines whether an optimum_pipe should be created and fitted. If False, no dependent files are created. user_id: The user name of the according PHOTONAI Wizard login. wizard_object_id: The object id to map the designed pipeline in the PHOTONAI Wizard to the results in the PHOTONAI CORE Database. wizard_project_name: How the project is titled in the PHOTONAI Wizard. project_folder: Deprecated Parameter - transferred to Hyperpipe. \"\"\" if project_folder : msg = \"Deprecated: The parameter 'project_folder' was moved to the Hyperpipe. \" \\ \"Please use Hyperpipe(..., project_folder='').\" logger . error ( msg ) raise DeprecationWarning ( msg ) self . mongodb_connect_url = mongodb_connect_url self . overwrite_results = overwrite_results self . user_id = user_id self . wizard_object_id = wizard_object_id self . wizard_project_name = wizard_project_name self . generate_best_model = generate_best_model self . save_output = save_output self . save_predictions_from_best_config_inner_folds = None self . verbosity = 0 self . results_folder = '' self . project_folder = '' self . log_file = '' self . logging_file_handler = None","title":"OutputSettings"},{"location":"api/base/output_settings/#documentation-for-outputsettings","text":"Configuration class that specifies the format in which the results are saved. Results can be saved to a MongoDB or a simple son-file. You can also choose whether to save predictions and/or feature importances. Source code in photonai/base/hyperpipe.py class OutputSettings : \"\"\" Configuration class that specifies the format in which the results are saved. Results can be saved to a MongoDB or a simple son-file. You can also choose whether to save predictions and/or feature importances. \"\"\" def __init__ ( self , mongodb_connect_url : str = None , save_output : bool = True , overwrite_results : bool = False , generate_best_model : bool = True , user_id : str = '' , wizard_object_id : str = '' , wizard_project_name : str = '' , project_folder : str = '' ): \"\"\" Initialize the object. Parameters: mongodb_connect_url: Valid mongodb connection url that specifies a database for storing the results. save_output: Controls the general saving of the results. overwrite_results: Allows overwriting the results folder if it already exists. generate_best_model: Determines whether an optimum_pipe should be created and fitted. If False, no dependent files are created. user_id: The user name of the according PHOTONAI Wizard login. wizard_object_id: The object id to map the designed pipeline in the PHOTONAI Wizard to the results in the PHOTONAI CORE Database. wizard_project_name: How the project is titled in the PHOTONAI Wizard. project_folder: Deprecated Parameter - transferred to Hyperpipe. \"\"\" if project_folder : msg = \"Deprecated: The parameter 'project_folder' was moved to the Hyperpipe. \" \\ \"Please use Hyperpipe(..., project_folder='').\" logger . error ( msg ) raise DeprecationWarning ( msg ) self . mongodb_connect_url = mongodb_connect_url self . overwrite_results = overwrite_results self . user_id = user_id self . wizard_object_id = wizard_object_id self . wizard_project_name = wizard_project_name self . generate_best_model = generate_best_model self . save_output = save_output self . save_predictions_from_best_config_inner_folds = None self . verbosity = 0 self . results_folder = '' self . project_folder = '' self . log_file = '' self . logging_file_handler = None # this is only allowed from hyperpipe def set_project_folder ( self , project_folder ): self . project_folder = project_folder self . initialize_log_file () @property def setup_error_file ( self ): if self . project_folder : return os . path . join ( self . project_folder , 'photon_setup_errors.log' ) else : return \"\" def initialize_log_file ( self ): self . log_file = self . setup_error_file def update_settings ( self , name , timestamp ): if self . save_output : if not os . path . exists ( self . project_folder ): os . makedirs ( self . project_folder ) # Todo: give rights to user if this is done by docker container if self . overwrite_results : self . results_folder = os . path . join ( self . project_folder , name + '_results' ) else : self . results_folder = os . path . join ( self . project_folder , name + '_results_' + timestamp ) logger . info ( \"Output Folder: \" + self . results_folder ) if not os . path . exists ( self . results_folder ): os . makedirs ( self . results_folder ) if os . path . basename ( self . log_file ) == \"photon_setup_errors.log\" : self . log_file = 'photon_output.log' self . log_file = self . _add_timestamp ( self . log_file ) self . set_log_file () # if we made it here, there should be no further setup errors, every error that comes # now can go to the standard logger instance if os . path . isfile ( self . setup_error_file ): os . remove ( self . setup_error_file ) def _add_timestamp ( self , file ): return os . path . join ( self . results_folder , os . path . basename ( file )) def _get_log_level ( self ): if self . verbosity == 0 : level = 25 elif self . verbosity == 1 : level = logging . INFO # 20 elif self . verbosity == 2 : level = logging . DEBUG # 10 else : level = logging . WARN # 30 return level def set_log_file ( self ): logfile_directory = os . path . dirname ( self . log_file ) if not os . path . exists ( logfile_directory ): os . makedirs ( logfile_directory ) if self . logging_file_handler is None : self . logging_file_handler = logging . FileHandler ( self . log_file ) self . logging_file_handler . setLevel ( self . _get_log_level ()) logger . addHandler ( self . logging_file_handler ) else : self . logging_file_handler . close () self . logging_file_handler . baseFilename = self . log_file def set_log_level ( self ): verbose_num = self . _get_log_level () logger . setLevel ( verbose_num ) for handler in logger . handlers : handler . setLevel ( verbose_num )","title":"Documentation for OutputSettings"},{"location":"api/base/output_settings/#photonai.base.hyperpipe.OutputSettings.__init__","text":"Initialize the object. Parameters: Name Type Description Default mongodb_connect_url str Valid mongodb connection url that specifies a database for storing the results. None save_output bool Controls the general saving of the results. True overwrite_results bool Allows overwriting the results folder if it already exists. False generate_best_model bool Determines whether an optimum_pipe should be created and fitted. If False, no dependent files are created. True user_id str The user name of the according PHOTONAI Wizard login. '' wizard_object_id str The object id to map the designed pipeline in the PHOTONAI Wizard to the results in the PHOTONAI CORE Database. '' wizard_project_name str How the project is titled in the PHOTONAI Wizard. '' project_folder str Deprecated Parameter - transferred to Hyperpipe. '' Source code in photonai/base/hyperpipe.py def __init__ ( self , mongodb_connect_url : str = None , save_output : bool = True , overwrite_results : bool = False , generate_best_model : bool = True , user_id : str = '' , wizard_object_id : str = '' , wizard_project_name : str = '' , project_folder : str = '' ): \"\"\" Initialize the object. Parameters: mongodb_connect_url: Valid mongodb connection url that specifies a database for storing the results. save_output: Controls the general saving of the results. overwrite_results: Allows overwriting the results folder if it already exists. generate_best_model: Determines whether an optimum_pipe should be created and fitted. If False, no dependent files are created. user_id: The user name of the according PHOTONAI Wizard login. wizard_object_id: The object id to map the designed pipeline in the PHOTONAI Wizard to the results in the PHOTONAI CORE Database. wizard_project_name: How the project is titled in the PHOTONAI Wizard. project_folder: Deprecated Parameter - transferred to Hyperpipe. \"\"\" if project_folder : msg = \"Deprecated: The parameter 'project_folder' was moved to the Hyperpipe. \" \\ \"Please use Hyperpipe(..., project_folder='').\" logger . error ( msg ) raise DeprecationWarning ( msg ) self . mongodb_connect_url = mongodb_connect_url self . overwrite_results = overwrite_results self . user_id = user_id self . wizard_object_id = wizard_object_id self . wizard_project_name = wizard_project_name self . generate_best_model = generate_best_model self . save_output = save_output self . save_predictions_from_best_config_inner_folds = None self . verbosity = 0 self . results_folder = '' self . project_folder = '' self . log_file = '' self . logging_file_handler = None","title":"__init__()"},{"location":"api/base/pipeline_element/","text":"Documentation for PipelineElement PHOTONAI wrapper class for any transformer or estimator in the pipeline. So called PHOTONAI PipelineElements can be added to the Hyperpipe, each of them being a data-processing method or a learning algorithm. By choosing, combining data-processing methods and algorithms, and arranging them with the PHOTONAI classes, simple and complex pipeline architectures can be designed rapidly. The PHOTONAI PipelineElement implements several helpful features: Saves the hyperparameters that should be tested and creates a grid of all hyperparameter configurations. Enables fast and rapid instantiation of pipeline elements per string identifier, e.g 'svc' creates an sklearn.svm.SVC object. Attaches a \"disable\" switch to every element in the pipeline in order to test a complete disable. Source code in photonai/base/photon_elements.py class PipelineElement ( BaseEstimator ): \"\"\" PHOTONAI wrapper class for any transformer or estimator in the pipeline. So called PHOTONAI PipelineElements can be added to the Hyperpipe, each of them being a data-processing method or a learning algorithm. By choosing, combining data-processing methods and algorithms, and arranging them with the PHOTONAI classes, simple and complex pipeline architectures can be designed rapidly. The PHOTONAI PipelineElement implements several helpful features: - Saves the hyperparameters that should be tested and creates a grid of all hyperparameter configurations. - Enables fast and rapid instantiation of pipeline elements per string identifier, e.g 'svc' creates an sklearn.svm.SVC object. - Attaches a \"disable\" switch to every element in the pipeline in order to test a complete disable. \"\"\" def __init__ ( self , name : str , hyperparameters : dict = None , test_disabled : bool = False , disabled : bool = False , base_element : BaseEstimator = None , batch_size : int = 0 , ** kwargs ) -> None : \"\"\" Takes a string literal and transforms it into an object of the associated class (see PhotonCore.JSON). Parameters: name: A string literal encoding the class to be instantiated. hyperparameters: Which values/value range should be tested for the hyperparameter. In form of Dict: parameter_name -> HyperparameterElement. test_disabled: If the hyperparameter search should evaluate a complete disabling of the element. disabled: If true, the element is currently disabled and does nothing except return the data it received. base_element: The underlying BaseEstimator. If not given the instantiation per string identifier takes place. batch_size: Size of the division on which is calculated separately. **kwargs: Any parameters that should be passed to the object to be instantiated, default parameters. \"\"\" if hyperparameters is None : hyperparameters = {} if base_element is None : # Registering Pipeline Elements if len ( PhotonRegistry . ELEMENT_DICTIONARY ) == 0 : registry = PhotonRegistry if name not in PhotonRegistry . ELEMENT_DICTIONARY : # try to reload PhotonRegistry . ELEMENT_DICTIONARY = PhotonRegistry () . get_package_info () if name in PhotonRegistry . ELEMENT_DICTIONARY : try : desired_class_info = PhotonRegistry . ELEMENT_DICTIONARY [ name ] desired_class_home = desired_class_info [ 0 ] desired_class_name = desired_class_info [ 1 ] imported_module = importlib . import_module ( desired_class_home ) desired_class = getattr ( imported_module , desired_class_name ) self . base_element = desired_class ( ** kwargs ) except AttributeError as ae : logger . error ( 'ValueError: Could not find according class:' + str ( PhotonRegistry . ELEMENT_DICTIONARY [ name ])) raise ValueError ( 'Could not find according class:' , PhotonRegistry . ELEMENT_DICTIONARY [ name ]) else : # if even after reload the element does not appear, it is not supported logger . error ( 'Element not supported right now:' + name ) raise NameError ( 'Element not supported right now:' , name ) else : self . base_element = base_element self . is_transformer = hasattr ( self . base_element , \"transform\" ) self . reduce_dimension = False # boolean - set on transform method self . is_estimator = hasattr ( self . base_element , \"predict\" ) self . _name = name self . initial_name = str ( name ) self . kwargs = kwargs self . current_config = None self . batch_size = batch_size self . test_disabled = test_disabled self . initial_hyperparameters = dict ( hyperparameters ) self . _sklearn_disabled = self . name + '__disabled' self . _hyperparameters = hyperparameters if len ( hyperparameters ) > 0 : key_0 = next ( iter ( hyperparameters )) if self . name not in key_0 : self . hyperparameters = hyperparameters else : self . hyperparameters = hyperparameters # self.initalize_hyperparameters = hyperparameters # check if hyperparameters are already in sklearn style # check if hyperparameters are members of the class if self . is_transformer or self . is_estimator : self . _check_hyperparameters ( BaseEstimator ) self . disabled = disabled # check if self.base element needs y for fitting and transforming if hasattr ( self . base_element , 'needs_y' ): self . needs_y = self . base_element . needs_y else : self . needs_y = False # or if it maybe needs covariates for fitting and transforming if hasattr ( self . base_element , 'needs_covariates' ): self . needs_covariates = self . base_element . needs_covariates else : self . needs_covariates = False self . _random_state = False @property def name ( self ): return self . _name @name . setter def name ( self , value ): self . _name = value self . generate_sklearn_hyperparameters ( self . initial_hyperparameters ) @property def hyperparameters ( self ): return self . _hyperparameters @hyperparameters . setter def hyperparameters ( self , value : dict ): self . generate_sklearn_hyperparameters ( value ) def _check_hyperparameters ( self , BaseEstimator ): # check if hyperparameters are members of the class not_supported_hyperparameters = list ( set ([ key . split ( \"__\" )[ - 1 ] for key in self . _hyperparameters . keys () if key . split ( \"__\" )[ - 1 ] != \"disabled\" ]) - set ( BaseEstimator . get_params ( self . base_element ) . keys ())) if not_supported_hyperparameters : error_message = 'ValueError: Set of hyperparameters are not valid, check hyperparameters:' + \\ str ( not_supported_hyperparameters ) logger . error ( error_message ) raise ValueError ( error_message ) def generate_sklearn_hyperparameters ( self , value : dict ): \"\"\" Generates a dictionary according to the sklearn convention of element_name__parameter_name: parameter_value. \"\"\" self . _hyperparameters = {} for attribute , value_list in value . items (): self . _hyperparameters [ self . name + '__' + attribute ] = value_list if self . test_disabled : self . _hyperparameters [ self . _sklearn_disabled ] = [ False , True ] @property def random_state ( self ): return self . _random_state @random_state . setter def random_state ( self , random_state ): self . _random_state = random_state if hasattr ( self , 'elements' ): for el in self . elements : if hasattr ( el , 'random_state' ): el . random_state = self . _random_state if hasattr ( self , \"base_element\" ) and hasattr ( self . base_element , \"random_state\" ): self . base_element . random_state = random_state @property def _estimator_type ( self ): # estimator_type obligation for estimators, is ignored if a transformer is given # prevention of misuse through predict test (predict method available <=> Estimator). est_type = getattr ( self . base_element , '_estimator_type' , None ) if est_type in [ None , 'transformer' ]: if hasattr ( self . base_element , 'predict' ): raise NotImplementedError ( \"Element has predict() method but does not specify whether it is a regressor\" \" or classifier. Remember to inherit from ClassifierMixin or RegressorMixin.\" ) return None else : if est_type not in [ 'classifier' , 'regressor' ]: raise NotImplementedError ( \"Currently, we only support type classifier or regressor.\" \" Is {} .\" . format ( est_type )) if not hasattr ( self . base_element , 'predict' ): raise NotImplementedError ( \"Estimator does not implement predict() method.\" ) return est_type # this is only here because everything inherits from PipelineElement. def __iadd__ ( self , pipe_element ): \"\"\" Add an element to the intern list of elements. Parameters: pipe_element (PipelineElement): The object to add, being either a transformer or an estimator. \"\"\" PipelineElement . sanity_check_element_type_for_building_photon_pipes ( pipe_element , type ( self )) # check if that exact instance has been added before already_added_objects = len ([ i for i in self . elements if i is pipe_element ]) if already_added_objects > 0 : error_msg = \"Cannot add the same instance twice to \" + self . name + \" - \" + str ( type ( self )) logger . error ( error_msg ) raise ValueError ( error_msg ) # check for doubled names: already_existing_element_with_that_name = len ([ i for i in self . elements if i . name == pipe_element . name ]) if already_existing_element_with_that_name > 0 : error_msg = \"Already added a pipeline element with the name \" + pipe_element . name + \" to \" + self . name logger . warning ( error_msg ) warnings . warn ( error_msg ) # check for other items that have been renamed nr_of_existing_elements_with_that_name = len ([ i for i in self . elements if i . name . startswith ( pipe_element . name )]) new_name = pipe_element . name + str ( nr_of_existing_elements_with_that_name + 1 ) while len ([ i for i in self . elements if i . name == new_name ]) > 0 : nr_of_existing_elements_with_that_name += 1 new_name = pipe_element . name + str ( nr_of_existing_elements_with_that_name + 1 ) msg = \"Renaming \" + pipe_element . name + \" in \" + self . name + \" to \" + new_name + \" in \" + self . name logger . warning ( msg ) warnings . warn ( msg ) pipe_element . name = new_name self . elements . append ( pipe_element ) return self def copy_me ( self ): if self . name in PhotonRegistry . ELEMENT_DICTIONARY : # we need initial name to refer to the class to be instantiated (SVC) even though the name might be SVC2 copy = PipelineElement ( self . initial_name , {}, test_disabled = self . test_disabled , disabled = self . disabled , batch_size = self . batch_size , ** self . kwargs ) copy . initial_hyperparameters = self . initial_hyperparameters # in the setter of the name, we use initial hyperparameters to adjust the hyperparameters to the name copy . name = self . name else : if hasattr ( self . base_element , 'copy_me' ): new_base_element = self . base_element . copy_me () else : try : new_base_element = deepcopy ( self . base_element ) except Exception as e : error_msg = \"Cannot copy custom element \" + self . name + \". Please specify a copy_me() method \" \\ \"returning a copy of the object\" logger . error ( error_msg ) raise e # handle custom elements copy = PipelineElement . create ( self . name , new_base_element , hyperparameters = self . hyperparameters , test_disabled = self . test_disabled , disabled = self . disabled , batch_size = self . batch_size , ** self . kwargs ) if self . current_config is not None : copy . set_params ( ** self . current_config ) copy . _random_state = self . _random_state return copy @classmethod def create ( cls , name : str , base_element : BaseEstimator , hyperparameters : dict , test_disabled : bool = False , disabled : bool = False , ** kwargs ): \"\"\" Takes an instantiated object and encapsulates it into the PHOTONAI structure. Add the disabled function and attaches information about the hyperparameters that should be tested. Parameters: name: A string literal encoding the class to be instantiated. base_element: The underlying transformer or estimator class. hyperparameters: Which values/value range should be tested for the hyperparameter. In form of Dict: parameter_name -> HyperparameterElement. test_disabled: If the hyperparameter search should evaluate a complete disabling of the element. disabled: If true, the element is currently disabled and does nothing except return the data it received. **kwargs: Any parameters that should be passed to the object to be instantiated, default parameters. Example: ``` python class RD(BaseEstimator, TransformerMixin): def fit(self, X, y, **kwargs): pass def fit_transform(self, X, y=None, **fit_params): return self.transform(X) def transform(self, X): return X[:, :3] trans = PipelineElement.create('MyTransformer', base_element=RD(), hyperparameters={}) ``` \"\"\" if isinstance ( base_element , type ): raise ValueError ( \"Base element should be an instance but is a class.\" ) return PipelineElement ( name , hyperparameters , test_disabled , disabled , base_element = base_element , ** kwargs ) @property def feature_importances_ ( self ): if hasattr ( self . base_element , 'feature_importances_' ): return self . base_element . feature_importances_ . tolist () elif hasattr ( self . base_element , 'coef_' ): return self . base_element . coef_ . tolist () def generate_config_grid ( self ): config_dict = create_global_config_dict ([ self ]) if len ( config_dict ) > 0 : if self . test_disabled : config_dict . pop ( self . _sklearn_disabled ) config_list = list ( ParameterGrid ( config_dict )) if self . test_disabled : for item in config_list : item [ self . _sklearn_disabled ] = False config_list . append ({ self . _sklearn_disabled : True }) if len ( config_list ) < 2 : config_list . append ({ self . _sklearn_disabled : False }) return config_list else : return [] def get_params ( self , deep : bool = True ): \"\"\" Forwards the get_params request to the wrapped base element. \"\"\" if hasattr ( self . base_element , 'get_params' ): params = self . base_element . get_params ( deep ) params [ \"name\" ] = self . name return params else : return None def set_params ( self , ** kwargs ): \"\"\" Forwards the set_params request to the wrapped base element Takes care of the disabled parameter which is additionally attached by the PHOTON wrapper \"\"\" # this is an ugly hack to approximate the right settings when copying the element self . current_config = kwargs # element disable is a construct used for this container only if self . _sklearn_disabled in kwargs : self . disabled = kwargs [ self . _sklearn_disabled ] del kwargs [ self . _sklearn_disabled ] elif 'disabled' in kwargs : self . disabled = kwargs [ 'disabled' ] del kwargs [ 'disabled' ] self . base_element . set_params ( ** kwargs ) return self def fit ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ): \"\"\" Calls the fit function of the base element. Parameters: X: The array-like training and test data with shape=[N, D], where N is the number of samples and D is the number of features. y: The truth array-like values with shape=[N], where N is the number of samples. **kwargs: Keyword arguments, passed to base_element.predict. Returns: Fitted self. \"\"\" if not self . disabled : obj = self . base_element arg_list = inspect . signature ( obj . fit ) if len ( arg_list . parameters ) > 2 : vals = arg_list . parameters . values () kwargs_param = list ( vals )[ - 1 ] if kwargs_param . kind == kwargs_param . VAR_KEYWORD : obj . fit ( X , y , ** kwargs ) return self obj . fit ( X , y ) return self def __batch_predict ( self , delegate , X , ** kwargs ): if not isinstance ( X , list ) and not isinstance ( X , np . ndarray ): msg = \"Cannot do batching on a single entity.\" logger . warning ( msg ) warnings . warn ( msg ) return delegate ( X , ** kwargs ) # initialize return values processed_y = None nr = PhotonDataHelper . find_n ( X ) batch_idx = 0 for start , stop in PhotonDataHelper . chunker ( nr , self . batch_size ): batch_idx += 1 logger . debug ( self . name + \" is predicting batch \" + str ( batch_idx )) # split data in batches X_batched , y_batched , kwargs_dict_batched = PhotonDataHelper . split_data ( X , None , kwargs , start , stop ) # predict y_pred = delegate ( X_batched , ** kwargs_dict_batched ) processed_y = PhotonDataHelper . stack_data_vertically ( processed_y , y_pred ) return processed_y def __predict ( self , X , ** kwargs ): if not self . disabled : if hasattr ( self . base_element , 'predict' ): return self . adjusted_predict_call ( self . base_element . predict , X , ** kwargs ) else : logger . error ( 'BaseException. base Element should have function ' + 'predict.' ) raise BaseException ( 'base Element should have function predict.' ) else : return X def predict ( self , X : np . ndarray , ** kwargs ) -> np . ndarray : \"\"\" Calls the predict function of the underlying base_element. Parameters: X: The array-like training and test data with shape=[N, D], where N is the number of samples and D is the number of features. **kwargs: Keyword arguments, passed to base_element.predict. Returns: Predictions values. \"\"\" if self . batch_size == 0 : return self . __predict ( X , ** kwargs ) else : return self . __batch_predict ( self . __predict , X , ** kwargs ) def predict_proba ( self , X , ** kwargs ): if self . batch_size == 0 : return self . __predict_proba ( X , ** kwargs ) else : return self . __batch_predict ( self . __predict_proba , X , ** kwargs ) def __predict_proba ( self , X : np . ndarray , ** kwargs ): \"\"\" Predict probabilities base element needs predict_proba() function, otherwise throw base exception. \"\"\" if not self . disabled : if hasattr ( self . base_element , 'predict_proba' ): # todo: here, I used delegate call (same as below in predict within the transform call) #return self.base_element.predict_proba(X) return self . adjusted_predict_call ( self . base_element . predict_proba , X , ** kwargs ) else : # todo: in case _final_estimator is a Branch, we do not know beforehand it the base elements will # have a predict_proba -> if not, just return None (@Ramona, does this make sense?) # logger.error('BaseException. base Element should have \"predict_proba\" function.') # raise BaseException('base Element should have predict_proba function.') return None return X def __transform ( self , X , y = None , ** kwargs ): if not self . disabled : if hasattr ( self . base_element , 'transform' ): return self . adjusted_delegate_call ( self . base_element . transform , X , y , ** kwargs ) elif hasattr ( self . base_element , 'predict' ): return self . predict ( X , ** kwargs ), y , kwargs else : logger . error ( 'BaseException: transform-predict-mess' ) raise BaseException ( 'transform-predict-mess' ) else : return X , y , kwargs def transform ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ) -> ( np . ndarray , np . ndarray , dict ): \"\"\" Calls transform on the base element. In case there is no transform method, calls predict. This is used if we are using an estimator as a preprocessing step. Parameters: X: The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. y: The truth array-like values with shape=[N], where N is the number of samples. **kwargs: Keyword arguments, passed to base_element.transform. Returns: (X, y) in transformed version and original kwargs. \"\"\" if self . batch_size == 0 : Xt , yt , kwargs = self . __transform ( X , y , ** kwargs ) else : Xt , yt , kwargs = self . __batch_transform ( X , y , ** kwargs ) if all ( hasattr ( data , \"shape\" ) for data in [ X , Xt ]) and all ( len ( data . shape ) > 1 for data in [ X , Xt ]): self . reduce_dimension = ( Xt . shape [ 1 ] < X . shape [ 1 ]) return Xt , yt , kwargs def inverse_transform ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ) -> ( np . ndarray , np . ndarray , dict ): \"\"\" Calls inverse_transform on the base element. When the dimension is preserved: transformers without inverse returns original input. Parameters: X: The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. y: The truth array-like values with shape=[N], where N is the number of samples. **kwargs: Keyword arguments, passed to base_element.transform. Raises: NotImplementedError: Thrown when there is a dimensional reduction but no inverse is defined. Returns: (X, y, kwargs) in back-transformed version. \"\"\" if hasattr ( self . base_element , 'inverse_transform' ): # todo: check this X , y , kwargs = self . adjusted_delegate_call ( self . base_element . inverse_transform , X , y , ** kwargs ) elif self . is_transformer and self . reduce_dimension : msg = \" {} has no inverse_transform, but element reduce dimesions.\" . format ( self . name ) logger . error ( msg ) raise NotImplementedError ( msg ) return X , y , kwargs def __batch_transform ( self , X , y = None , ** kwargs ): if not isinstance ( X , list ) and not isinstance ( X , np . ndarray ): warning = \"Cannot do batching on a single entity.\" logger . warning ( warning ) warnings . warn ( warning ) return self . __transform ( X , y , ** kwargs ) # initialize return values processed_X = None processed_y = None processed_kwargs = dict () nr = PhotonDataHelper . find_n ( X ) batch_idx = 0 for start , stop in PhotonDataHelper . chunker ( nr , self . batch_size ): batch_idx += 1 # split data in batches X_batched , y_batched , kwargs_dict_batched = PhotonDataHelper . split_data ( X , y , kwargs , start , stop ) actual_batch_size = PhotonDataHelper . find_n ( X_batched ) logger . debug ( self . name + \" is transforming batch \" + str ( batch_idx ) + \" with \" + str ( actual_batch_size ) + \" items.\" ) # call transform X_new , y_new , kwargs_new = self . adjusted_delegate_call ( self . base_element . transform , X_batched , y_batched , ** kwargs_dict_batched ) # stack results processed_X , processed_y , processed_kwargs = PhotonDataHelper . join_data ( processed_X , X_new , processed_y , y_new , processed_kwargs , kwargs_new ) return processed_X , processed_y , processed_kwargs def adjusted_delegate_call ( self , delegate , X , y , ** kwargs ): # Case| transforms X | needs_y | needs_covariates # ------------------------------------------------------- # 1 yes no no = transform(X) -> returns Xt # todo: case does not exist any longer # 2 yes yes no = transform(X, y) -> returns Xt, yt # 3 yes yes yes = transform(X, y, kwargs) -> returns Xt, yt, kwargst # 4 yes no yes = transform(X, kwargs) -> returns Xt, kwargst # 5 no yes or no yes or no = NOT ALLOWED # todo: we don't need to check for Switch, Stack or Branch since those classes define # needs_y and needs_covariates in their __init__() if self . needs_y : # if we dont have any target vector, we are in \"predict\"-mode although we are currently transforming # in this case, we want to skip the transformation and pass X, None and kwargs onwards # so basically, we skip all training_only elements # todo: I think, there's no way around this if we want to pass y and kwargs down to children of Switch and Branch if isinstance ( self , ( Switch , Branch , Preprocessing )): X , y , kwargs = delegate ( X , y , ** kwargs ) else : if y is not None : # todo: in case a method needs y, we should also always pass kwargs # i.e. if we change the number of samples, we also need to apply that change to all kwargs # todo: talk to Ramona! Maybe we actually DO need this case if self . needs_covariates : X , y , kwargs = delegate ( X , y , ** kwargs ) else : X , y = delegate ( X , y ) elif self . needs_covariates : X , kwargs = delegate ( X , ** kwargs ) else : X = delegate ( X ) return X , y , kwargs def adjusted_predict_call ( self , delegate , X , ** kwargs ): if self . needs_covariates : return delegate ( X , ** kwargs ) else : return delegate ( X ) def score ( self , X_test : np . ndarray , y_test : np . ndarray ) -> float : \"\"\" Calls the score function on the base element. Parameters: X_test: Input test data to score on. y_test: Input true targets to score on. Returns: A goodness of fit measure or a likelihood of unseen data. \"\"\" return self . base_element . score ( X_test , y_test ) def prettify_config_output ( self , config_name : str , config_value , return_dict : bool = False ): \"\"\"Make hyperparameter combinations human readable \"\"\" if config_name == \"disabled\" and config_value is False : if return_dict : return { 'disabled' : False } else : return \"disabled = False\" else : if return_dict : return { config_name : config_value } else : return config_name + '=' + str ( config_value ) @staticmethod def sanity_check_element_type_for_building_photon_pipes ( pipe_element , type_of_self ): if ( not isinstance ( pipe_element , PipelineElement ) and not isinstance ( pipe_element , PhotonNative )) or isinstance ( pipe_element , Preprocessing ): raise TypeError ( str ( type_of_self ) + \" only accepts PHOTON elements. Cannot add element of type \" + str ( type ( pipe_element ))) __iadd__ ( self , pipe_element ) special Add an element to the intern list of elements. Parameters: Name Type Description Default pipe_element PipelineElement The object to add, being either a transformer or an estimator. required Source code in photonai/base/photon_elements.py def __iadd__ ( self , pipe_element ): \"\"\" Add an element to the intern list of elements. Parameters: pipe_element (PipelineElement): The object to add, being either a transformer or an estimator. \"\"\" PipelineElement . sanity_check_element_type_for_building_photon_pipes ( pipe_element , type ( self )) # check if that exact instance has been added before already_added_objects = len ([ i for i in self . elements if i is pipe_element ]) if already_added_objects > 0 : error_msg = \"Cannot add the same instance twice to \" + self . name + \" - \" + str ( type ( self )) logger . error ( error_msg ) raise ValueError ( error_msg ) # check for doubled names: already_existing_element_with_that_name = len ([ i for i in self . elements if i . name == pipe_element . name ]) if already_existing_element_with_that_name > 0 : error_msg = \"Already added a pipeline element with the name \" + pipe_element . name + \" to \" + self . name logger . warning ( error_msg ) warnings . warn ( error_msg ) # check for other items that have been renamed nr_of_existing_elements_with_that_name = len ([ i for i in self . elements if i . name . startswith ( pipe_element . name )]) new_name = pipe_element . name + str ( nr_of_existing_elements_with_that_name + 1 ) while len ([ i for i in self . elements if i . name == new_name ]) > 0 : nr_of_existing_elements_with_that_name += 1 new_name = pipe_element . name + str ( nr_of_existing_elements_with_that_name + 1 ) msg = \"Renaming \" + pipe_element . name + \" in \" + self . name + \" to \" + new_name + \" in \" + self . name logger . warning ( msg ) warnings . warn ( msg ) pipe_element . name = new_name self . elements . append ( pipe_element ) return self __init__ ( self , name , hyperparameters = None , test_disabled = False , disabled = False , base_element = None , batch_size = 0 , ** kwargs ) special Takes a string literal and transforms it into an object of the associated class (see PhotonCore.JSON). Parameters: Name Type Description Default name str A string literal encoding the class to be instantiated. required hyperparameters dict Which values/value range should be tested for the hyperparameter. In form of Dict: parameter_name -> HyperparameterElement. None test_disabled bool If the hyperparameter search should evaluate a complete disabling of the element. False disabled bool If true, the element is currently disabled and does nothing except return the data it received. False base_element BaseEstimator The underlying BaseEstimator. If not given the instantiation per string identifier takes place. None batch_size int Size of the division on which is calculated separately. 0 **kwargs Any parameters that should be passed to the object to be instantiated, default parameters. {} Source code in photonai/base/photon_elements.py def __init__ ( self , name : str , hyperparameters : dict = None , test_disabled : bool = False , disabled : bool = False , base_element : BaseEstimator = None , batch_size : int = 0 , ** kwargs ) -> None : \"\"\" Takes a string literal and transforms it into an object of the associated class (see PhotonCore.JSON). Parameters: name: A string literal encoding the class to be instantiated. hyperparameters: Which values/value range should be tested for the hyperparameter. In form of Dict: parameter_name -> HyperparameterElement. test_disabled: If the hyperparameter search should evaluate a complete disabling of the element. disabled: If true, the element is currently disabled and does nothing except return the data it received. base_element: The underlying BaseEstimator. If not given the instantiation per string identifier takes place. batch_size: Size of the division on which is calculated separately. **kwargs: Any parameters that should be passed to the object to be instantiated, default parameters. \"\"\" if hyperparameters is None : hyperparameters = {} if base_element is None : # Registering Pipeline Elements if len ( PhotonRegistry . ELEMENT_DICTIONARY ) == 0 : registry = PhotonRegistry if name not in PhotonRegistry . ELEMENT_DICTIONARY : # try to reload PhotonRegistry . ELEMENT_DICTIONARY = PhotonRegistry () . get_package_info () if name in PhotonRegistry . ELEMENT_DICTIONARY : try : desired_class_info = PhotonRegistry . ELEMENT_DICTIONARY [ name ] desired_class_home = desired_class_info [ 0 ] desired_class_name = desired_class_info [ 1 ] imported_module = importlib . import_module ( desired_class_home ) desired_class = getattr ( imported_module , desired_class_name ) self . base_element = desired_class ( ** kwargs ) except AttributeError as ae : logger . error ( 'ValueError: Could not find according class:' + str ( PhotonRegistry . ELEMENT_DICTIONARY [ name ])) raise ValueError ( 'Could not find according class:' , PhotonRegistry . ELEMENT_DICTIONARY [ name ]) else : # if even after reload the element does not appear, it is not supported logger . error ( 'Element not supported right now:' + name ) raise NameError ( 'Element not supported right now:' , name ) else : self . base_element = base_element self . is_transformer = hasattr ( self . base_element , \"transform\" ) self . reduce_dimension = False # boolean - set on transform method self . is_estimator = hasattr ( self . base_element , \"predict\" ) self . _name = name self . initial_name = str ( name ) self . kwargs = kwargs self . current_config = None self . batch_size = batch_size self . test_disabled = test_disabled self . initial_hyperparameters = dict ( hyperparameters ) self . _sklearn_disabled = self . name + '__disabled' self . _hyperparameters = hyperparameters if len ( hyperparameters ) > 0 : key_0 = next ( iter ( hyperparameters )) if self . name not in key_0 : self . hyperparameters = hyperparameters else : self . hyperparameters = hyperparameters # self.initalize_hyperparameters = hyperparameters # check if hyperparameters are already in sklearn style # check if hyperparameters are members of the class if self . is_transformer or self . is_estimator : self . _check_hyperparameters ( BaseEstimator ) self . disabled = disabled # check if self.base element needs y for fitting and transforming if hasattr ( self . base_element , 'needs_y' ): self . needs_y = self . base_element . needs_y else : self . needs_y = False # or if it maybe needs covariates for fitting and transforming if hasattr ( self . base_element , 'needs_covariates' ): self . needs_covariates = self . base_element . needs_covariates else : self . needs_covariates = False self . _random_state = False create ( name , base_element , hyperparameters , test_disabled = False , disabled = False , ** kwargs ) classmethod Takes an instantiated object and encapsulates it into the PHOTONAI structure. Add the disabled function and attaches information about the hyperparameters that should be tested. Parameters: Name Type Description Default name str A string literal encoding the class to be instantiated. required base_element BaseEstimator The underlying transformer or estimator class. required hyperparameters dict Which values/value range should be tested for the hyperparameter. In form of Dict: parameter_name -> HyperparameterElement. required test_disabled bool If the hyperparameter search should evaluate a complete disabling of the element. False disabled bool If true, the element is currently disabled and does nothing except return the data it received. False **kwargs Any parameters that should be passed to the object to be instantiated, default parameters. {} Examples: 1 2 3 4 5 6 7 8 9 10 11 12 class RD ( BaseEstimator , TransformerMixin ): def fit ( self , X , y , ** kwargs ): pass def fit_transform ( self , X , y = None , ** fit_params ): return self . transform ( X ) def transform ( self , X ): return X [:, : 3 ] trans = PipelineElement . create ( 'MyTransformer' , base_element = RD (), hyperparameters = {}) Source code in photonai/base/photon_elements.py @classmethod def create ( cls , name : str , base_element : BaseEstimator , hyperparameters : dict , test_disabled : bool = False , disabled : bool = False , ** kwargs ): \"\"\" Takes an instantiated object and encapsulates it into the PHOTONAI structure. Add the disabled function and attaches information about the hyperparameters that should be tested. Parameters: name: A string literal encoding the class to be instantiated. base_element: The underlying transformer or estimator class. hyperparameters: Which values/value range should be tested for the hyperparameter. In form of Dict: parameter_name -> HyperparameterElement. test_disabled: If the hyperparameter search should evaluate a complete disabling of the element. disabled: If true, the element is currently disabled and does nothing except return the data it received. **kwargs: Any parameters that should be passed to the object to be instantiated, default parameters. Example: ``` python class RD(BaseEstimator, TransformerMixin): def fit(self, X, y, **kwargs): pass def fit_transform(self, X, y=None, **fit_params): return self.transform(X) def transform(self, X): return X[:, :3] trans = PipelineElement.create('MyTransformer', base_element=RD(), hyperparameters={}) ``` \"\"\" if isinstance ( base_element , type ): raise ValueError ( \"Base element should be an instance but is a class.\" ) return PipelineElement ( name , hyperparameters , test_disabled , disabled , base_element = base_element , ** kwargs ) fit ( self , X , y = None , ** kwargs ) Calls the fit function of the base element. Parameters: Name Type Description Default X ndarray The array-like training and test data with shape=[N, D], where N is the number of samples and D is the number of features. required y ndarray The truth array-like values with shape=[N], where N is the number of samples. None **kwargs Keyword arguments, passed to base_element.predict. {} Returns: Type Description Fitted self. Source code in photonai/base/photon_elements.py def fit ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ): \"\"\" Calls the fit function of the base element. Parameters: X: The array-like training and test data with shape=[N, D], where N is the number of samples and D is the number of features. y: The truth array-like values with shape=[N], where N is the number of samples. **kwargs: Keyword arguments, passed to base_element.predict. Returns: Fitted self. \"\"\" if not self . disabled : obj = self . base_element arg_list = inspect . signature ( obj . fit ) if len ( arg_list . parameters ) > 2 : vals = arg_list . parameters . values () kwargs_param = list ( vals )[ - 1 ] if kwargs_param . kind == kwargs_param . VAR_KEYWORD : obj . fit ( X , y , ** kwargs ) return self obj . fit ( X , y ) return self inverse_transform ( self , X , y = None , ** kwargs ) Calls inverse_transform on the base element. When the dimension is preserved: transformers without inverse returns original input. Parameters: Name Type Description Default X ndarray The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. required y ndarray The truth array-like values with shape=[N], where N is the number of samples. None **kwargs Keyword arguments, passed to base_element.transform. {} Returns: Type Description (<class 'numpy.ndarray'>, <class 'numpy.ndarray'>, <class 'dict'>) (X, y, kwargs) in back-transformed version. Source code in photonai/base/photon_elements.py def inverse_transform ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ) -> ( np . ndarray , np . ndarray , dict ): \"\"\" Calls inverse_transform on the base element. When the dimension is preserved: transformers without inverse returns original input. Parameters: X: The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. y: The truth array-like values with shape=[N], where N is the number of samples. **kwargs: Keyword arguments, passed to base_element.transform. Raises: NotImplementedError: Thrown when there is a dimensional reduction but no inverse is defined. Returns: (X, y, kwargs) in back-transformed version. \"\"\" if hasattr ( self . base_element , 'inverse_transform' ): # todo: check this X , y , kwargs = self . adjusted_delegate_call ( self . base_element . inverse_transform , X , y , ** kwargs ) elif self . is_transformer and self . reduce_dimension : msg = \" {} has no inverse_transform, but element reduce dimesions.\" . format ( self . name ) logger . error ( msg ) raise NotImplementedError ( msg ) return X , y , kwargs predict ( self , X , ** kwargs ) Calls the predict function of the underlying base_element. Parameters: Name Type Description Default X ndarray The array-like training and test data with shape=[N, D], where N is the number of samples and D is the number of features. required **kwargs Keyword arguments, passed to base_element.predict. {} Returns: Type Description ndarray Predictions values. Source code in photonai/base/photon_elements.py def predict ( self , X : np . ndarray , ** kwargs ) -> np . ndarray : \"\"\" Calls the predict function of the underlying base_element. Parameters: X: The array-like training and test data with shape=[N, D], where N is the number of samples and D is the number of features. **kwargs: Keyword arguments, passed to base_element.predict. Returns: Predictions values. \"\"\" if self . batch_size == 0 : return self . __predict ( X , ** kwargs ) else : return self . __batch_predict ( self . __predict , X , ** kwargs ) score ( self , X_test , y_test ) Calls the score function on the base element. Parameters: Name Type Description Default X_test ndarray Input test data to score on. required y_test ndarray Input true targets to score on. required Returns: Type Description float A goodness of fit measure or a likelihood of unseen data. Source code in photonai/base/photon_elements.py def score ( self , X_test : np . ndarray , y_test : np . ndarray ) -> float : \"\"\" Calls the score function on the base element. Parameters: X_test: Input test data to score on. y_test: Input true targets to score on. Returns: A goodness of fit measure or a likelihood of unseen data. \"\"\" return self . base_element . score ( X_test , y_test ) transform ( self , X , y = None , ** kwargs ) Calls transform on the base element. In case there is no transform method, calls predict. This is used if we are using an estimator as a preprocessing step. Parameters: Name Type Description Default X ndarray The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. required y ndarray The truth array-like values with shape=[N], where N is the number of samples. None **kwargs Keyword arguments, passed to base_element.transform. {} Returns: Type Description (<class 'numpy.ndarray'>, <class 'numpy.ndarray'>, <class 'dict'>) (X, y) in transformed version and original kwargs. Source code in photonai/base/photon_elements.py def transform ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ) -> ( np . ndarray , np . ndarray , dict ): \"\"\" Calls transform on the base element. In case there is no transform method, calls predict. This is used if we are using an estimator as a preprocessing step. Parameters: X: The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. y: The truth array-like values with shape=[N], where N is the number of samples. **kwargs: Keyword arguments, passed to base_element.transform. Returns: (X, y) in transformed version and original kwargs. \"\"\" if self . batch_size == 0 : Xt , yt , kwargs = self . __transform ( X , y , ** kwargs ) else : Xt , yt , kwargs = self . __batch_transform ( X , y , ** kwargs ) if all ( hasattr ( data , \"shape\" ) for data in [ X , Xt ]) and all ( len ( data . shape ) > 1 for data in [ X , Xt ]): self . reduce_dimension = ( Xt . shape [ 1 ] < X . shape [ 1 ]) return Xt , yt , kwargs","title":"PipelineElement"},{"location":"api/base/pipeline_element/#documentation-for-pipelineelement","text":"PHOTONAI wrapper class for any transformer or estimator in the pipeline. So called PHOTONAI PipelineElements can be added to the Hyperpipe, each of them being a data-processing method or a learning algorithm. By choosing, combining data-processing methods and algorithms, and arranging them with the PHOTONAI classes, simple and complex pipeline architectures can be designed rapidly. The PHOTONAI PipelineElement implements several helpful features: Saves the hyperparameters that should be tested and creates a grid of all hyperparameter configurations. Enables fast and rapid instantiation of pipeline elements per string identifier, e.g 'svc' creates an sklearn.svm.SVC object. Attaches a \"disable\" switch to every element in the pipeline in order to test a complete disable. Source code in photonai/base/photon_elements.py class PipelineElement ( BaseEstimator ): \"\"\" PHOTONAI wrapper class for any transformer or estimator in the pipeline. So called PHOTONAI PipelineElements can be added to the Hyperpipe, each of them being a data-processing method or a learning algorithm. By choosing, combining data-processing methods and algorithms, and arranging them with the PHOTONAI classes, simple and complex pipeline architectures can be designed rapidly. The PHOTONAI PipelineElement implements several helpful features: - Saves the hyperparameters that should be tested and creates a grid of all hyperparameter configurations. - Enables fast and rapid instantiation of pipeline elements per string identifier, e.g 'svc' creates an sklearn.svm.SVC object. - Attaches a \"disable\" switch to every element in the pipeline in order to test a complete disable. \"\"\" def __init__ ( self , name : str , hyperparameters : dict = None , test_disabled : bool = False , disabled : bool = False , base_element : BaseEstimator = None , batch_size : int = 0 , ** kwargs ) -> None : \"\"\" Takes a string literal and transforms it into an object of the associated class (see PhotonCore.JSON). Parameters: name: A string literal encoding the class to be instantiated. hyperparameters: Which values/value range should be tested for the hyperparameter. In form of Dict: parameter_name -> HyperparameterElement. test_disabled: If the hyperparameter search should evaluate a complete disabling of the element. disabled: If true, the element is currently disabled and does nothing except return the data it received. base_element: The underlying BaseEstimator. If not given the instantiation per string identifier takes place. batch_size: Size of the division on which is calculated separately. **kwargs: Any parameters that should be passed to the object to be instantiated, default parameters. \"\"\" if hyperparameters is None : hyperparameters = {} if base_element is None : # Registering Pipeline Elements if len ( PhotonRegistry . ELEMENT_DICTIONARY ) == 0 : registry = PhotonRegistry if name not in PhotonRegistry . ELEMENT_DICTIONARY : # try to reload PhotonRegistry . ELEMENT_DICTIONARY = PhotonRegistry () . get_package_info () if name in PhotonRegistry . ELEMENT_DICTIONARY : try : desired_class_info = PhotonRegistry . ELEMENT_DICTIONARY [ name ] desired_class_home = desired_class_info [ 0 ] desired_class_name = desired_class_info [ 1 ] imported_module = importlib . import_module ( desired_class_home ) desired_class = getattr ( imported_module , desired_class_name ) self . base_element = desired_class ( ** kwargs ) except AttributeError as ae : logger . error ( 'ValueError: Could not find according class:' + str ( PhotonRegistry . ELEMENT_DICTIONARY [ name ])) raise ValueError ( 'Could not find according class:' , PhotonRegistry . ELEMENT_DICTIONARY [ name ]) else : # if even after reload the element does not appear, it is not supported logger . error ( 'Element not supported right now:' + name ) raise NameError ( 'Element not supported right now:' , name ) else : self . base_element = base_element self . is_transformer = hasattr ( self . base_element , \"transform\" ) self . reduce_dimension = False # boolean - set on transform method self . is_estimator = hasattr ( self . base_element , \"predict\" ) self . _name = name self . initial_name = str ( name ) self . kwargs = kwargs self . current_config = None self . batch_size = batch_size self . test_disabled = test_disabled self . initial_hyperparameters = dict ( hyperparameters ) self . _sklearn_disabled = self . name + '__disabled' self . _hyperparameters = hyperparameters if len ( hyperparameters ) > 0 : key_0 = next ( iter ( hyperparameters )) if self . name not in key_0 : self . hyperparameters = hyperparameters else : self . hyperparameters = hyperparameters # self.initalize_hyperparameters = hyperparameters # check if hyperparameters are already in sklearn style # check if hyperparameters are members of the class if self . is_transformer or self . is_estimator : self . _check_hyperparameters ( BaseEstimator ) self . disabled = disabled # check if self.base element needs y for fitting and transforming if hasattr ( self . base_element , 'needs_y' ): self . needs_y = self . base_element . needs_y else : self . needs_y = False # or if it maybe needs covariates for fitting and transforming if hasattr ( self . base_element , 'needs_covariates' ): self . needs_covariates = self . base_element . needs_covariates else : self . needs_covariates = False self . _random_state = False @property def name ( self ): return self . _name @name . setter def name ( self , value ): self . _name = value self . generate_sklearn_hyperparameters ( self . initial_hyperparameters ) @property def hyperparameters ( self ): return self . _hyperparameters @hyperparameters . setter def hyperparameters ( self , value : dict ): self . generate_sklearn_hyperparameters ( value ) def _check_hyperparameters ( self , BaseEstimator ): # check if hyperparameters are members of the class not_supported_hyperparameters = list ( set ([ key . split ( \"__\" )[ - 1 ] for key in self . _hyperparameters . keys () if key . split ( \"__\" )[ - 1 ] != \"disabled\" ]) - set ( BaseEstimator . get_params ( self . base_element ) . keys ())) if not_supported_hyperparameters : error_message = 'ValueError: Set of hyperparameters are not valid, check hyperparameters:' + \\ str ( not_supported_hyperparameters ) logger . error ( error_message ) raise ValueError ( error_message ) def generate_sklearn_hyperparameters ( self , value : dict ): \"\"\" Generates a dictionary according to the sklearn convention of element_name__parameter_name: parameter_value. \"\"\" self . _hyperparameters = {} for attribute , value_list in value . items (): self . _hyperparameters [ self . name + '__' + attribute ] = value_list if self . test_disabled : self . _hyperparameters [ self . _sklearn_disabled ] = [ False , True ] @property def random_state ( self ): return self . _random_state @random_state . setter def random_state ( self , random_state ): self . _random_state = random_state if hasattr ( self , 'elements' ): for el in self . elements : if hasattr ( el , 'random_state' ): el . random_state = self . _random_state if hasattr ( self , \"base_element\" ) and hasattr ( self . base_element , \"random_state\" ): self . base_element . random_state = random_state @property def _estimator_type ( self ): # estimator_type obligation for estimators, is ignored if a transformer is given # prevention of misuse through predict test (predict method available <=> Estimator). est_type = getattr ( self . base_element , '_estimator_type' , None ) if est_type in [ None , 'transformer' ]: if hasattr ( self . base_element , 'predict' ): raise NotImplementedError ( \"Element has predict() method but does not specify whether it is a regressor\" \" or classifier. Remember to inherit from ClassifierMixin or RegressorMixin.\" ) return None else : if est_type not in [ 'classifier' , 'regressor' ]: raise NotImplementedError ( \"Currently, we only support type classifier or regressor.\" \" Is {} .\" . format ( est_type )) if not hasattr ( self . base_element , 'predict' ): raise NotImplementedError ( \"Estimator does not implement predict() method.\" ) return est_type # this is only here because everything inherits from PipelineElement. def __iadd__ ( self , pipe_element ): \"\"\" Add an element to the intern list of elements. Parameters: pipe_element (PipelineElement): The object to add, being either a transformer or an estimator. \"\"\" PipelineElement . sanity_check_element_type_for_building_photon_pipes ( pipe_element , type ( self )) # check if that exact instance has been added before already_added_objects = len ([ i for i in self . elements if i is pipe_element ]) if already_added_objects > 0 : error_msg = \"Cannot add the same instance twice to \" + self . name + \" - \" + str ( type ( self )) logger . error ( error_msg ) raise ValueError ( error_msg ) # check for doubled names: already_existing_element_with_that_name = len ([ i for i in self . elements if i . name == pipe_element . name ]) if already_existing_element_with_that_name > 0 : error_msg = \"Already added a pipeline element with the name \" + pipe_element . name + \" to \" + self . name logger . warning ( error_msg ) warnings . warn ( error_msg ) # check for other items that have been renamed nr_of_existing_elements_with_that_name = len ([ i for i in self . elements if i . name . startswith ( pipe_element . name )]) new_name = pipe_element . name + str ( nr_of_existing_elements_with_that_name + 1 ) while len ([ i for i in self . elements if i . name == new_name ]) > 0 : nr_of_existing_elements_with_that_name += 1 new_name = pipe_element . name + str ( nr_of_existing_elements_with_that_name + 1 ) msg = \"Renaming \" + pipe_element . name + \" in \" + self . name + \" to \" + new_name + \" in \" + self . name logger . warning ( msg ) warnings . warn ( msg ) pipe_element . name = new_name self . elements . append ( pipe_element ) return self def copy_me ( self ): if self . name in PhotonRegistry . ELEMENT_DICTIONARY : # we need initial name to refer to the class to be instantiated (SVC) even though the name might be SVC2 copy = PipelineElement ( self . initial_name , {}, test_disabled = self . test_disabled , disabled = self . disabled , batch_size = self . batch_size , ** self . kwargs ) copy . initial_hyperparameters = self . initial_hyperparameters # in the setter of the name, we use initial hyperparameters to adjust the hyperparameters to the name copy . name = self . name else : if hasattr ( self . base_element , 'copy_me' ): new_base_element = self . base_element . copy_me () else : try : new_base_element = deepcopy ( self . base_element ) except Exception as e : error_msg = \"Cannot copy custom element \" + self . name + \". Please specify a copy_me() method \" \\ \"returning a copy of the object\" logger . error ( error_msg ) raise e # handle custom elements copy = PipelineElement . create ( self . name , new_base_element , hyperparameters = self . hyperparameters , test_disabled = self . test_disabled , disabled = self . disabled , batch_size = self . batch_size , ** self . kwargs ) if self . current_config is not None : copy . set_params ( ** self . current_config ) copy . _random_state = self . _random_state return copy @classmethod def create ( cls , name : str , base_element : BaseEstimator , hyperparameters : dict , test_disabled : bool = False , disabled : bool = False , ** kwargs ): \"\"\" Takes an instantiated object and encapsulates it into the PHOTONAI structure. Add the disabled function and attaches information about the hyperparameters that should be tested. Parameters: name: A string literal encoding the class to be instantiated. base_element: The underlying transformer or estimator class. hyperparameters: Which values/value range should be tested for the hyperparameter. In form of Dict: parameter_name -> HyperparameterElement. test_disabled: If the hyperparameter search should evaluate a complete disabling of the element. disabled: If true, the element is currently disabled and does nothing except return the data it received. **kwargs: Any parameters that should be passed to the object to be instantiated, default parameters. Example: ``` python class RD(BaseEstimator, TransformerMixin): def fit(self, X, y, **kwargs): pass def fit_transform(self, X, y=None, **fit_params): return self.transform(X) def transform(self, X): return X[:, :3] trans = PipelineElement.create('MyTransformer', base_element=RD(), hyperparameters={}) ``` \"\"\" if isinstance ( base_element , type ): raise ValueError ( \"Base element should be an instance but is a class.\" ) return PipelineElement ( name , hyperparameters , test_disabled , disabled , base_element = base_element , ** kwargs ) @property def feature_importances_ ( self ): if hasattr ( self . base_element , 'feature_importances_' ): return self . base_element . feature_importances_ . tolist () elif hasattr ( self . base_element , 'coef_' ): return self . base_element . coef_ . tolist () def generate_config_grid ( self ): config_dict = create_global_config_dict ([ self ]) if len ( config_dict ) > 0 : if self . test_disabled : config_dict . pop ( self . _sklearn_disabled ) config_list = list ( ParameterGrid ( config_dict )) if self . test_disabled : for item in config_list : item [ self . _sklearn_disabled ] = False config_list . append ({ self . _sklearn_disabled : True }) if len ( config_list ) < 2 : config_list . append ({ self . _sklearn_disabled : False }) return config_list else : return [] def get_params ( self , deep : bool = True ): \"\"\" Forwards the get_params request to the wrapped base element. \"\"\" if hasattr ( self . base_element , 'get_params' ): params = self . base_element . get_params ( deep ) params [ \"name\" ] = self . name return params else : return None def set_params ( self , ** kwargs ): \"\"\" Forwards the set_params request to the wrapped base element Takes care of the disabled parameter which is additionally attached by the PHOTON wrapper \"\"\" # this is an ugly hack to approximate the right settings when copying the element self . current_config = kwargs # element disable is a construct used for this container only if self . _sklearn_disabled in kwargs : self . disabled = kwargs [ self . _sklearn_disabled ] del kwargs [ self . _sklearn_disabled ] elif 'disabled' in kwargs : self . disabled = kwargs [ 'disabled' ] del kwargs [ 'disabled' ] self . base_element . set_params ( ** kwargs ) return self def fit ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ): \"\"\" Calls the fit function of the base element. Parameters: X: The array-like training and test data with shape=[N, D], where N is the number of samples and D is the number of features. y: The truth array-like values with shape=[N], where N is the number of samples. **kwargs: Keyword arguments, passed to base_element.predict. Returns: Fitted self. \"\"\" if not self . disabled : obj = self . base_element arg_list = inspect . signature ( obj . fit ) if len ( arg_list . parameters ) > 2 : vals = arg_list . parameters . values () kwargs_param = list ( vals )[ - 1 ] if kwargs_param . kind == kwargs_param . VAR_KEYWORD : obj . fit ( X , y , ** kwargs ) return self obj . fit ( X , y ) return self def __batch_predict ( self , delegate , X , ** kwargs ): if not isinstance ( X , list ) and not isinstance ( X , np . ndarray ): msg = \"Cannot do batching on a single entity.\" logger . warning ( msg ) warnings . warn ( msg ) return delegate ( X , ** kwargs ) # initialize return values processed_y = None nr = PhotonDataHelper . find_n ( X ) batch_idx = 0 for start , stop in PhotonDataHelper . chunker ( nr , self . batch_size ): batch_idx += 1 logger . debug ( self . name + \" is predicting batch \" + str ( batch_idx )) # split data in batches X_batched , y_batched , kwargs_dict_batched = PhotonDataHelper . split_data ( X , None , kwargs , start , stop ) # predict y_pred = delegate ( X_batched , ** kwargs_dict_batched ) processed_y = PhotonDataHelper . stack_data_vertically ( processed_y , y_pred ) return processed_y def __predict ( self , X , ** kwargs ): if not self . disabled : if hasattr ( self . base_element , 'predict' ): return self . adjusted_predict_call ( self . base_element . predict , X , ** kwargs ) else : logger . error ( 'BaseException. base Element should have function ' + 'predict.' ) raise BaseException ( 'base Element should have function predict.' ) else : return X def predict ( self , X : np . ndarray , ** kwargs ) -> np . ndarray : \"\"\" Calls the predict function of the underlying base_element. Parameters: X: The array-like training and test data with shape=[N, D], where N is the number of samples and D is the number of features. **kwargs: Keyword arguments, passed to base_element.predict. Returns: Predictions values. \"\"\" if self . batch_size == 0 : return self . __predict ( X , ** kwargs ) else : return self . __batch_predict ( self . __predict , X , ** kwargs ) def predict_proba ( self , X , ** kwargs ): if self . batch_size == 0 : return self . __predict_proba ( X , ** kwargs ) else : return self . __batch_predict ( self . __predict_proba , X , ** kwargs ) def __predict_proba ( self , X : np . ndarray , ** kwargs ): \"\"\" Predict probabilities base element needs predict_proba() function, otherwise throw base exception. \"\"\" if not self . disabled : if hasattr ( self . base_element , 'predict_proba' ): # todo: here, I used delegate call (same as below in predict within the transform call) #return self.base_element.predict_proba(X) return self . adjusted_predict_call ( self . base_element . predict_proba , X , ** kwargs ) else : # todo: in case _final_estimator is a Branch, we do not know beforehand it the base elements will # have a predict_proba -> if not, just return None (@Ramona, does this make sense?) # logger.error('BaseException. base Element should have \"predict_proba\" function.') # raise BaseException('base Element should have predict_proba function.') return None return X def __transform ( self , X , y = None , ** kwargs ): if not self . disabled : if hasattr ( self . base_element , 'transform' ): return self . adjusted_delegate_call ( self . base_element . transform , X , y , ** kwargs ) elif hasattr ( self . base_element , 'predict' ): return self . predict ( X , ** kwargs ), y , kwargs else : logger . error ( 'BaseException: transform-predict-mess' ) raise BaseException ( 'transform-predict-mess' ) else : return X , y , kwargs def transform ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ) -> ( np . ndarray , np . ndarray , dict ): \"\"\" Calls transform on the base element. In case there is no transform method, calls predict. This is used if we are using an estimator as a preprocessing step. Parameters: X: The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. y: The truth array-like values with shape=[N], where N is the number of samples. **kwargs: Keyword arguments, passed to base_element.transform. Returns: (X, y) in transformed version and original kwargs. \"\"\" if self . batch_size == 0 : Xt , yt , kwargs = self . __transform ( X , y , ** kwargs ) else : Xt , yt , kwargs = self . __batch_transform ( X , y , ** kwargs ) if all ( hasattr ( data , \"shape\" ) for data in [ X , Xt ]) and all ( len ( data . shape ) > 1 for data in [ X , Xt ]): self . reduce_dimension = ( Xt . shape [ 1 ] < X . shape [ 1 ]) return Xt , yt , kwargs def inverse_transform ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ) -> ( np . ndarray , np . ndarray , dict ): \"\"\" Calls inverse_transform on the base element. When the dimension is preserved: transformers without inverse returns original input. Parameters: X: The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. y: The truth array-like values with shape=[N], where N is the number of samples. **kwargs: Keyword arguments, passed to base_element.transform. Raises: NotImplementedError: Thrown when there is a dimensional reduction but no inverse is defined. Returns: (X, y, kwargs) in back-transformed version. \"\"\" if hasattr ( self . base_element , 'inverse_transform' ): # todo: check this X , y , kwargs = self . adjusted_delegate_call ( self . base_element . inverse_transform , X , y , ** kwargs ) elif self . is_transformer and self . reduce_dimension : msg = \" {} has no inverse_transform, but element reduce dimesions.\" . format ( self . name ) logger . error ( msg ) raise NotImplementedError ( msg ) return X , y , kwargs def __batch_transform ( self , X , y = None , ** kwargs ): if not isinstance ( X , list ) and not isinstance ( X , np . ndarray ): warning = \"Cannot do batching on a single entity.\" logger . warning ( warning ) warnings . warn ( warning ) return self . __transform ( X , y , ** kwargs ) # initialize return values processed_X = None processed_y = None processed_kwargs = dict () nr = PhotonDataHelper . find_n ( X ) batch_idx = 0 for start , stop in PhotonDataHelper . chunker ( nr , self . batch_size ): batch_idx += 1 # split data in batches X_batched , y_batched , kwargs_dict_batched = PhotonDataHelper . split_data ( X , y , kwargs , start , stop ) actual_batch_size = PhotonDataHelper . find_n ( X_batched ) logger . debug ( self . name + \" is transforming batch \" + str ( batch_idx ) + \" with \" + str ( actual_batch_size ) + \" items.\" ) # call transform X_new , y_new , kwargs_new = self . adjusted_delegate_call ( self . base_element . transform , X_batched , y_batched , ** kwargs_dict_batched ) # stack results processed_X , processed_y , processed_kwargs = PhotonDataHelper . join_data ( processed_X , X_new , processed_y , y_new , processed_kwargs , kwargs_new ) return processed_X , processed_y , processed_kwargs def adjusted_delegate_call ( self , delegate , X , y , ** kwargs ): # Case| transforms X | needs_y | needs_covariates # ------------------------------------------------------- # 1 yes no no = transform(X) -> returns Xt # todo: case does not exist any longer # 2 yes yes no = transform(X, y) -> returns Xt, yt # 3 yes yes yes = transform(X, y, kwargs) -> returns Xt, yt, kwargst # 4 yes no yes = transform(X, kwargs) -> returns Xt, kwargst # 5 no yes or no yes or no = NOT ALLOWED # todo: we don't need to check for Switch, Stack or Branch since those classes define # needs_y and needs_covariates in their __init__() if self . needs_y : # if we dont have any target vector, we are in \"predict\"-mode although we are currently transforming # in this case, we want to skip the transformation and pass X, None and kwargs onwards # so basically, we skip all training_only elements # todo: I think, there's no way around this if we want to pass y and kwargs down to children of Switch and Branch if isinstance ( self , ( Switch , Branch , Preprocessing )): X , y , kwargs = delegate ( X , y , ** kwargs ) else : if y is not None : # todo: in case a method needs y, we should also always pass kwargs # i.e. if we change the number of samples, we also need to apply that change to all kwargs # todo: talk to Ramona! Maybe we actually DO need this case if self . needs_covariates : X , y , kwargs = delegate ( X , y , ** kwargs ) else : X , y = delegate ( X , y ) elif self . needs_covariates : X , kwargs = delegate ( X , ** kwargs ) else : X = delegate ( X ) return X , y , kwargs def adjusted_predict_call ( self , delegate , X , ** kwargs ): if self . needs_covariates : return delegate ( X , ** kwargs ) else : return delegate ( X ) def score ( self , X_test : np . ndarray , y_test : np . ndarray ) -> float : \"\"\" Calls the score function on the base element. Parameters: X_test: Input test data to score on. y_test: Input true targets to score on. Returns: A goodness of fit measure or a likelihood of unseen data. \"\"\" return self . base_element . score ( X_test , y_test ) def prettify_config_output ( self , config_name : str , config_value , return_dict : bool = False ): \"\"\"Make hyperparameter combinations human readable \"\"\" if config_name == \"disabled\" and config_value is False : if return_dict : return { 'disabled' : False } else : return \"disabled = False\" else : if return_dict : return { config_name : config_value } else : return config_name + '=' + str ( config_value ) @staticmethod def sanity_check_element_type_for_building_photon_pipes ( pipe_element , type_of_self ): if ( not isinstance ( pipe_element , PipelineElement ) and not isinstance ( pipe_element , PhotonNative )) or isinstance ( pipe_element , Preprocessing ): raise TypeError ( str ( type_of_self ) + \" only accepts PHOTON elements. Cannot add element of type \" + str ( type ( pipe_element )))","title":"Documentation for PipelineElement"},{"location":"api/base/pipeline_element/#photonai.base.photon_elements.PipelineElement.__iadd__","text":"Add an element to the intern list of elements. Parameters: Name Type Description Default pipe_element PipelineElement The object to add, being either a transformer or an estimator. required Source code in photonai/base/photon_elements.py def __iadd__ ( self , pipe_element ): \"\"\" Add an element to the intern list of elements. Parameters: pipe_element (PipelineElement): The object to add, being either a transformer or an estimator. \"\"\" PipelineElement . sanity_check_element_type_for_building_photon_pipes ( pipe_element , type ( self )) # check if that exact instance has been added before already_added_objects = len ([ i for i in self . elements if i is pipe_element ]) if already_added_objects > 0 : error_msg = \"Cannot add the same instance twice to \" + self . name + \" - \" + str ( type ( self )) logger . error ( error_msg ) raise ValueError ( error_msg ) # check for doubled names: already_existing_element_with_that_name = len ([ i for i in self . elements if i . name == pipe_element . name ]) if already_existing_element_with_that_name > 0 : error_msg = \"Already added a pipeline element with the name \" + pipe_element . name + \" to \" + self . name logger . warning ( error_msg ) warnings . warn ( error_msg ) # check for other items that have been renamed nr_of_existing_elements_with_that_name = len ([ i for i in self . elements if i . name . startswith ( pipe_element . name )]) new_name = pipe_element . name + str ( nr_of_existing_elements_with_that_name + 1 ) while len ([ i for i in self . elements if i . name == new_name ]) > 0 : nr_of_existing_elements_with_that_name += 1 new_name = pipe_element . name + str ( nr_of_existing_elements_with_that_name + 1 ) msg = \"Renaming \" + pipe_element . name + \" in \" + self . name + \" to \" + new_name + \" in \" + self . name logger . warning ( msg ) warnings . warn ( msg ) pipe_element . name = new_name self . elements . append ( pipe_element ) return self","title":"__iadd__()"},{"location":"api/base/pipeline_element/#photonai.base.photon_elements.PipelineElement.__init__","text":"Takes a string literal and transforms it into an object of the associated class (see PhotonCore.JSON). Parameters: Name Type Description Default name str A string literal encoding the class to be instantiated. required hyperparameters dict Which values/value range should be tested for the hyperparameter. In form of Dict: parameter_name -> HyperparameterElement. None test_disabled bool If the hyperparameter search should evaluate a complete disabling of the element. False disabled bool If true, the element is currently disabled and does nothing except return the data it received. False base_element BaseEstimator The underlying BaseEstimator. If not given the instantiation per string identifier takes place. None batch_size int Size of the division on which is calculated separately. 0 **kwargs Any parameters that should be passed to the object to be instantiated, default parameters. {} Source code in photonai/base/photon_elements.py def __init__ ( self , name : str , hyperparameters : dict = None , test_disabled : bool = False , disabled : bool = False , base_element : BaseEstimator = None , batch_size : int = 0 , ** kwargs ) -> None : \"\"\" Takes a string literal and transforms it into an object of the associated class (see PhotonCore.JSON). Parameters: name: A string literal encoding the class to be instantiated. hyperparameters: Which values/value range should be tested for the hyperparameter. In form of Dict: parameter_name -> HyperparameterElement. test_disabled: If the hyperparameter search should evaluate a complete disabling of the element. disabled: If true, the element is currently disabled and does nothing except return the data it received. base_element: The underlying BaseEstimator. If not given the instantiation per string identifier takes place. batch_size: Size of the division on which is calculated separately. **kwargs: Any parameters that should be passed to the object to be instantiated, default parameters. \"\"\" if hyperparameters is None : hyperparameters = {} if base_element is None : # Registering Pipeline Elements if len ( PhotonRegistry . ELEMENT_DICTIONARY ) == 0 : registry = PhotonRegistry if name not in PhotonRegistry . ELEMENT_DICTIONARY : # try to reload PhotonRegistry . ELEMENT_DICTIONARY = PhotonRegistry () . get_package_info () if name in PhotonRegistry . ELEMENT_DICTIONARY : try : desired_class_info = PhotonRegistry . ELEMENT_DICTIONARY [ name ] desired_class_home = desired_class_info [ 0 ] desired_class_name = desired_class_info [ 1 ] imported_module = importlib . import_module ( desired_class_home ) desired_class = getattr ( imported_module , desired_class_name ) self . base_element = desired_class ( ** kwargs ) except AttributeError as ae : logger . error ( 'ValueError: Could not find according class:' + str ( PhotonRegistry . ELEMENT_DICTIONARY [ name ])) raise ValueError ( 'Could not find according class:' , PhotonRegistry . ELEMENT_DICTIONARY [ name ]) else : # if even after reload the element does not appear, it is not supported logger . error ( 'Element not supported right now:' + name ) raise NameError ( 'Element not supported right now:' , name ) else : self . base_element = base_element self . is_transformer = hasattr ( self . base_element , \"transform\" ) self . reduce_dimension = False # boolean - set on transform method self . is_estimator = hasattr ( self . base_element , \"predict\" ) self . _name = name self . initial_name = str ( name ) self . kwargs = kwargs self . current_config = None self . batch_size = batch_size self . test_disabled = test_disabled self . initial_hyperparameters = dict ( hyperparameters ) self . _sklearn_disabled = self . name + '__disabled' self . _hyperparameters = hyperparameters if len ( hyperparameters ) > 0 : key_0 = next ( iter ( hyperparameters )) if self . name not in key_0 : self . hyperparameters = hyperparameters else : self . hyperparameters = hyperparameters # self.initalize_hyperparameters = hyperparameters # check if hyperparameters are already in sklearn style # check if hyperparameters are members of the class if self . is_transformer or self . is_estimator : self . _check_hyperparameters ( BaseEstimator ) self . disabled = disabled # check if self.base element needs y for fitting and transforming if hasattr ( self . base_element , 'needs_y' ): self . needs_y = self . base_element . needs_y else : self . needs_y = False # or if it maybe needs covariates for fitting and transforming if hasattr ( self . base_element , 'needs_covariates' ): self . needs_covariates = self . base_element . needs_covariates else : self . needs_covariates = False self . _random_state = False","title":"__init__()"},{"location":"api/base/pipeline_element/#photonai.base.photon_elements.PipelineElement.create","text":"Takes an instantiated object and encapsulates it into the PHOTONAI structure. Add the disabled function and attaches information about the hyperparameters that should be tested. Parameters: Name Type Description Default name str A string literal encoding the class to be instantiated. required base_element BaseEstimator The underlying transformer or estimator class. required hyperparameters dict Which values/value range should be tested for the hyperparameter. In form of Dict: parameter_name -> HyperparameterElement. required test_disabled bool If the hyperparameter search should evaluate a complete disabling of the element. False disabled bool If true, the element is currently disabled and does nothing except return the data it received. False **kwargs Any parameters that should be passed to the object to be instantiated, default parameters. {} Examples: 1 2 3 4 5 6 7 8 9 10 11 12 class RD ( BaseEstimator , TransformerMixin ): def fit ( self , X , y , ** kwargs ): pass def fit_transform ( self , X , y = None , ** fit_params ): return self . transform ( X ) def transform ( self , X ): return X [:, : 3 ] trans = PipelineElement . create ( 'MyTransformer' , base_element = RD (), hyperparameters = {}) Source code in photonai/base/photon_elements.py @classmethod def create ( cls , name : str , base_element : BaseEstimator , hyperparameters : dict , test_disabled : bool = False , disabled : bool = False , ** kwargs ): \"\"\" Takes an instantiated object and encapsulates it into the PHOTONAI structure. Add the disabled function and attaches information about the hyperparameters that should be tested. Parameters: name: A string literal encoding the class to be instantiated. base_element: The underlying transformer or estimator class. hyperparameters: Which values/value range should be tested for the hyperparameter. In form of Dict: parameter_name -> HyperparameterElement. test_disabled: If the hyperparameter search should evaluate a complete disabling of the element. disabled: If true, the element is currently disabled and does nothing except return the data it received. **kwargs: Any parameters that should be passed to the object to be instantiated, default parameters. Example: ``` python class RD(BaseEstimator, TransformerMixin): def fit(self, X, y, **kwargs): pass def fit_transform(self, X, y=None, **fit_params): return self.transform(X) def transform(self, X): return X[:, :3] trans = PipelineElement.create('MyTransformer', base_element=RD(), hyperparameters={}) ``` \"\"\" if isinstance ( base_element , type ): raise ValueError ( \"Base element should be an instance but is a class.\" ) return PipelineElement ( name , hyperparameters , test_disabled , disabled , base_element = base_element , ** kwargs )","title":"create()"},{"location":"api/base/pipeline_element/#photonai.base.photon_elements.PipelineElement.fit","text":"Calls the fit function of the base element. Parameters: Name Type Description Default X ndarray The array-like training and test data with shape=[N, D], where N is the number of samples and D is the number of features. required y ndarray The truth array-like values with shape=[N], where N is the number of samples. None **kwargs Keyword arguments, passed to base_element.predict. {} Returns: Type Description Fitted self. Source code in photonai/base/photon_elements.py def fit ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ): \"\"\" Calls the fit function of the base element. Parameters: X: The array-like training and test data with shape=[N, D], where N is the number of samples and D is the number of features. y: The truth array-like values with shape=[N], where N is the number of samples. **kwargs: Keyword arguments, passed to base_element.predict. Returns: Fitted self. \"\"\" if not self . disabled : obj = self . base_element arg_list = inspect . signature ( obj . fit ) if len ( arg_list . parameters ) > 2 : vals = arg_list . parameters . values () kwargs_param = list ( vals )[ - 1 ] if kwargs_param . kind == kwargs_param . VAR_KEYWORD : obj . fit ( X , y , ** kwargs ) return self obj . fit ( X , y ) return self","title":"fit()"},{"location":"api/base/pipeline_element/#photonai.base.photon_elements.PipelineElement.inverse_transform","text":"Calls inverse_transform on the base element. When the dimension is preserved: transformers without inverse returns original input. Parameters: Name Type Description Default X ndarray The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. required y ndarray The truth array-like values with shape=[N], where N is the number of samples. None **kwargs Keyword arguments, passed to base_element.transform. {} Returns: Type Description (<class 'numpy.ndarray'>, <class 'numpy.ndarray'>, <class 'dict'>) (X, y, kwargs) in back-transformed version. Source code in photonai/base/photon_elements.py def inverse_transform ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ) -> ( np . ndarray , np . ndarray , dict ): \"\"\" Calls inverse_transform on the base element. When the dimension is preserved: transformers without inverse returns original input. Parameters: X: The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. y: The truth array-like values with shape=[N], where N is the number of samples. **kwargs: Keyword arguments, passed to base_element.transform. Raises: NotImplementedError: Thrown when there is a dimensional reduction but no inverse is defined. Returns: (X, y, kwargs) in back-transformed version. \"\"\" if hasattr ( self . base_element , 'inverse_transform' ): # todo: check this X , y , kwargs = self . adjusted_delegate_call ( self . base_element . inverse_transform , X , y , ** kwargs ) elif self . is_transformer and self . reduce_dimension : msg = \" {} has no inverse_transform, but element reduce dimesions.\" . format ( self . name ) logger . error ( msg ) raise NotImplementedError ( msg ) return X , y , kwargs","title":"inverse_transform()"},{"location":"api/base/pipeline_element/#photonai.base.photon_elements.PipelineElement.predict","text":"Calls the predict function of the underlying base_element. Parameters: Name Type Description Default X ndarray The array-like training and test data with shape=[N, D], where N is the number of samples and D is the number of features. required **kwargs Keyword arguments, passed to base_element.predict. {} Returns: Type Description ndarray Predictions values. Source code in photonai/base/photon_elements.py def predict ( self , X : np . ndarray , ** kwargs ) -> np . ndarray : \"\"\" Calls the predict function of the underlying base_element. Parameters: X: The array-like training and test data with shape=[N, D], where N is the number of samples and D is the number of features. **kwargs: Keyword arguments, passed to base_element.predict. Returns: Predictions values. \"\"\" if self . batch_size == 0 : return self . __predict ( X , ** kwargs ) else : return self . __batch_predict ( self . __predict , X , ** kwargs )","title":"predict()"},{"location":"api/base/pipeline_element/#photonai.base.photon_elements.PipelineElement.score","text":"Calls the score function on the base element. Parameters: Name Type Description Default X_test ndarray Input test data to score on. required y_test ndarray Input true targets to score on. required Returns: Type Description float A goodness of fit measure or a likelihood of unseen data. Source code in photonai/base/photon_elements.py def score ( self , X_test : np . ndarray , y_test : np . ndarray ) -> float : \"\"\" Calls the score function on the base element. Parameters: X_test: Input test data to score on. y_test: Input true targets to score on. Returns: A goodness of fit measure or a likelihood of unseen data. \"\"\" return self . base_element . score ( X_test , y_test )","title":"score()"},{"location":"api/base/pipeline_element/#photonai.base.photon_elements.PipelineElement.transform","text":"Calls transform on the base element. In case there is no transform method, calls predict. This is used if we are using an estimator as a preprocessing step. Parameters: Name Type Description Default X ndarray The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. required y ndarray The truth array-like values with shape=[N], where N is the number of samples. None **kwargs Keyword arguments, passed to base_element.transform. {} Returns: Type Description (<class 'numpy.ndarray'>, <class 'numpy.ndarray'>, <class 'dict'>) (X, y) in transformed version and original kwargs. Source code in photonai/base/photon_elements.py def transform ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ) -> ( np . ndarray , np . ndarray , dict ): \"\"\" Calls transform on the base element. In case there is no transform method, calls predict. This is used if we are using an estimator as a preprocessing step. Parameters: X: The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. y: The truth array-like values with shape=[N], where N is the number of samples. **kwargs: Keyword arguments, passed to base_element.transform. Returns: (X, y) in transformed version and original kwargs. \"\"\" if self . batch_size == 0 : Xt , yt , kwargs = self . __transform ( X , y , ** kwargs ) else : Xt , yt , kwargs = self . __batch_transform ( X , y , ** kwargs ) if all ( hasattr ( data , \"shape\" ) for data in [ X , Xt ]) and all ( len ( data . shape ) > 1 for data in [ X , Xt ]): self . reduce_dimension = ( Xt . shape [ 1 ] < X . shape [ 1 ]) return Xt , yt , kwargs","title":"transform()"},{"location":"api/base/preprocessing/","text":"Documentation for Preprocessing Special kind of Branch. If a Preprocessing pipe is added to a PHOTONAI Hyperpipe, all transformers are applied to the data ONCE BEFORE cross validation starts in order to prepare the data. Every added element should be a transformer PipelineElement. Examples: 1 2 3 pre_proc = Preprocessing () pre_proc += PipelineElement ( 'OneHotEncoder' , sparse = False ) my_pipe += pre_proc Some transformations should be performed bundled at the beginning. Here at the example of the OneHotEncoder. Due to the cross-validation split, some cateogries can no longer occur in any subsets. Therefore, a trained OneHotEncoding could fail on other subsets. By using the Preprocessing object, this effect can no longer appear. Source code in photonai/base/photon_elements.py class Preprocessing ( Branch ): \"\"\" Special kind of Branch. If a Preprocessing pipe is added to a PHOTONAI Hyperpipe, all transformers are applied to the data ONCE BEFORE cross validation starts in order to prepare the data. Every added element should be a transformer PipelineElement. Example: ``` python pre_proc = Preprocessing() pre_proc += PipelineElement('OneHotEncoder', sparse=False) my_pipe += pre_proc ``` Some transformations should be performed bundled at the beginning. Here at the example of the OneHotEncoder. Due to the cross-validation split, some cateogries can no longer occur in any subsets. Therefore, a trained OneHotEncoding could fail on other subsets. By using the Preprocessing object, this effect can no longer appear. \"\"\" def __init__ ( self ): \"\"\"Initialize the object.\"\"\" super () . __init__ ( 'Preprocessing' ) self . has_hyperparameters = False self . needs_y = True self . needs_covariates = True self . _name = 'Preprocessing' self . is_transformer = True self . is_estimator = False def __iadd__ ( self , pipe_element : PipelineElement ): \"\"\" Add an element to the sub pipeline. Parameters: pipe_element: The transformer object to add. \"\"\" if hasattr ( pipe_element , \"transform\" ): super ( Preprocessing , self ) . __iadd__ ( pipe_element ) if len ( pipe_element . hyperparameters ) > 0 : raise ValueError ( \"A preprocessing transformer must not have any hyperparameter \" \"because it is not part of the optimization and cross validation procedure\" ) else : raise ValueError ( \"Pipeline Element must have transform function\" ) return self def predict ( self , data , ** kwargs ): warnings . warn ( \"There is no predict function of the preprocessing pipe, it is a transformer only.\" ) pass @property def _estimator_type ( self ): return __iadd__ ( self , pipe_element ) special Add an element to the sub pipeline. Parameters: Name Type Description Default pipe_element PipelineElement The transformer object to add. required Source code in photonai/base/photon_elements.py def __iadd__ ( self , pipe_element : PipelineElement ): \"\"\" Add an element to the sub pipeline. Parameters: pipe_element: The transformer object to add. \"\"\" if hasattr ( pipe_element , \"transform\" ): super ( Preprocessing , self ) . __iadd__ ( pipe_element ) if len ( pipe_element . hyperparameters ) > 0 : raise ValueError ( \"A preprocessing transformer must not have any hyperparameter \" \"because it is not part of the optimization and cross validation procedure\" ) else : raise ValueError ( \"Pipeline Element must have transform function\" ) return self __init__ ( self ) special Initialize the object. Source code in photonai/base/photon_elements.py def __init__ ( self ): \"\"\"Initialize the object.\"\"\" super () . __init__ ( 'Preprocessing' ) self . has_hyperparameters = False self . needs_y = True self . needs_covariates = True self . _name = 'Preprocessing' self . is_transformer = True self . is_estimator = False","title":"Preprocessing"},{"location":"api/base/preprocessing/#documentation-for-preprocessing","text":"Special kind of Branch. If a Preprocessing pipe is added to a PHOTONAI Hyperpipe, all transformers are applied to the data ONCE BEFORE cross validation starts in order to prepare the data. Every added element should be a transformer PipelineElement. Examples: 1 2 3 pre_proc = Preprocessing () pre_proc += PipelineElement ( 'OneHotEncoder' , sparse = False ) my_pipe += pre_proc Some transformations should be performed bundled at the beginning. Here at the example of the OneHotEncoder. Due to the cross-validation split, some cateogries can no longer occur in any subsets. Therefore, a trained OneHotEncoding could fail on other subsets. By using the Preprocessing object, this effect can no longer appear. Source code in photonai/base/photon_elements.py class Preprocessing ( Branch ): \"\"\" Special kind of Branch. If a Preprocessing pipe is added to a PHOTONAI Hyperpipe, all transformers are applied to the data ONCE BEFORE cross validation starts in order to prepare the data. Every added element should be a transformer PipelineElement. Example: ``` python pre_proc = Preprocessing() pre_proc += PipelineElement('OneHotEncoder', sparse=False) my_pipe += pre_proc ``` Some transformations should be performed bundled at the beginning. Here at the example of the OneHotEncoder. Due to the cross-validation split, some cateogries can no longer occur in any subsets. Therefore, a trained OneHotEncoding could fail on other subsets. By using the Preprocessing object, this effect can no longer appear. \"\"\" def __init__ ( self ): \"\"\"Initialize the object.\"\"\" super () . __init__ ( 'Preprocessing' ) self . has_hyperparameters = False self . needs_y = True self . needs_covariates = True self . _name = 'Preprocessing' self . is_transformer = True self . is_estimator = False def __iadd__ ( self , pipe_element : PipelineElement ): \"\"\" Add an element to the sub pipeline. Parameters: pipe_element: The transformer object to add. \"\"\" if hasattr ( pipe_element , \"transform\" ): super ( Preprocessing , self ) . __iadd__ ( pipe_element ) if len ( pipe_element . hyperparameters ) > 0 : raise ValueError ( \"A preprocessing transformer must not have any hyperparameter \" \"because it is not part of the optimization and cross validation procedure\" ) else : raise ValueError ( \"Pipeline Element must have transform function\" ) return self def predict ( self , data , ** kwargs ): warnings . warn ( \"There is no predict function of the preprocessing pipe, it is a transformer only.\" ) pass @property def _estimator_type ( self ): return","title":"Documentation for Preprocessing"},{"location":"api/base/preprocessing/#photonai.base.photon_elements.Preprocessing.__iadd__","text":"Add an element to the sub pipeline. Parameters: Name Type Description Default pipe_element PipelineElement The transformer object to add. required Source code in photonai/base/photon_elements.py def __iadd__ ( self , pipe_element : PipelineElement ): \"\"\" Add an element to the sub pipeline. Parameters: pipe_element: The transformer object to add. \"\"\" if hasattr ( pipe_element , \"transform\" ): super ( Preprocessing , self ) . __iadd__ ( pipe_element ) if len ( pipe_element . hyperparameters ) > 0 : raise ValueError ( \"A preprocessing transformer must not have any hyperparameter \" \"because it is not part of the optimization and cross validation procedure\" ) else : raise ValueError ( \"Pipeline Element must have transform function\" ) return self","title":"__iadd__()"},{"location":"api/base/preprocessing/#photonai.base.photon_elements.Preprocessing.__init__","text":"Initialize the object. Source code in photonai/base/photon_elements.py def __init__ ( self ): \"\"\"Initialize the object.\"\"\" super () . __init__ ( 'Preprocessing' ) self . has_hyperparameters = False self . needs_y = True self . needs_covariates = True self . _name = 'Preprocessing' self . is_transformer = True self . is_estimator = False","title":"__init__()"},{"location":"api/base/registry/","text":"Documentation for PhotonRegistry Helper class to manage the PHOTONAI Element Register. Use it to add and remove items into the register. You can also retrieve information about items and its hyperparameters. Every item in the register is encoded by a string literal that points to a python class and its namespace. You can access the python class via the string literal. The class PhotonElement imports and instantiates the class for you. Examples: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import os from photonai.base import PhotonRegistry # REGISTER ELEMENT saved in folder custom_elements_folder base_folder = os . path . dirname ( os . path . abspath ( __file__ )) custom_elements_folder = os . path . join ( base_folder , 'custom_elements' ) registry = PhotonRegistry ( custom_elements_folder = custom_elements_folder ) registry . register ( photon_name = 'MyCustomEstimator' , class_str = 'custom_estimator.CustomEstimator' , element_type = 'Estimator' ) registry . activate () registry . info ( 'MyCustomEstimator' ) # get informations of other available elements registry . info ( 'SVC' ) Source code in photonai/base/registry/registry.py class PhotonRegistry : \"\"\" Helper class to manage the PHOTONAI Element Register. Use it to add and remove items into the register. You can also retrieve information about items and its hyperparameters. Every item in the register is encoded by a string literal that points to a python class and its namespace. You can access the python class via the string literal. The class PhotonElement imports and instantiates the class for you. Example: ``` python import os from photonai.base import PhotonRegistry # REGISTER ELEMENT saved in folder custom_elements_folder base_folder = os.path.dirname(os.path.abspath(__file__)) custom_elements_folder = os.path.join(base_folder, 'custom_elements') registry = PhotonRegistry(custom_elements_folder=custom_elements_folder) registry.register(photon_name='MyCustomEstimator', class_str='custom_estimator.CustomEstimator', element_type='Estimator') registry.activate() registry.info('MyCustomEstimator') # get informations of other available elements registry.info('SVC') ``` \"\"\" ELEMENT_DICTIONARY = dict () PHOTON_REGISTRIES = [ 'PhotonCore' ] CUSTOM_ELEMENTS_FOLDER = None CUSTOM_ELEMENTS = None def __init__ ( self , custom_elements_folder : str = None ): \"\"\" Initialize the object. Parameters: custom_elements_folder: Path to folder with custom element in it. \"\"\" self . current_folder = os . path . dirname ( os . path . abspath ( inspect . getfile ( inspect . currentframe ()))) self . module_path = os . path . join ( self . current_folder , \"modules\" ) if not os . path . isdir ( self . module_path ): os . mkdir ( self . module_path ) # update list with available sub_elements self . _list_available_modules () PhotonRegistry . CUSTOM_ELEMENTS_FOLDER = custom_elements_folder self . _load_custom_folder ( custom_elements_folder ) if len ( PhotonRegistry . ELEMENT_DICTIONARY ) == 0 or \\ PhotonRegistry . ELEMENT_DICTIONARY == PhotonRegistry . CUSTOM_ELEMENTS : PhotonRegistry . ELEMENT_DICTIONARY . update ( self . get_package_info ()) def _list_available_modules ( self ): for abs_filename in glob ( os . path . join ( self . module_path , \"*.json\" )): basename = os . path . basename ( abs_filename ) file , ext = os . path . splitext ( basename ) if file not in PhotonRegistry . PHOTON_REGISTRIES : PhotonRegistry . PHOTON_REGISTRIES . append ( file ) def add_module ( self , path_to_file : str ): filename = os . path . basename ( path_to_file ) copyfile ( path_to_file , os . path . join ( self . module_path , filename )) self . _list_available_modules () PhotonRegistry . ELEMENT_DICTIONARY = self . get_package_info () def delete_module ( self , module_name : str ): PhotonRegistry . PHOTON_REGISTRIES . remove ( module_name ) os . remove ( os . path . join ( self . module_path , module_name + \".json\" )) PhotonRegistry . ELEMENT_DICTIONARY = self . get_package_info () def _load_json ( self , photon_package : str ) -> dict : \"\"\" Load JSON file in which the elements for the PHOTON submodule are stored. The JSON files are stored in the framework folder by the name convention 'photon_package.json' Parameters: photon_package: The name of the photonai submodule Returns: JSON file as dict, file path as str. \"\"\" if photon_package == 'CustomElements' : folder = PhotonRegistry . CUSTOM_ELEMENTS_FOLDER if not folder : return {} elif photon_package == \"PhotonCore\" : folder = self . current_folder else : folder = self . module_path file_name = os . path . join ( folder , photon_package + '.json' ) file_content = {} # Reading json try : with open ( file_name , 'r' ) as f : try : file_content = json . load ( f ) except json . JSONDecodeError as jde : # handle empty file if jde . msg == 'Expecting value' : logger . error ( \"Package File \" + file_name + \" was empty.\" ) else : raise jde except FileNotFoundError : logger . error ( \"Could not find file for \" + photon_package ) if not file_content : file_content = dict () return file_content def get_package_info ( self , photon_package : list = PHOTON_REGISTRIES ) -> dict : \"\"\" Collect all registered elements from JSON file. Parameters: photon_package: The names of the PHOTONAI submodules for which the elements should be retrieved. Returns: Dict of registered elements. \"\"\" class_info = dict () for package in photon_package : content = self . _load_json ( package ) for idx , key in enumerate ( content ): class_path , class_name = os . path . splitext ( content [ key ][ 0 ]) if idx == 0 and package not in [ \"PhotonCore\" , \"CustomElements\" ]: # try to import something from module. # if that fails. drop this shit. try : imported_module = importlib . import_module ( class_path ) desired_class = getattr ( imported_module , class_name [ 1 :]) custom_element = desired_class () except ( AttributeError , ModuleNotFoundError ) as e : logger . error ( e ) logger . error ( \"Could not import from package {} . Deleting json.\" . format ( package )) self . delete_module ( package ) class_info [ key ] = class_path , class_name [ 1 :] return class_info def _load_custom_folder ( self , custom_elements_folder ): if PhotonRegistry . CUSTOM_ELEMENTS_FOLDER is not None : PhotonRegistry . CUSTOM_ELEMENTS_FOLDER = self . _check_custom_folder ( custom_elements_folder ) # load custom elements from json PhotonRegistry . CUSTOM_ELEMENTS = self . _load_json ( 'CustomElements' ) PhotonRegistry . PHOTON_REGISTRIES . append ( 'CustomElements' ) PhotonRegistry . ELEMENT_DICTIONARY . update ( PhotonRegistry . CUSTOM_ELEMENTS ) @staticmethod def _check_custom_folder ( custom_folder ): if not os . path . exists ( custom_folder ): logger . info ( 'Creating folder {} ' . format ( custom_folder )) os . makedirs ( custom_folder ) custom_file = os . path . join ( custom_folder , 'CustomElements.json' ) if not os . path . isfile ( custom_file ): logger . info ( 'Creating CustomElements.json' ) with open ( custom_file , 'w' ) as f : json . dump ( '' , f ) return custom_folder def activate ( self ): if not PhotonRegistry . CUSTOM_ELEMENTS_FOLDER : raise ValueError ( \"To activate a custom elements folder, specify a folder when instantiating the registry \" \"module. Example: registry = PhotonRegistry('/MY/CUSTOM/ELEMENTS/FOLDER) \" \"In case you don't have any custom models, there is no need to activate the registry.\" ) if not os . path . exists ( PhotonRegistry . CUSTOM_ELEMENTS_FOLDER ): raise FileNotFoundError ( \"Couldn't find custom elements folder: {} \" . format ( PhotonRegistry . CUSTOM_ELEMENTS_FOLDER )) if not os . path . isfile ( os . path . join ( PhotonRegistry . CUSTOM_ELEMENTS_FOLDER , 'CustomElements.json' )): raise FileNotFoundError ( \"Couldn't find CustomElements.json. Did you register your element first?\" ) # add folder to python path logger . info ( \"Adding custom elements folder to system path...\" ) sys . path . append ( PhotonRegistry . CUSTOM_ELEMENTS_FOLDER ) PhotonRegistry . ELEMENT_DICTIONARY . update ( self . get_package_info ([ 'CustomElements' ])) logger . info ( 'Successfully activated custom elements!' ) def register ( self , photon_name : str , class_str : str , element_type : str ): \"\"\" Save element information to the JSON file. Parameters: photon_name: The string literal with which you want to access the class. class_str: The namespace of the class, like in the import statement. element_type: Can be 'Estimator' or 'Transformer' \"\"\" # check if folder exists if not PhotonRegistry . CUSTOM_ELEMENTS_FOLDER : raise ValueError ( \"To register an element, specify a custom elements folder when instantiating the registry \" \"module. Example: registry = PhotonRegistry('/MY/CUSTOM/ELEMENTS/FOLDER)\" ) if not element_type == \"Estimator\" and not element_type == \"Transformer\" : raise ValueError ( \"Variable element_type must be 'Estimator' or 'Transformer'\" ) duplicate = self . _check_duplicate ( photon_name = photon_name , class_str = class_str , content = PhotonRegistry . CUSTOM_ELEMENTS ) if not duplicate : python_file = os . path . join ( PhotonRegistry . CUSTOM_ELEMENTS_FOLDER , class_str . split ( '.' )[ 0 ] + '.py' ) if not os . path . isfile ( python_file ): raise FileNotFoundError ( \"Couldn't find python file {} in your custom elements folder. \" \"Please copy your file into this folder first!\" . format ( python_file )) # add new element PhotonRegistry . CUSTOM_ELEMENTS [ photon_name ] = class_str , element_type # write back to file self . _write_to_json ( PhotonRegistry . CUSTOM_ELEMENTS ) logger . info ( 'Adding PipelineElement ' + class_str + ' to CustomElements.json as \"' + photon_name + '\".' ) # activate custom elements self . activate () # check custom element logger . info ( \"Running tests on custom element...\" ) return self . _run_tests ( photon_name , element_type ) else : logger . error ( 'Could not register element!' ) def _run_tests ( self , photon_name , element_type ): # this is a sneaky hack to avoid circular imports of PipelineElement imported_module = importlib . import_module ( \"photonai.base\" ) desired_class = getattr ( imported_module , \"PipelineElement\" ) custom_element = desired_class ( photon_name ) # check if has fit, transform, predict if not hasattr ( custom_element . base_element , 'fit' ): raise NotImplementedError ( \"Custom element does not implement fit() method.\" ) if element_type == 'Transformer' and not hasattr ( custom_element . base_element , 'transform' ): raise NotImplementedError ( \"Custom element does not implement transform() method.\" ) if element_type == 'Estimator' and not hasattr ( custom_element . base_element , 'predict' ): raise NotImplementedError ( \"Custom element does not implement predict() method.\" ) # check if estimator is regressor or classifier if element_type == 'Estimator' : if hasattr ( custom_element , '_estimator_type' ): est_type = getattr ( custom_element , '_estimator_type' ) if est_type == \"regressor\" : X , y = load_boston ( return_X_y = True ) elif est_type == \"classifier\" : X , y = load_breast_cancer ( return_X_y = True ) else : raise ValueError ( \"Custom element does not specify whether it is a regressor or classifier. \" \"Is {} \" . format ( est_type )) else : raise NotImplementedError ( \"Custom element does not specify whether it is a regressor or classifier. \" \"Consider inheritance from ClassifierMixin or RegressorMixin or set \" \"_estimator_type manually.\" ) else : X , y = load_boston ( return_X_y = True ) # try and test functionality kwargs = { 'covariates' : np . random . randn ( len ( y ))} try : # test fit returned_element = custom_element . base_element . fit ( X , y , ** kwargs ) except Exception as e : logger . info ( \"Not able to run tests on fit() method. Test data not compatible.\" ) return e if not isinstance ( returned_element , custom_element . base_element . __class__ ): raise NotImplementedError ( \"fit() method does not return self.\" ) try : # test transform or predict (if base element does not implement transform method, predict should be called # by PipelineElement -> so we only need to test transform() if custom_element . needs_y : if element_type == 'Estimator' : raise NotImplementedError ( \"Estimator should not need y.\" ) Xt , yt , kwargst = custom_element . base_element . transform ( X , y , ** kwargs ) if 'covariates' not in kwargst . keys (): raise ValueError ( \"Custom element does not correctly transform kwargs although needs_y is True. \" \"If you change the number of samples in transform(), make sure to transform kwargs \" \"respectively.\" ) if not len ( kwargst [ 'covariates' ]) == len ( X ): raise ValueError ( \"Custom element is not returning the correct number of samples!\" ) elif custom_element . needs_covariates : if element_type == 'Estimator' : yt , kwargst = custom_element . base_element . predict ( X , ** kwargs ) if not len ( yt ) == len ( y ): raise ValueError ( \"Custom element is not returning the correct number of samples!\" ) else : Xt , kwargst = custom_element . base_element . transform ( X , ** kwargs ) if not len ( Xt ) == len ( X ) or not len ( kwargst [ 'covariates' ]) == len ( X ): raise ValueError ( \"Custom element is not returning the correct number of samples!\" ) else : if element_type == 'Estimator' : yt = custom_element . base_element . predict ( X ) if not len ( yt ) == len ( y ): raise ValueError ( \"Custom element is not returning the correct number of samples!\" ) else : Xt = custom_element . base_element . transform ( X ) if not len ( Xt ) == len ( X ): raise ValueError ( \"Custom element is not returning the correct number of samples!\" ) except ValueError as ve : if \"too many values to unpack\" in ve . args [ 0 ]: raise ValueError ( \"Custom element does not return X, y and kwargs the way it should \" \"according to needs_y and needs_covariates.\" ) else : logger . info ( ve . args [ 0 ]) return ve except Exception as e : logger . info ( e . args [ 0 ]) logger . info ( \"Not able to run tests on transform() or predict() method. Test data probably not compatible.\" ) return e logger . info ( 'All tests on custom element passed.' ) def info ( self , photon_name : str ): \"\"\" Show information for object that is encoded by this name. Parameters: photon_name: The string literal which accesses the class. \"\"\" content = self . get_package_info () # load existing json if photon_name in content : element_namespace , element_name = content [ photon_name ] print ( \"----------------------------------\" ) print ( \"Name: \" + element_name ) print ( \"Namespace: \" + element_namespace ) print ( \"----------------------------------\" ) try : imported_module = __import__ ( element_namespace , globals (), locals (), element_name , 0 ) desired_class = getattr ( imported_module , element_name ) base_element = desired_class () print ( \"Possible Hyperparameters as derived from constructor:\" ) class_args = inspect . signature ( base_element . __init__ ) for item , more_info in class_args . parameters . items (): print ( \" {:<35} {:<75} \" . format ( item , str ( more_info ))) print ( \"----------------------------------\" ) except Exception as e : logger . error ( e ) logger . error ( \"Could not instantiate class \" + element_namespace + \".\" + element_name ) else : logger . error ( \"Could not find element \" + photon_name ) def delete ( self , photon_name : str ): \"\"\" Delete Element from JSON file. Parameters: photon_name: The string literal encoding the class. \"\"\" if photon_name in PhotonRegistry . CUSTOM_ELEMENTS : del PhotonRegistry . CUSTOM_ELEMENTS [ photon_name ] self . _write_to_json ( PhotonRegistry . CUSTOM_ELEMENTS ) logger . info ( 'Removing the PipelineElement named \" {0} \" from CustomElements.json.' . format ( photon_name )) else : logger . info ( 'Cannot remove \" {0} \" from CustomElements.json. Element has not been registered before.' . format ( photon_name )) @staticmethod def _check_duplicate ( photon_name , class_str : str , content : str ) -> bool : \"\"\" Helper function to check if the entry is either registered by a different name or if the name is already given to another class. Parameters: content: The content of the CustomElements.json. class_str: The namespace.Classname, where the class lives, from where it should be imported. photon_name: The name of the element with which it is called within PHOTONAI. Returns: False if there is no key with this name and the class is not already registered with another key. \"\"\" # check for duplicate name (dict key) if photon_name in content : logger . info ( 'A PipelineElement named ' + photon_name + ' has already been registered.' ) return True # check for duplicate class_str if any ( class_str in '.' . join ([ s [ 0 ], s [ 1 ]]) for s in content . values ()): logger . info ( 'The Class named ' + class_str + ' has already been registered.' ) return True return False def _write_to_json ( self , content_to_write : dict ): \"\"\" Write json content to file Parameters: content2write: The new information to attach to the file. \"\"\" # Writing JSON data with open ( os . path . join ( self . CUSTOM_ELEMENTS_FOLDER , \"CustomElements.json\" ), 'w' ) as f : json . dump ( content_to_write , f ) def list_available_elements ( self , photon_package : list = PHOTON_REGISTRIES ): \"\"\" Print info about all items that are registered for the PHOTONAI submodule to the console. Parameters: photon_package: The names of the PHOTON submodules for which the elements should be retrieved. \"\"\" if isinstance ( photon_package , str ): photon_package = [ photon_package ] for package in photon_package : content = self . _load_json ( package ) if len ( content ) > 0 : print ( ' \\n ' + package ) for k , v in sorted ( content . items ()): class_info , package_type = v print ( \" {:<35} {:<75} {:<5} \" . format ( k , class_info , package_type )) __init__ ( self , custom_elements_folder = None ) special Initialize the object. Parameters: Name Type Description Default custom_elements_folder str Path to folder with custom element in it. None Source code in photonai/base/registry/registry.py def __init__ ( self , custom_elements_folder : str = None ): \"\"\" Initialize the object. Parameters: custom_elements_folder: Path to folder with custom element in it. \"\"\" self . current_folder = os . path . dirname ( os . path . abspath ( inspect . getfile ( inspect . currentframe ()))) self . module_path = os . path . join ( self . current_folder , \"modules\" ) if not os . path . isdir ( self . module_path ): os . mkdir ( self . module_path ) # update list with available sub_elements self . _list_available_modules () PhotonRegistry . CUSTOM_ELEMENTS_FOLDER = custom_elements_folder self . _load_custom_folder ( custom_elements_folder ) if len ( PhotonRegistry . ELEMENT_DICTIONARY ) == 0 or \\ PhotonRegistry . ELEMENT_DICTIONARY == PhotonRegistry . CUSTOM_ELEMENTS : PhotonRegistry . ELEMENT_DICTIONARY . update ( self . get_package_info ()) delete ( self , photon_name ) Delete Element from JSON file. Parameters: Name Type Description Default photon_name str The string literal encoding the class. required Source code in photonai/base/registry/registry.py def delete ( self , photon_name : str ): \"\"\" Delete Element from JSON file. Parameters: photon_name: The string literal encoding the class. \"\"\" if photon_name in PhotonRegistry . CUSTOM_ELEMENTS : del PhotonRegistry . CUSTOM_ELEMENTS [ photon_name ] self . _write_to_json ( PhotonRegistry . CUSTOM_ELEMENTS ) logger . info ( 'Removing the PipelineElement named \" {0} \" from CustomElements.json.' . format ( photon_name )) else : logger . info ( 'Cannot remove \" {0} \" from CustomElements.json. Element has not been registered before.' . format ( photon_name )) get_package_info ( self , photon_package = [ 'PhotonCore' ]) Collect all registered elements from JSON file. Parameters: Name Type Description Default photon_package list The names of the PHOTONAI submodules for which the elements should be retrieved. ['PhotonCore'] Returns: Type Description dict Dict of registered elements. Source code in photonai/base/registry/registry.py def get_package_info ( self , photon_package : list = PHOTON_REGISTRIES ) -> dict : \"\"\" Collect all registered elements from JSON file. Parameters: photon_package: The names of the PHOTONAI submodules for which the elements should be retrieved. Returns: Dict of registered elements. \"\"\" class_info = dict () for package in photon_package : content = self . _load_json ( package ) for idx , key in enumerate ( content ): class_path , class_name = os . path . splitext ( content [ key ][ 0 ]) if idx == 0 and package not in [ \"PhotonCore\" , \"CustomElements\" ]: # try to import something from module. # if that fails. drop this shit. try : imported_module = importlib . import_module ( class_path ) desired_class = getattr ( imported_module , class_name [ 1 :]) custom_element = desired_class () except ( AttributeError , ModuleNotFoundError ) as e : logger . error ( e ) logger . error ( \"Could not import from package {} . Deleting json.\" . format ( package )) self . delete_module ( package ) class_info [ key ] = class_path , class_name [ 1 :] return class_info info ( self , photon_name ) Show information for object that is encoded by this name. Parameters: Name Type Description Default photon_name str The string literal which accesses the class. required Source code in photonai/base/registry/registry.py def info ( self , photon_name : str ): \"\"\" Show information for object that is encoded by this name. Parameters: photon_name: The string literal which accesses the class. \"\"\" content = self . get_package_info () # load existing json if photon_name in content : element_namespace , element_name = content [ photon_name ] print ( \"----------------------------------\" ) print ( \"Name: \" + element_name ) print ( \"Namespace: \" + element_namespace ) print ( \"----------------------------------\" ) try : imported_module = __import__ ( element_namespace , globals (), locals (), element_name , 0 ) desired_class = getattr ( imported_module , element_name ) base_element = desired_class () print ( \"Possible Hyperparameters as derived from constructor:\" ) class_args = inspect . signature ( base_element . __init__ ) for item , more_info in class_args . parameters . items (): print ( \" {:<35} {:<75} \" . format ( item , str ( more_info ))) print ( \"----------------------------------\" ) except Exception as e : logger . error ( e ) logger . error ( \"Could not instantiate class \" + element_namespace + \".\" + element_name ) else : logger . error ( \"Could not find element \" + photon_name ) list_available_elements ( self , photon_package = [ 'PhotonCore' ]) Print info about all items that are registered for the PHOTONAI submodule to the console. Parameters: Name Type Description Default photon_package list The names of the PHOTON submodules for which the elements should be retrieved. ['PhotonCore'] Source code in photonai/base/registry/registry.py def list_available_elements ( self , photon_package : list = PHOTON_REGISTRIES ): \"\"\" Print info about all items that are registered for the PHOTONAI submodule to the console. Parameters: photon_package: The names of the PHOTON submodules for which the elements should be retrieved. \"\"\" if isinstance ( photon_package , str ): photon_package = [ photon_package ] for package in photon_package : content = self . _load_json ( package ) if len ( content ) > 0 : print ( ' \\n ' + package ) for k , v in sorted ( content . items ()): class_info , package_type = v print ( \" {:<35} {:<75} {:<5} \" . format ( k , class_info , package_type )) register ( self , photon_name , class_str , element_type ) Save element information to the JSON file. Parameters: Name Type Description Default photon_name str The string literal with which you want to access the class. required class_str str The namespace of the class, like in the import statement. required element_type str Can be 'Estimator' or 'Transformer' required Source code in photonai/base/registry/registry.py def register ( self , photon_name : str , class_str : str , element_type : str ): \"\"\" Save element information to the JSON file. Parameters: photon_name: The string literal with which you want to access the class. class_str: The namespace of the class, like in the import statement. element_type: Can be 'Estimator' or 'Transformer' \"\"\" # check if folder exists if not PhotonRegistry . CUSTOM_ELEMENTS_FOLDER : raise ValueError ( \"To register an element, specify a custom elements folder when instantiating the registry \" \"module. Example: registry = PhotonRegistry('/MY/CUSTOM/ELEMENTS/FOLDER)\" ) if not element_type == \"Estimator\" and not element_type == \"Transformer\" : raise ValueError ( \"Variable element_type must be 'Estimator' or 'Transformer'\" ) duplicate = self . _check_duplicate ( photon_name = photon_name , class_str = class_str , content = PhotonRegistry . CUSTOM_ELEMENTS ) if not duplicate : python_file = os . path . join ( PhotonRegistry . CUSTOM_ELEMENTS_FOLDER , class_str . split ( '.' )[ 0 ] + '.py' ) if not os . path . isfile ( python_file ): raise FileNotFoundError ( \"Couldn't find python file {} in your custom elements folder. \" \"Please copy your file into this folder first!\" . format ( python_file )) # add new element PhotonRegistry . CUSTOM_ELEMENTS [ photon_name ] = class_str , element_type # write back to file self . _write_to_json ( PhotonRegistry . CUSTOM_ELEMENTS ) logger . info ( 'Adding PipelineElement ' + class_str + ' to CustomElements.json as \"' + photon_name + '\".' ) # activate custom elements self . activate () # check custom element logger . info ( \"Running tests on custom element...\" ) return self . _run_tests ( photon_name , element_type ) else : logger . error ( 'Could not register element!' )","title":"Registry"},{"location":"api/base/registry/#documentation-for-photonregistry","text":"Helper class to manage the PHOTONAI Element Register. Use it to add and remove items into the register. You can also retrieve information about items and its hyperparameters. Every item in the register is encoded by a string literal that points to a python class and its namespace. You can access the python class via the string literal. The class PhotonElement imports and instantiates the class for you. Examples: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import os from photonai.base import PhotonRegistry # REGISTER ELEMENT saved in folder custom_elements_folder base_folder = os . path . dirname ( os . path . abspath ( __file__ )) custom_elements_folder = os . path . join ( base_folder , 'custom_elements' ) registry = PhotonRegistry ( custom_elements_folder = custom_elements_folder ) registry . register ( photon_name = 'MyCustomEstimator' , class_str = 'custom_estimator.CustomEstimator' , element_type = 'Estimator' ) registry . activate () registry . info ( 'MyCustomEstimator' ) # get informations of other available elements registry . info ( 'SVC' ) Source code in photonai/base/registry/registry.py class PhotonRegistry : \"\"\" Helper class to manage the PHOTONAI Element Register. Use it to add and remove items into the register. You can also retrieve information about items and its hyperparameters. Every item in the register is encoded by a string literal that points to a python class and its namespace. You can access the python class via the string literal. The class PhotonElement imports and instantiates the class for you. Example: ``` python import os from photonai.base import PhotonRegistry # REGISTER ELEMENT saved in folder custom_elements_folder base_folder = os.path.dirname(os.path.abspath(__file__)) custom_elements_folder = os.path.join(base_folder, 'custom_elements') registry = PhotonRegistry(custom_elements_folder=custom_elements_folder) registry.register(photon_name='MyCustomEstimator', class_str='custom_estimator.CustomEstimator', element_type='Estimator') registry.activate() registry.info('MyCustomEstimator') # get informations of other available elements registry.info('SVC') ``` \"\"\" ELEMENT_DICTIONARY = dict () PHOTON_REGISTRIES = [ 'PhotonCore' ] CUSTOM_ELEMENTS_FOLDER = None CUSTOM_ELEMENTS = None def __init__ ( self , custom_elements_folder : str = None ): \"\"\" Initialize the object. Parameters: custom_elements_folder: Path to folder with custom element in it. \"\"\" self . current_folder = os . path . dirname ( os . path . abspath ( inspect . getfile ( inspect . currentframe ()))) self . module_path = os . path . join ( self . current_folder , \"modules\" ) if not os . path . isdir ( self . module_path ): os . mkdir ( self . module_path ) # update list with available sub_elements self . _list_available_modules () PhotonRegistry . CUSTOM_ELEMENTS_FOLDER = custom_elements_folder self . _load_custom_folder ( custom_elements_folder ) if len ( PhotonRegistry . ELEMENT_DICTIONARY ) == 0 or \\ PhotonRegistry . ELEMENT_DICTIONARY == PhotonRegistry . CUSTOM_ELEMENTS : PhotonRegistry . ELEMENT_DICTIONARY . update ( self . get_package_info ()) def _list_available_modules ( self ): for abs_filename in glob ( os . path . join ( self . module_path , \"*.json\" )): basename = os . path . basename ( abs_filename ) file , ext = os . path . splitext ( basename ) if file not in PhotonRegistry . PHOTON_REGISTRIES : PhotonRegistry . PHOTON_REGISTRIES . append ( file ) def add_module ( self , path_to_file : str ): filename = os . path . basename ( path_to_file ) copyfile ( path_to_file , os . path . join ( self . module_path , filename )) self . _list_available_modules () PhotonRegistry . ELEMENT_DICTIONARY = self . get_package_info () def delete_module ( self , module_name : str ): PhotonRegistry . PHOTON_REGISTRIES . remove ( module_name ) os . remove ( os . path . join ( self . module_path , module_name + \".json\" )) PhotonRegistry . ELEMENT_DICTIONARY = self . get_package_info () def _load_json ( self , photon_package : str ) -> dict : \"\"\" Load JSON file in which the elements for the PHOTON submodule are stored. The JSON files are stored in the framework folder by the name convention 'photon_package.json' Parameters: photon_package: The name of the photonai submodule Returns: JSON file as dict, file path as str. \"\"\" if photon_package == 'CustomElements' : folder = PhotonRegistry . CUSTOM_ELEMENTS_FOLDER if not folder : return {} elif photon_package == \"PhotonCore\" : folder = self . current_folder else : folder = self . module_path file_name = os . path . join ( folder , photon_package + '.json' ) file_content = {} # Reading json try : with open ( file_name , 'r' ) as f : try : file_content = json . load ( f ) except json . JSONDecodeError as jde : # handle empty file if jde . msg == 'Expecting value' : logger . error ( \"Package File \" + file_name + \" was empty.\" ) else : raise jde except FileNotFoundError : logger . error ( \"Could not find file for \" + photon_package ) if not file_content : file_content = dict () return file_content def get_package_info ( self , photon_package : list = PHOTON_REGISTRIES ) -> dict : \"\"\" Collect all registered elements from JSON file. Parameters: photon_package: The names of the PHOTONAI submodules for which the elements should be retrieved. Returns: Dict of registered elements. \"\"\" class_info = dict () for package in photon_package : content = self . _load_json ( package ) for idx , key in enumerate ( content ): class_path , class_name = os . path . splitext ( content [ key ][ 0 ]) if idx == 0 and package not in [ \"PhotonCore\" , \"CustomElements\" ]: # try to import something from module. # if that fails. drop this shit. try : imported_module = importlib . import_module ( class_path ) desired_class = getattr ( imported_module , class_name [ 1 :]) custom_element = desired_class () except ( AttributeError , ModuleNotFoundError ) as e : logger . error ( e ) logger . error ( \"Could not import from package {} . Deleting json.\" . format ( package )) self . delete_module ( package ) class_info [ key ] = class_path , class_name [ 1 :] return class_info def _load_custom_folder ( self , custom_elements_folder ): if PhotonRegistry . CUSTOM_ELEMENTS_FOLDER is not None : PhotonRegistry . CUSTOM_ELEMENTS_FOLDER = self . _check_custom_folder ( custom_elements_folder ) # load custom elements from json PhotonRegistry . CUSTOM_ELEMENTS = self . _load_json ( 'CustomElements' ) PhotonRegistry . PHOTON_REGISTRIES . append ( 'CustomElements' ) PhotonRegistry . ELEMENT_DICTIONARY . update ( PhotonRegistry . CUSTOM_ELEMENTS ) @staticmethod def _check_custom_folder ( custom_folder ): if not os . path . exists ( custom_folder ): logger . info ( 'Creating folder {} ' . format ( custom_folder )) os . makedirs ( custom_folder ) custom_file = os . path . join ( custom_folder , 'CustomElements.json' ) if not os . path . isfile ( custom_file ): logger . info ( 'Creating CustomElements.json' ) with open ( custom_file , 'w' ) as f : json . dump ( '' , f ) return custom_folder def activate ( self ): if not PhotonRegistry . CUSTOM_ELEMENTS_FOLDER : raise ValueError ( \"To activate a custom elements folder, specify a folder when instantiating the registry \" \"module. Example: registry = PhotonRegistry('/MY/CUSTOM/ELEMENTS/FOLDER) \" \"In case you don't have any custom models, there is no need to activate the registry.\" ) if not os . path . exists ( PhotonRegistry . CUSTOM_ELEMENTS_FOLDER ): raise FileNotFoundError ( \"Couldn't find custom elements folder: {} \" . format ( PhotonRegistry . CUSTOM_ELEMENTS_FOLDER )) if not os . path . isfile ( os . path . join ( PhotonRegistry . CUSTOM_ELEMENTS_FOLDER , 'CustomElements.json' )): raise FileNotFoundError ( \"Couldn't find CustomElements.json. Did you register your element first?\" ) # add folder to python path logger . info ( \"Adding custom elements folder to system path...\" ) sys . path . append ( PhotonRegistry . CUSTOM_ELEMENTS_FOLDER ) PhotonRegistry . ELEMENT_DICTIONARY . update ( self . get_package_info ([ 'CustomElements' ])) logger . info ( 'Successfully activated custom elements!' ) def register ( self , photon_name : str , class_str : str , element_type : str ): \"\"\" Save element information to the JSON file. Parameters: photon_name: The string literal with which you want to access the class. class_str: The namespace of the class, like in the import statement. element_type: Can be 'Estimator' or 'Transformer' \"\"\" # check if folder exists if not PhotonRegistry . CUSTOM_ELEMENTS_FOLDER : raise ValueError ( \"To register an element, specify a custom elements folder when instantiating the registry \" \"module. Example: registry = PhotonRegistry('/MY/CUSTOM/ELEMENTS/FOLDER)\" ) if not element_type == \"Estimator\" and not element_type == \"Transformer\" : raise ValueError ( \"Variable element_type must be 'Estimator' or 'Transformer'\" ) duplicate = self . _check_duplicate ( photon_name = photon_name , class_str = class_str , content = PhotonRegistry . CUSTOM_ELEMENTS ) if not duplicate : python_file = os . path . join ( PhotonRegistry . CUSTOM_ELEMENTS_FOLDER , class_str . split ( '.' )[ 0 ] + '.py' ) if not os . path . isfile ( python_file ): raise FileNotFoundError ( \"Couldn't find python file {} in your custom elements folder. \" \"Please copy your file into this folder first!\" . format ( python_file )) # add new element PhotonRegistry . CUSTOM_ELEMENTS [ photon_name ] = class_str , element_type # write back to file self . _write_to_json ( PhotonRegistry . CUSTOM_ELEMENTS ) logger . info ( 'Adding PipelineElement ' + class_str + ' to CustomElements.json as \"' + photon_name + '\".' ) # activate custom elements self . activate () # check custom element logger . info ( \"Running tests on custom element...\" ) return self . _run_tests ( photon_name , element_type ) else : logger . error ( 'Could not register element!' ) def _run_tests ( self , photon_name , element_type ): # this is a sneaky hack to avoid circular imports of PipelineElement imported_module = importlib . import_module ( \"photonai.base\" ) desired_class = getattr ( imported_module , \"PipelineElement\" ) custom_element = desired_class ( photon_name ) # check if has fit, transform, predict if not hasattr ( custom_element . base_element , 'fit' ): raise NotImplementedError ( \"Custom element does not implement fit() method.\" ) if element_type == 'Transformer' and not hasattr ( custom_element . base_element , 'transform' ): raise NotImplementedError ( \"Custom element does not implement transform() method.\" ) if element_type == 'Estimator' and not hasattr ( custom_element . base_element , 'predict' ): raise NotImplementedError ( \"Custom element does not implement predict() method.\" ) # check if estimator is regressor or classifier if element_type == 'Estimator' : if hasattr ( custom_element , '_estimator_type' ): est_type = getattr ( custom_element , '_estimator_type' ) if est_type == \"regressor\" : X , y = load_boston ( return_X_y = True ) elif est_type == \"classifier\" : X , y = load_breast_cancer ( return_X_y = True ) else : raise ValueError ( \"Custom element does not specify whether it is a regressor or classifier. \" \"Is {} \" . format ( est_type )) else : raise NotImplementedError ( \"Custom element does not specify whether it is a regressor or classifier. \" \"Consider inheritance from ClassifierMixin or RegressorMixin or set \" \"_estimator_type manually.\" ) else : X , y = load_boston ( return_X_y = True ) # try and test functionality kwargs = { 'covariates' : np . random . randn ( len ( y ))} try : # test fit returned_element = custom_element . base_element . fit ( X , y , ** kwargs ) except Exception as e : logger . info ( \"Not able to run tests on fit() method. Test data not compatible.\" ) return e if not isinstance ( returned_element , custom_element . base_element . __class__ ): raise NotImplementedError ( \"fit() method does not return self.\" ) try : # test transform or predict (if base element does not implement transform method, predict should be called # by PipelineElement -> so we only need to test transform() if custom_element . needs_y : if element_type == 'Estimator' : raise NotImplementedError ( \"Estimator should not need y.\" ) Xt , yt , kwargst = custom_element . base_element . transform ( X , y , ** kwargs ) if 'covariates' not in kwargst . keys (): raise ValueError ( \"Custom element does not correctly transform kwargs although needs_y is True. \" \"If you change the number of samples in transform(), make sure to transform kwargs \" \"respectively.\" ) if not len ( kwargst [ 'covariates' ]) == len ( X ): raise ValueError ( \"Custom element is not returning the correct number of samples!\" ) elif custom_element . needs_covariates : if element_type == 'Estimator' : yt , kwargst = custom_element . base_element . predict ( X , ** kwargs ) if not len ( yt ) == len ( y ): raise ValueError ( \"Custom element is not returning the correct number of samples!\" ) else : Xt , kwargst = custom_element . base_element . transform ( X , ** kwargs ) if not len ( Xt ) == len ( X ) or not len ( kwargst [ 'covariates' ]) == len ( X ): raise ValueError ( \"Custom element is not returning the correct number of samples!\" ) else : if element_type == 'Estimator' : yt = custom_element . base_element . predict ( X ) if not len ( yt ) == len ( y ): raise ValueError ( \"Custom element is not returning the correct number of samples!\" ) else : Xt = custom_element . base_element . transform ( X ) if not len ( Xt ) == len ( X ): raise ValueError ( \"Custom element is not returning the correct number of samples!\" ) except ValueError as ve : if \"too many values to unpack\" in ve . args [ 0 ]: raise ValueError ( \"Custom element does not return X, y and kwargs the way it should \" \"according to needs_y and needs_covariates.\" ) else : logger . info ( ve . args [ 0 ]) return ve except Exception as e : logger . info ( e . args [ 0 ]) logger . info ( \"Not able to run tests on transform() or predict() method. Test data probably not compatible.\" ) return e logger . info ( 'All tests on custom element passed.' ) def info ( self , photon_name : str ): \"\"\" Show information for object that is encoded by this name. Parameters: photon_name: The string literal which accesses the class. \"\"\" content = self . get_package_info () # load existing json if photon_name in content : element_namespace , element_name = content [ photon_name ] print ( \"----------------------------------\" ) print ( \"Name: \" + element_name ) print ( \"Namespace: \" + element_namespace ) print ( \"----------------------------------\" ) try : imported_module = __import__ ( element_namespace , globals (), locals (), element_name , 0 ) desired_class = getattr ( imported_module , element_name ) base_element = desired_class () print ( \"Possible Hyperparameters as derived from constructor:\" ) class_args = inspect . signature ( base_element . __init__ ) for item , more_info in class_args . parameters . items (): print ( \" {:<35} {:<75} \" . format ( item , str ( more_info ))) print ( \"----------------------------------\" ) except Exception as e : logger . error ( e ) logger . error ( \"Could not instantiate class \" + element_namespace + \".\" + element_name ) else : logger . error ( \"Could not find element \" + photon_name ) def delete ( self , photon_name : str ): \"\"\" Delete Element from JSON file. Parameters: photon_name: The string literal encoding the class. \"\"\" if photon_name in PhotonRegistry . CUSTOM_ELEMENTS : del PhotonRegistry . CUSTOM_ELEMENTS [ photon_name ] self . _write_to_json ( PhotonRegistry . CUSTOM_ELEMENTS ) logger . info ( 'Removing the PipelineElement named \" {0} \" from CustomElements.json.' . format ( photon_name )) else : logger . info ( 'Cannot remove \" {0} \" from CustomElements.json. Element has not been registered before.' . format ( photon_name )) @staticmethod def _check_duplicate ( photon_name , class_str : str , content : str ) -> bool : \"\"\" Helper function to check if the entry is either registered by a different name or if the name is already given to another class. Parameters: content: The content of the CustomElements.json. class_str: The namespace.Classname, where the class lives, from where it should be imported. photon_name: The name of the element with which it is called within PHOTONAI. Returns: False if there is no key with this name and the class is not already registered with another key. \"\"\" # check for duplicate name (dict key) if photon_name in content : logger . info ( 'A PipelineElement named ' + photon_name + ' has already been registered.' ) return True # check for duplicate class_str if any ( class_str in '.' . join ([ s [ 0 ], s [ 1 ]]) for s in content . values ()): logger . info ( 'The Class named ' + class_str + ' has already been registered.' ) return True return False def _write_to_json ( self , content_to_write : dict ): \"\"\" Write json content to file Parameters: content2write: The new information to attach to the file. \"\"\" # Writing JSON data with open ( os . path . join ( self . CUSTOM_ELEMENTS_FOLDER , \"CustomElements.json\" ), 'w' ) as f : json . dump ( content_to_write , f ) def list_available_elements ( self , photon_package : list = PHOTON_REGISTRIES ): \"\"\" Print info about all items that are registered for the PHOTONAI submodule to the console. Parameters: photon_package: The names of the PHOTON submodules for which the elements should be retrieved. \"\"\" if isinstance ( photon_package , str ): photon_package = [ photon_package ] for package in photon_package : content = self . _load_json ( package ) if len ( content ) > 0 : print ( ' \\n ' + package ) for k , v in sorted ( content . items ()): class_info , package_type = v print ( \" {:<35} {:<75} {:<5} \" . format ( k , class_info , package_type ))","title":"Documentation for PhotonRegistry"},{"location":"api/base/registry/#photonai.base.registry.registry.PhotonRegistry.__init__","text":"Initialize the object. Parameters: Name Type Description Default custom_elements_folder str Path to folder with custom element in it. None Source code in photonai/base/registry/registry.py def __init__ ( self , custom_elements_folder : str = None ): \"\"\" Initialize the object. Parameters: custom_elements_folder: Path to folder with custom element in it. \"\"\" self . current_folder = os . path . dirname ( os . path . abspath ( inspect . getfile ( inspect . currentframe ()))) self . module_path = os . path . join ( self . current_folder , \"modules\" ) if not os . path . isdir ( self . module_path ): os . mkdir ( self . module_path ) # update list with available sub_elements self . _list_available_modules () PhotonRegistry . CUSTOM_ELEMENTS_FOLDER = custom_elements_folder self . _load_custom_folder ( custom_elements_folder ) if len ( PhotonRegistry . ELEMENT_DICTIONARY ) == 0 or \\ PhotonRegistry . ELEMENT_DICTIONARY == PhotonRegistry . CUSTOM_ELEMENTS : PhotonRegistry . ELEMENT_DICTIONARY . update ( self . get_package_info ())","title":"__init__()"},{"location":"api/base/registry/#photonai.base.registry.registry.PhotonRegistry.delete","text":"Delete Element from JSON file. Parameters: Name Type Description Default photon_name str The string literal encoding the class. required Source code in photonai/base/registry/registry.py def delete ( self , photon_name : str ): \"\"\" Delete Element from JSON file. Parameters: photon_name: The string literal encoding the class. \"\"\" if photon_name in PhotonRegistry . CUSTOM_ELEMENTS : del PhotonRegistry . CUSTOM_ELEMENTS [ photon_name ] self . _write_to_json ( PhotonRegistry . CUSTOM_ELEMENTS ) logger . info ( 'Removing the PipelineElement named \" {0} \" from CustomElements.json.' . format ( photon_name )) else : logger . info ( 'Cannot remove \" {0} \" from CustomElements.json. Element has not been registered before.' . format ( photon_name ))","title":"delete()"},{"location":"api/base/registry/#photonai.base.registry.registry.PhotonRegistry.get_package_info","text":"Collect all registered elements from JSON file. Parameters: Name Type Description Default photon_package list The names of the PHOTONAI submodules for which the elements should be retrieved. ['PhotonCore'] Returns: Type Description dict Dict of registered elements. Source code in photonai/base/registry/registry.py def get_package_info ( self , photon_package : list = PHOTON_REGISTRIES ) -> dict : \"\"\" Collect all registered elements from JSON file. Parameters: photon_package: The names of the PHOTONAI submodules for which the elements should be retrieved. Returns: Dict of registered elements. \"\"\" class_info = dict () for package in photon_package : content = self . _load_json ( package ) for idx , key in enumerate ( content ): class_path , class_name = os . path . splitext ( content [ key ][ 0 ]) if idx == 0 and package not in [ \"PhotonCore\" , \"CustomElements\" ]: # try to import something from module. # if that fails. drop this shit. try : imported_module = importlib . import_module ( class_path ) desired_class = getattr ( imported_module , class_name [ 1 :]) custom_element = desired_class () except ( AttributeError , ModuleNotFoundError ) as e : logger . error ( e ) logger . error ( \"Could not import from package {} . Deleting json.\" . format ( package )) self . delete_module ( package ) class_info [ key ] = class_path , class_name [ 1 :] return class_info","title":"get_package_info()"},{"location":"api/base/registry/#photonai.base.registry.registry.PhotonRegistry.info","text":"Show information for object that is encoded by this name. Parameters: Name Type Description Default photon_name str The string literal which accesses the class. required Source code in photonai/base/registry/registry.py def info ( self , photon_name : str ): \"\"\" Show information for object that is encoded by this name. Parameters: photon_name: The string literal which accesses the class. \"\"\" content = self . get_package_info () # load existing json if photon_name in content : element_namespace , element_name = content [ photon_name ] print ( \"----------------------------------\" ) print ( \"Name: \" + element_name ) print ( \"Namespace: \" + element_namespace ) print ( \"----------------------------------\" ) try : imported_module = __import__ ( element_namespace , globals (), locals (), element_name , 0 ) desired_class = getattr ( imported_module , element_name ) base_element = desired_class () print ( \"Possible Hyperparameters as derived from constructor:\" ) class_args = inspect . signature ( base_element . __init__ ) for item , more_info in class_args . parameters . items (): print ( \" {:<35} {:<75} \" . format ( item , str ( more_info ))) print ( \"----------------------------------\" ) except Exception as e : logger . error ( e ) logger . error ( \"Could not instantiate class \" + element_namespace + \".\" + element_name ) else : logger . error ( \"Could not find element \" + photon_name )","title":"info()"},{"location":"api/base/registry/#photonai.base.registry.registry.PhotonRegistry.list_available_elements","text":"Print info about all items that are registered for the PHOTONAI submodule to the console. Parameters: Name Type Description Default photon_package list The names of the PHOTON submodules for which the elements should be retrieved. ['PhotonCore'] Source code in photonai/base/registry/registry.py def list_available_elements ( self , photon_package : list = PHOTON_REGISTRIES ): \"\"\" Print info about all items that are registered for the PHOTONAI submodule to the console. Parameters: photon_package: The names of the PHOTON submodules for which the elements should be retrieved. \"\"\" if isinstance ( photon_package , str ): photon_package = [ photon_package ] for package in photon_package : content = self . _load_json ( package ) if len ( content ) > 0 : print ( ' \\n ' + package ) for k , v in sorted ( content . items ()): class_info , package_type = v print ( \" {:<35} {:<75} {:<5} \" . format ( k , class_info , package_type ))","title":"list_available_elements()"},{"location":"api/base/registry/#photonai.base.registry.registry.PhotonRegistry.register","text":"Save element information to the JSON file. Parameters: Name Type Description Default photon_name str The string literal with which you want to access the class. required class_str str The namespace of the class, like in the import statement. required element_type str Can be 'Estimator' or 'Transformer' required Source code in photonai/base/registry/registry.py def register ( self , photon_name : str , class_str : str , element_type : str ): \"\"\" Save element information to the JSON file. Parameters: photon_name: The string literal with which you want to access the class. class_str: The namespace of the class, like in the import statement. element_type: Can be 'Estimator' or 'Transformer' \"\"\" # check if folder exists if not PhotonRegistry . CUSTOM_ELEMENTS_FOLDER : raise ValueError ( \"To register an element, specify a custom elements folder when instantiating the registry \" \"module. Example: registry = PhotonRegistry('/MY/CUSTOM/ELEMENTS/FOLDER)\" ) if not element_type == \"Estimator\" and not element_type == \"Transformer\" : raise ValueError ( \"Variable element_type must be 'Estimator' or 'Transformer'\" ) duplicate = self . _check_duplicate ( photon_name = photon_name , class_str = class_str , content = PhotonRegistry . CUSTOM_ELEMENTS ) if not duplicate : python_file = os . path . join ( PhotonRegistry . CUSTOM_ELEMENTS_FOLDER , class_str . split ( '.' )[ 0 ] + '.py' ) if not os . path . isfile ( python_file ): raise FileNotFoundError ( \"Couldn't find python file {} in your custom elements folder. \" \"Please copy your file into this folder first!\" . format ( python_file )) # add new element PhotonRegistry . CUSTOM_ELEMENTS [ photon_name ] = class_str , element_type # write back to file self . _write_to_json ( PhotonRegistry . CUSTOM_ELEMENTS ) logger . info ( 'Adding PipelineElement ' + class_str + ' to CustomElements.json as \"' + photon_name + '\".' ) # activate custom elements self . activate () # check custom element logger . info ( \"Running tests on custom element...\" ) return self . _run_tests ( photon_name , element_type ) else : logger . error ( 'Could not register element!' )","title":"register()"},{"location":"api/base/stack/","text":"Documentation for Stack Creates a vertical stacking/parallelization of pipeline items. The object acts as a single PipelineElement and encapsulates several vertically stacked other PipelineElements, each child receiving the same input data. The data is iteratively distributed to all children, the results are collected and horizontally concatenated. Examples: 1 2 3 4 tree = PipelineElement ( 'DecisionTreeClassifier' ) svc = PipelineElement ( 'LinearSVC' ) my_pipe += Stack ( 'final_stack' , [ tree , svc ], use_probabilities = True ) Source code in photonai/base/photon_elements.py class Stack ( PipelineElement ): \"\"\" Creates a vertical stacking/parallelization of pipeline items. The object acts as a single PipelineElement and encapsulates several vertically stacked other PipelineElements, each child receiving the same input data. The data is iteratively distributed to all children, the results are collected and horizontally concatenated. Example: ``` python tree = PipelineElement('DecisionTreeClassifier') svc = PipelineElement('LinearSVC') my_pipe += Stack('final_stack', [tree, svc], use_probabilities=True) ``` \"\"\" def __init__ ( self , name : str , elements : List [ PipelineElement ] = None , use_probabilities : bool = False ): \"\"\" Creates a new Stack element. Collects all possible hyperparameter combinations of the children. Parameters: name: Give the pipeline element a name. elements: List of pipeline elements that should run in parallel. use_probabilities: For a stack that includes estimators you can choose whether predict or predict_proba is called for all estimators. In case only some implement predict_proba, predict is called for the remaining estimators. \"\"\" super ( Stack , self ) . __init__ ( name , hyperparameters = {}, test_disabled = False , disabled = False , base_element = True ) self . _hyperparameters = {} self . elements = list () if elements is not None : for item_to_stack in elements : self . __iadd__ ( item_to_stack ) # todo: Stack should not be allowed to change y, only covariates self . needs_y = False self . needs_covariates = True self . identifier = \"STACK:\" self . use_probabilities = use_probabilities def __iadd__ ( self , item : PipelineElement ): \"\"\" Add a new element to the stack. Generate sklearn hyperparameter names in order to set the item's hyperparameters in the optimization process. Parameters: item: The Element that should be stacked and will run in a vertical parallelization in the original pipe. \"\"\" self . check_if_needs_y ( item ) super ( Stack , self ) . __iadd__ ( item ) # for each configuration tmp_dict = dict ( item . hyperparameters ) for key , element in tmp_dict . items (): self . _hyperparameters [ self . name + '__' + key ] = tmp_dict [ key ] return self def check_if_needs_y ( self , item ): if isinstance ( item , ( Branch , Stack , Switch )): for child_item in item . elements : self . check_if_needs_y ( child_item ) elif isinstance ( item , PipelineElement ): if item . needs_y : raise NotImplementedError ( \"Elements in Stack must not transform y because the number of samples in every \" \"element of the stack might differ. Then, it will not be possible to concatenate those \" \"data and target matrices. Please use the transformer that is using y before or after \" \"the stack.\" ) def add ( self , item : PipelineElement ): \"\"\" Add a new element to the stack. Generate sklearn hyperparameter names in order to set the item's hyperparameters in the optimization process. Parameters: item: The Element that should be stacked and will run in a vertical parallelization in the original pipe. \"\"\" self . __iadd__ ( item ) @property def hyperparameters ( self ): return self . _hyperparameters @hyperparameters . setter def hyperparameters ( self , value ): \"\"\" Setting hyperparameters does not make sense, only the items that added can be optimized, not the container (self). \"\"\" pass def generate_config_grid ( self ): tmp_grid = create_global_config_grid ( self . elements , self . name ) return tmp_grid def get_params ( self , deep = True ): all_params = {} for element in self . elements : all_params [ element . name ] = element . get_params ( deep ) return all_params def set_params ( self , ** kwargs ): \"\"\"Find the particular child and distribute the params to it\"\"\" spread_params_dict = {} for k , val in kwargs . items (): splitted_k = k . split ( '__' ) item_name = splitted_k [ 0 ] if item_name not in spread_params_dict : spread_params_dict [ item_name ] = {} dict_entry = { '__' . join ( splitted_k [ 1 ::]): val } spread_params_dict [ item_name ] . update ( dict_entry ) for name , params in spread_params_dict . items (): missing_element = ( name , params ) for element in self . elements : if element . name == name : element . set_params ( ** params ) missing_element = None if missing_element : raise ValueError ( \"Couldn't set hyperparameter for element {} -> {} \" . format ( missing_element [ 0 ], missing_element [ 1 ])) return self def fit ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ): \"\"\" Calls fit iteratively on every child. Parameters: X: The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. y: The truth array-like values with shape=[N], where N is the number of samples. **kwargs: Keyword arguments, passed to base_elements fit. Returns: Fitted self. \"\"\" for element in self . elements : # Todo: parallellize fitting element . fit ( X , y , ** kwargs ) return self def predict ( self , X : np . ndarray , ** kwargs ) -> np . ndarray : \"\"\" Calls the predict function on underlying base elements. Parameters: X: The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. **kwargs: Keyword arguments, passed to base_elements predict. Returns: Prediction values. \"\"\" if not self . use_probabilities : return self . _predict ( X , ** kwargs ) else : return self . predict_proba ( X , ** kwargs ) def _predict ( self , X : np . ndarray , ** kwargs ): \"\"\"Iteratively calls predict on every child.\"\"\" # Todo: strategy for concatenating data from different pipes # todo: parallelize prediction predicted_data = np . array ([]) for element in self . elements : element_transform = element . predict ( X , ** kwargs ) predicted_data = PhotonDataHelper . stack_data_horizontally ( predicted_data , element_transform ) return predicted_data def predict_proba ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ) -> np . ndarray : \"\"\" Predict probabilities for every pipe element and stack them together. Parameters: X: The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. y: The truth array-like values with shape=[N], where N is the number of samples. **kwargs: Keyword arguments, not used yet. Returns: Probability values. \"\"\" predicted_data = np . array ([]) for element in self . elements : element_transform = element . predict_proba ( X ) if element_transform is None : element_transform = element . predict ( X ) predicted_data = PhotonDataHelper . stack_data_horizontally ( predicted_data , element_transform ) return predicted_data def transform ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ) -> ( np . ndarray , np . ndarray , dict ): \"\"\" Calls transform on every child. If the encapsulated child is a hyperpipe, also calls predict on the last element in the pipeline. Parameters: X: The array-liketraining with shape=[N, D] and test data, where N is the number of samples and D is the number of features. y: The truth array-like values with shape=[N], where N is the number of samples. **kwargs: Keyword arguments, passed to base_elements transform. Returns: Prediction values. \"\"\" transformed_data = np . array ([]) for element in self . elements : # if it is a hyperpipe with a final estimator, we want to use predict: element_transform , _ , _ = element . transform ( X , y , ** kwargs ) transformed_data = PhotonDataHelper . stack_data_horizontally ( transformed_data , element_transform ) return transformed_data , y , kwargs def copy_me ( self ): ps = Stack ( self . name ) for element in self . elements : new_element = element . copy_me () ps += new_element ps . base_element = self . base_element ps . _random_state = self . _random_state return ps def inverse_transform ( self , X , y = None , ** kwargs ): raise NotImplementedError ( \"Inverse Transform is not yet implemented for a Stacking Element in PHOTON\" ) @property def _estimator_type ( self ): return None def _check_hyper ( self , BaseEstimator ): pass @property def feature_importances_ ( self ): return __iadd__ ( self , item ) special Add a new element to the stack. Generate sklearn hyperparameter names in order to set the item's hyperparameters in the optimization process. Parameters: Name Type Description Default item PipelineElement The Element that should be stacked and will run in a vertical parallelization in the original pipe. required Source code in photonai/base/photon_elements.py def __iadd__ ( self , item : PipelineElement ): \"\"\" Add a new element to the stack. Generate sklearn hyperparameter names in order to set the item's hyperparameters in the optimization process. Parameters: item: The Element that should be stacked and will run in a vertical parallelization in the original pipe. \"\"\" self . check_if_needs_y ( item ) super ( Stack , self ) . __iadd__ ( item ) # for each configuration tmp_dict = dict ( item . hyperparameters ) for key , element in tmp_dict . items (): self . _hyperparameters [ self . name + '__' + key ] = tmp_dict [ key ] return self __init__ ( self , name , elements = None , use_probabilities = False ) special Creates a new Stack element. Collects all possible hyperparameter combinations of the children. Parameters: Name Type Description Default name str Give the pipeline element a name. required elements List[photonai.base.photon_elements.PipelineElement] List of pipeline elements that should run in parallel. None use_probabilities bool For a stack that includes estimators you can choose whether predict or predict_proba is called for all estimators. In case only some implement predict_proba, predict is called for the remaining estimators. False Source code in photonai/base/photon_elements.py def __init__ ( self , name : str , elements : List [ PipelineElement ] = None , use_probabilities : bool = False ): \"\"\" Creates a new Stack element. Collects all possible hyperparameter combinations of the children. Parameters: name: Give the pipeline element a name. elements: List of pipeline elements that should run in parallel. use_probabilities: For a stack that includes estimators you can choose whether predict or predict_proba is called for all estimators. In case only some implement predict_proba, predict is called for the remaining estimators. \"\"\" super ( Stack , self ) . __init__ ( name , hyperparameters = {}, test_disabled = False , disabled = False , base_element = True ) self . _hyperparameters = {} self . elements = list () if elements is not None : for item_to_stack in elements : self . __iadd__ ( item_to_stack ) # todo: Stack should not be allowed to change y, only covariates self . needs_y = False self . needs_covariates = True self . identifier = \"STACK:\" self . use_probabilities = use_probabilities add ( self , item ) Add a new element to the stack. Generate sklearn hyperparameter names in order to set the item's hyperparameters in the optimization process. Parameters: Name Type Description Default item PipelineElement The Element that should be stacked and will run in a vertical parallelization in the original pipe. required Source code in photonai/base/photon_elements.py def add ( self , item : PipelineElement ): \"\"\" Add a new element to the stack. Generate sklearn hyperparameter names in order to set the item's hyperparameters in the optimization process. Parameters: item: The Element that should be stacked and will run in a vertical parallelization in the original pipe. \"\"\" self . __iadd__ ( item ) fit ( self , X , y = None , ** kwargs ) Calls fit iteratively on every child. Parameters: Name Type Description Default X ndarray The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. required y ndarray The truth array-like values with shape=[N], where N is the number of samples. None **kwargs Keyword arguments, passed to base_elements fit. {} Returns: Type Description Fitted self. Source code in photonai/base/photon_elements.py def fit ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ): \"\"\" Calls fit iteratively on every child. Parameters: X: The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. y: The truth array-like values with shape=[N], where N is the number of samples. **kwargs: Keyword arguments, passed to base_elements fit. Returns: Fitted self. \"\"\" for element in self . elements : # Todo: parallellize fitting element . fit ( X , y , ** kwargs ) return self predict ( self , X , ** kwargs ) Calls the predict function on underlying base elements. Parameters: Name Type Description Default X ndarray The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. required **kwargs Keyword arguments, passed to base_elements predict. {} Returns: Type Description ndarray Prediction values. Source code in photonai/base/photon_elements.py def predict ( self , X : np . ndarray , ** kwargs ) -> np . ndarray : \"\"\" Calls the predict function on underlying base elements. Parameters: X: The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. **kwargs: Keyword arguments, passed to base_elements predict. Returns: Prediction values. \"\"\" if not self . use_probabilities : return self . _predict ( X , ** kwargs ) else : return self . predict_proba ( X , ** kwargs ) predict_proba ( self , X , y = None , ** kwargs ) Predict probabilities for every pipe element and stack them together. Parameters: Name Type Description Default X ndarray The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. required y ndarray The truth array-like values with shape=[N], where N is the number of samples. None **kwargs Keyword arguments, not used yet. {} Returns: Type Description ndarray Probability values. Source code in photonai/base/photon_elements.py def predict_proba ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ) -> np . ndarray : \"\"\" Predict probabilities for every pipe element and stack them together. Parameters: X: The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. y: The truth array-like values with shape=[N], where N is the number of samples. **kwargs: Keyword arguments, not used yet. Returns: Probability values. \"\"\" predicted_data = np . array ([]) for element in self . elements : element_transform = element . predict_proba ( X ) if element_transform is None : element_transform = element . predict ( X ) predicted_data = PhotonDataHelper . stack_data_horizontally ( predicted_data , element_transform ) return predicted_data transform ( self , X , y = None , ** kwargs ) Calls transform on every child. If the encapsulated child is a hyperpipe, also calls predict on the last element in the pipeline. Parameters: Name Type Description Default X ndarray The array-liketraining with shape=[N, D] and test data, where N is the number of samples and D is the number of features. required y ndarray The truth array-like values with shape=[N], where N is the number of samples. None **kwargs Keyword arguments, passed to base_elements transform. {} Returns: Type Description (<class 'numpy.ndarray'>, <class 'numpy.ndarray'>, <class 'dict'>) Prediction values. Source code in photonai/base/photon_elements.py def transform ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ) -> ( np . ndarray , np . ndarray , dict ): \"\"\" Calls transform on every child. If the encapsulated child is a hyperpipe, also calls predict on the last element in the pipeline. Parameters: X: The array-liketraining with shape=[N, D] and test data, where N is the number of samples and D is the number of features. y: The truth array-like values with shape=[N], where N is the number of samples. **kwargs: Keyword arguments, passed to base_elements transform. Returns: Prediction values. \"\"\" transformed_data = np . array ([]) for element in self . elements : # if it is a hyperpipe with a final estimator, we want to use predict: element_transform , _ , _ = element . transform ( X , y , ** kwargs ) transformed_data = PhotonDataHelper . stack_data_horizontally ( transformed_data , element_transform ) return transformed_data , y , kwargs","title":"Stack"},{"location":"api/base/stack/#documentation-for-stack","text":"Creates a vertical stacking/parallelization of pipeline items. The object acts as a single PipelineElement and encapsulates several vertically stacked other PipelineElements, each child receiving the same input data. The data is iteratively distributed to all children, the results are collected and horizontally concatenated. Examples: 1 2 3 4 tree = PipelineElement ( 'DecisionTreeClassifier' ) svc = PipelineElement ( 'LinearSVC' ) my_pipe += Stack ( 'final_stack' , [ tree , svc ], use_probabilities = True ) Source code in photonai/base/photon_elements.py class Stack ( PipelineElement ): \"\"\" Creates a vertical stacking/parallelization of pipeline items. The object acts as a single PipelineElement and encapsulates several vertically stacked other PipelineElements, each child receiving the same input data. The data is iteratively distributed to all children, the results are collected and horizontally concatenated. Example: ``` python tree = PipelineElement('DecisionTreeClassifier') svc = PipelineElement('LinearSVC') my_pipe += Stack('final_stack', [tree, svc], use_probabilities=True) ``` \"\"\" def __init__ ( self , name : str , elements : List [ PipelineElement ] = None , use_probabilities : bool = False ): \"\"\" Creates a new Stack element. Collects all possible hyperparameter combinations of the children. Parameters: name: Give the pipeline element a name. elements: List of pipeline elements that should run in parallel. use_probabilities: For a stack that includes estimators you can choose whether predict or predict_proba is called for all estimators. In case only some implement predict_proba, predict is called for the remaining estimators. \"\"\" super ( Stack , self ) . __init__ ( name , hyperparameters = {}, test_disabled = False , disabled = False , base_element = True ) self . _hyperparameters = {} self . elements = list () if elements is not None : for item_to_stack in elements : self . __iadd__ ( item_to_stack ) # todo: Stack should not be allowed to change y, only covariates self . needs_y = False self . needs_covariates = True self . identifier = \"STACK:\" self . use_probabilities = use_probabilities def __iadd__ ( self , item : PipelineElement ): \"\"\" Add a new element to the stack. Generate sklearn hyperparameter names in order to set the item's hyperparameters in the optimization process. Parameters: item: The Element that should be stacked and will run in a vertical parallelization in the original pipe. \"\"\" self . check_if_needs_y ( item ) super ( Stack , self ) . __iadd__ ( item ) # for each configuration tmp_dict = dict ( item . hyperparameters ) for key , element in tmp_dict . items (): self . _hyperparameters [ self . name + '__' + key ] = tmp_dict [ key ] return self def check_if_needs_y ( self , item ): if isinstance ( item , ( Branch , Stack , Switch )): for child_item in item . elements : self . check_if_needs_y ( child_item ) elif isinstance ( item , PipelineElement ): if item . needs_y : raise NotImplementedError ( \"Elements in Stack must not transform y because the number of samples in every \" \"element of the stack might differ. Then, it will not be possible to concatenate those \" \"data and target matrices. Please use the transformer that is using y before or after \" \"the stack.\" ) def add ( self , item : PipelineElement ): \"\"\" Add a new element to the stack. Generate sklearn hyperparameter names in order to set the item's hyperparameters in the optimization process. Parameters: item: The Element that should be stacked and will run in a vertical parallelization in the original pipe. \"\"\" self . __iadd__ ( item ) @property def hyperparameters ( self ): return self . _hyperparameters @hyperparameters . setter def hyperparameters ( self , value ): \"\"\" Setting hyperparameters does not make sense, only the items that added can be optimized, not the container (self). \"\"\" pass def generate_config_grid ( self ): tmp_grid = create_global_config_grid ( self . elements , self . name ) return tmp_grid def get_params ( self , deep = True ): all_params = {} for element in self . elements : all_params [ element . name ] = element . get_params ( deep ) return all_params def set_params ( self , ** kwargs ): \"\"\"Find the particular child and distribute the params to it\"\"\" spread_params_dict = {} for k , val in kwargs . items (): splitted_k = k . split ( '__' ) item_name = splitted_k [ 0 ] if item_name not in spread_params_dict : spread_params_dict [ item_name ] = {} dict_entry = { '__' . join ( splitted_k [ 1 ::]): val } spread_params_dict [ item_name ] . update ( dict_entry ) for name , params in spread_params_dict . items (): missing_element = ( name , params ) for element in self . elements : if element . name == name : element . set_params ( ** params ) missing_element = None if missing_element : raise ValueError ( \"Couldn't set hyperparameter for element {} -> {} \" . format ( missing_element [ 0 ], missing_element [ 1 ])) return self def fit ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ): \"\"\" Calls fit iteratively on every child. Parameters: X: The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. y: The truth array-like values with shape=[N], where N is the number of samples. **kwargs: Keyword arguments, passed to base_elements fit. Returns: Fitted self. \"\"\" for element in self . elements : # Todo: parallellize fitting element . fit ( X , y , ** kwargs ) return self def predict ( self , X : np . ndarray , ** kwargs ) -> np . ndarray : \"\"\" Calls the predict function on underlying base elements. Parameters: X: The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. **kwargs: Keyword arguments, passed to base_elements predict. Returns: Prediction values. \"\"\" if not self . use_probabilities : return self . _predict ( X , ** kwargs ) else : return self . predict_proba ( X , ** kwargs ) def _predict ( self , X : np . ndarray , ** kwargs ): \"\"\"Iteratively calls predict on every child.\"\"\" # Todo: strategy for concatenating data from different pipes # todo: parallelize prediction predicted_data = np . array ([]) for element in self . elements : element_transform = element . predict ( X , ** kwargs ) predicted_data = PhotonDataHelper . stack_data_horizontally ( predicted_data , element_transform ) return predicted_data def predict_proba ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ) -> np . ndarray : \"\"\" Predict probabilities for every pipe element and stack them together. Parameters: X: The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. y: The truth array-like values with shape=[N], where N is the number of samples. **kwargs: Keyword arguments, not used yet. Returns: Probability values. \"\"\" predicted_data = np . array ([]) for element in self . elements : element_transform = element . predict_proba ( X ) if element_transform is None : element_transform = element . predict ( X ) predicted_data = PhotonDataHelper . stack_data_horizontally ( predicted_data , element_transform ) return predicted_data def transform ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ) -> ( np . ndarray , np . ndarray , dict ): \"\"\" Calls transform on every child. If the encapsulated child is a hyperpipe, also calls predict on the last element in the pipeline. Parameters: X: The array-liketraining with shape=[N, D] and test data, where N is the number of samples and D is the number of features. y: The truth array-like values with shape=[N], where N is the number of samples. **kwargs: Keyword arguments, passed to base_elements transform. Returns: Prediction values. \"\"\" transformed_data = np . array ([]) for element in self . elements : # if it is a hyperpipe with a final estimator, we want to use predict: element_transform , _ , _ = element . transform ( X , y , ** kwargs ) transformed_data = PhotonDataHelper . stack_data_horizontally ( transformed_data , element_transform ) return transformed_data , y , kwargs def copy_me ( self ): ps = Stack ( self . name ) for element in self . elements : new_element = element . copy_me () ps += new_element ps . base_element = self . base_element ps . _random_state = self . _random_state return ps def inverse_transform ( self , X , y = None , ** kwargs ): raise NotImplementedError ( \"Inverse Transform is not yet implemented for a Stacking Element in PHOTON\" ) @property def _estimator_type ( self ): return None def _check_hyper ( self , BaseEstimator ): pass @property def feature_importances_ ( self ): return","title":"Documentation for Stack"},{"location":"api/base/stack/#photonai.base.photon_elements.Stack.__iadd__","text":"Add a new element to the stack. Generate sklearn hyperparameter names in order to set the item's hyperparameters in the optimization process. Parameters: Name Type Description Default item PipelineElement The Element that should be stacked and will run in a vertical parallelization in the original pipe. required Source code in photonai/base/photon_elements.py def __iadd__ ( self , item : PipelineElement ): \"\"\" Add a new element to the stack. Generate sklearn hyperparameter names in order to set the item's hyperparameters in the optimization process. Parameters: item: The Element that should be stacked and will run in a vertical parallelization in the original pipe. \"\"\" self . check_if_needs_y ( item ) super ( Stack , self ) . __iadd__ ( item ) # for each configuration tmp_dict = dict ( item . hyperparameters ) for key , element in tmp_dict . items (): self . _hyperparameters [ self . name + '__' + key ] = tmp_dict [ key ] return self","title":"__iadd__()"},{"location":"api/base/stack/#photonai.base.photon_elements.Stack.__init__","text":"Creates a new Stack element. Collects all possible hyperparameter combinations of the children. Parameters: Name Type Description Default name str Give the pipeline element a name. required elements List[photonai.base.photon_elements.PipelineElement] List of pipeline elements that should run in parallel. None use_probabilities bool For a stack that includes estimators you can choose whether predict or predict_proba is called for all estimators. In case only some implement predict_proba, predict is called for the remaining estimators. False Source code in photonai/base/photon_elements.py def __init__ ( self , name : str , elements : List [ PipelineElement ] = None , use_probabilities : bool = False ): \"\"\" Creates a new Stack element. Collects all possible hyperparameter combinations of the children. Parameters: name: Give the pipeline element a name. elements: List of pipeline elements that should run in parallel. use_probabilities: For a stack that includes estimators you can choose whether predict or predict_proba is called for all estimators. In case only some implement predict_proba, predict is called for the remaining estimators. \"\"\" super ( Stack , self ) . __init__ ( name , hyperparameters = {}, test_disabled = False , disabled = False , base_element = True ) self . _hyperparameters = {} self . elements = list () if elements is not None : for item_to_stack in elements : self . __iadd__ ( item_to_stack ) # todo: Stack should not be allowed to change y, only covariates self . needs_y = False self . needs_covariates = True self . identifier = \"STACK:\" self . use_probabilities = use_probabilities","title":"__init__()"},{"location":"api/base/stack/#photonai.base.photon_elements.Stack.add","text":"Add a new element to the stack. Generate sklearn hyperparameter names in order to set the item's hyperparameters in the optimization process. Parameters: Name Type Description Default item PipelineElement The Element that should be stacked and will run in a vertical parallelization in the original pipe. required Source code in photonai/base/photon_elements.py def add ( self , item : PipelineElement ): \"\"\" Add a new element to the stack. Generate sklearn hyperparameter names in order to set the item's hyperparameters in the optimization process. Parameters: item: The Element that should be stacked and will run in a vertical parallelization in the original pipe. \"\"\" self . __iadd__ ( item )","title":"add()"},{"location":"api/base/stack/#photonai.base.photon_elements.Stack.fit","text":"Calls fit iteratively on every child. Parameters: Name Type Description Default X ndarray The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. required y ndarray The truth array-like values with shape=[N], where N is the number of samples. None **kwargs Keyword arguments, passed to base_elements fit. {} Returns: Type Description Fitted self. Source code in photonai/base/photon_elements.py def fit ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ): \"\"\" Calls fit iteratively on every child. Parameters: X: The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. y: The truth array-like values with shape=[N], where N is the number of samples. **kwargs: Keyword arguments, passed to base_elements fit. Returns: Fitted self. \"\"\" for element in self . elements : # Todo: parallellize fitting element . fit ( X , y , ** kwargs ) return self","title":"fit()"},{"location":"api/base/stack/#photonai.base.photon_elements.Stack.predict","text":"Calls the predict function on underlying base elements. Parameters: Name Type Description Default X ndarray The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. required **kwargs Keyword arguments, passed to base_elements predict. {} Returns: Type Description ndarray Prediction values. Source code in photonai/base/photon_elements.py def predict ( self , X : np . ndarray , ** kwargs ) -> np . ndarray : \"\"\" Calls the predict function on underlying base elements. Parameters: X: The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. **kwargs: Keyword arguments, passed to base_elements predict. Returns: Prediction values. \"\"\" if not self . use_probabilities : return self . _predict ( X , ** kwargs ) else : return self . predict_proba ( X , ** kwargs )","title":"predict()"},{"location":"api/base/stack/#photonai.base.photon_elements.Stack.predict_proba","text":"Predict probabilities for every pipe element and stack them together. Parameters: Name Type Description Default X ndarray The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. required y ndarray The truth array-like values with shape=[N], where N is the number of samples. None **kwargs Keyword arguments, not used yet. {} Returns: Type Description ndarray Probability values. Source code in photonai/base/photon_elements.py def predict_proba ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ) -> np . ndarray : \"\"\" Predict probabilities for every pipe element and stack them together. Parameters: X: The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. y: The truth array-like values with shape=[N], where N is the number of samples. **kwargs: Keyword arguments, not used yet. Returns: Probability values. \"\"\" predicted_data = np . array ([]) for element in self . elements : element_transform = element . predict_proba ( X ) if element_transform is None : element_transform = element . predict ( X ) predicted_data = PhotonDataHelper . stack_data_horizontally ( predicted_data , element_transform ) return predicted_data","title":"predict_proba()"},{"location":"api/base/stack/#photonai.base.photon_elements.Stack.transform","text":"Calls transform on every child. If the encapsulated child is a hyperpipe, also calls predict on the last element in the pipeline. Parameters: Name Type Description Default X ndarray The array-liketraining with shape=[N, D] and test data, where N is the number of samples and D is the number of features. required y ndarray The truth array-like values with shape=[N], where N is the number of samples. None **kwargs Keyword arguments, passed to base_elements transform. {} Returns: Type Description (<class 'numpy.ndarray'>, <class 'numpy.ndarray'>, <class 'dict'>) Prediction values. Source code in photonai/base/photon_elements.py def transform ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ) -> ( np . ndarray , np . ndarray , dict ): \"\"\" Calls transform on every child. If the encapsulated child is a hyperpipe, also calls predict on the last element in the pipeline. Parameters: X: The array-liketraining with shape=[N, D] and test data, where N is the number of samples and D is the number of features. y: The truth array-like values with shape=[N], where N is the number of samples. **kwargs: Keyword arguments, passed to base_elements transform. Returns: Prediction values. \"\"\" transformed_data = np . array ([]) for element in self . elements : # if it is a hyperpipe with a final estimator, we want to use predict: element_transform , _ , _ = element . transform ( X , y , ** kwargs ) transformed_data = PhotonDataHelper . stack_data_horizontally ( transformed_data , element_transform ) return transformed_data , y , kwargs","title":"transform()"},{"location":"api/base/switch/","text":"Documentation for Switch This class encapsulates several PipelineElements that belong at the same step of the pipeline, competing for being the best choice. If for example you want to find out if Preprocessing A or Preprocessing B is better at this position in the pipe. Or you want to test if a random forest outperforms the good old SVM. ATTENTION: This class is a construct that may be convenient but is not suitable for any complex optimizations. Currently optimization works for grid_search, random search and smac and the specializes switch optimizer. Examples: 1 2 3 4 5 6 7 8 9 10 11 12 from photonai.base import PipelineElement , Switch from photonai.optimization import IntegerRange # Estimator Switch svm = PipelineElement ( 'SVC' , hyperparameters = { 'kernel' : [ 'rbf' , 'linear' ]}) tree = PipelineElement ( 'DecisionTreeClassifier' , hyperparameters = { 'min_samples_split' : IntegerRange ( 2 , 5 ), 'min_samples_leaf' : IntegerRange ( 1 , 5 ), 'criterion' : [ 'gini' , 'entropy' ]}) my_pipe += Switch ( 'EstimatorSwitch' , [ svm , tree ]) Source code in photonai/base/photon_elements.py class Switch ( PipelineElement ): \"\"\" This class encapsulates several PipelineElements that belong at the same step of the pipeline, competing for being the best choice. If for example you want to find out if Preprocessing A or Preprocessing B is better at this position in the pipe. Or you want to test if a random forest outperforms the good old SVM. ATTENTION: This class is a construct that may be convenient but is not suitable for any complex optimizations. Currently optimization works for grid_search, random search and smac and the specializes switch optimizer. Example: ``` python from photonai.base import PipelineElement, Switch from photonai.optimization import IntegerRange # Estimator Switch svm = PipelineElement('SVC', hyperparameters={'kernel': ['rbf', 'linear']}) tree = PipelineElement('DecisionTreeClassifier', hyperparameters={'min_samples_split': IntegerRange(2, 5), 'min_samples_leaf': IntegerRange(1, 5), 'criterion': ['gini', 'entropy']}) my_pipe += Switch('EstimatorSwitch', [svm, tree]) ``` \"\"\" def __init__ ( self , name : str , elements : List [ PipelineElement ] = None , estimator_name : str = '' ): \"\"\" Creates a new Switch object and generated the hyperparameter combination grid. Parameters: name: How the element is called in the pipeline. elements: The competing pipeline elements. estimator_name: - \"\"\" self . _name = name self . initial_name = self . _name self . sklearn_name = self . name + \"__current_element\" self . _hyperparameters = {} self . _current_element = ( 1 , 1 ) self . pipeline_element_configurations = [] self . base_element = None self . disabled = False self . test_disabled = False self . batch_size = 0 self . estimator_name = estimator_name self . needs_y = True self . needs_covariates = True # we assume we test models against each other, but only guessing self . is_estimator = True self . is_transformer = True self . identifier = \"SWITCH:\" self . _random_state = False self . elements_dict = {} if elements : self . elements = elements self . generate_private_config_grid () for p_element in elements : self . elements_dict [ p_element . name ] = p_element else : self . elements = [] def __iadd__ ( self , pipeline_element : PipelineElement ): \"\"\" Add a new estimator or transformer object to the switch container. All items change their positions during testing. Parameters: pipeline_element: Item that should be tested against other competing elements at that position in the pipeline. \"\"\" super ( Switch , self ) . __iadd__ ( pipeline_element ) self . elements_dict [ pipeline_element . name ] = pipeline_element self . generate_private_config_grid () return self def add ( self , pipeline_element : PipelineElement ): \"\"\" Add a new estimator or transformer object to the switch container. All items change their positions during testing. Parameters: pipeline_element: Item that should be tested against other competing elements at that position in the pipeline. \"\"\" self . __iadd__ ( pipeline_element ) @property def hyperparameters ( self ): # Todo: return actual hyperparameters of all pipeline elements?? return self . _hyperparameters @hyperparameters . setter def hyperparameters ( self , value ): pass def generate_private_config_grid ( self ): # reset self . pipeline_element_configurations = [] # calculate anew hyperparameters = [] # generate possible combinations for each item respectively - do not mix hyperparameters across items for i , pipe_element in enumerate ( self . elements ): # distinct_values_config = create_global_config([pipe_element]) # add pipeline switch name in the config so that the hyperparameters can be set from other classes # pipeline switch will give the hyperparameters to the respective child # distinct_values_config_copy = {} # for config_key, config_value in distinct_values_config.items(): # distinct_values_config_copy[self.name + \"__\" + config_key] = config_value if hasattr ( pipe_element , 'generate_config_grid' ): element_configurations = pipe_element . generate_config_grid () final_configuration_list = [] if len ( element_configurations ) == 0 : final_configuration_list . append ({}) # else: for dict_item in element_configurations : # copy_of_dict_item = {} # for key, value in dict_item.items(): # copy_of_dict_item[self.name + '__' + key] = value final_configuration_list . append ( dict ( dict_item )) self . pipeline_element_configurations . append ( final_configuration_list ) hyperparameters += [( i , nr ) for nr in range ( len ( final_configuration_list ))] self . _hyperparameters = { self . sklearn_name : hyperparameters } @property def current_element ( self ): return self . _current_element @current_element . setter def current_element ( self , value ): self . _current_element = value self . base_element = self . elements [ self . current_element [ 0 ]] def get_params ( self , deep : bool = True ): if self . base_element : return self . base_element . get_params ( deep ) else : return {} def set_params ( self , ** kwargs ): \"\"\" The optimization process sees the amount of possible combinations and chooses one of them. Then this class activates the belonging element and prepared the element with the particular chosen configuration. \"\"\" config_nr = None config = None self . estimator_name = '' # copy dict for adaptations params = dict ( kwargs ) # in case we are operating with grid search if self . sklearn_name in params : config_nr = params [ self . sklearn_name ] elif 'current_element' in params : config_nr = params [ 'current_element' ] if \"estimator_name\" in kwargs : self . estimator_name = params [ \"estimator_name\" ] del params [ \"estimator_name\" ] self . base_element = self . elements_dict [ self . estimator_name ] if params is not None : config = params # todo: raise Warning that Switch could not identify which estimator to set when estimator # has no params to optimize # in case we are operating with grid search or any derivates if config_nr is not None : if not isinstance ( config_nr , ( tuple , list )): logger . error ( 'ValueError: current_element must be of type Tuple' ) raise ValueError ( 'current_element must be of type Tuple' ) # grid search hack self . current_element = config_nr config = self . pipeline_element_configurations [ config_nr [ 0 ]][ config_nr [ 1 ]] # if we don't use the specialized switch optimizer # we need to identify the element to activate by checking for which element the optimizer gave params elif not self . estimator_name : # ugly hack because subscription is somehow not possible, we use the for loop but break for kwargs_key , kwargs_value in params . items (): first_element_name = kwargs_key . split ( \"__\" )[ 0 ] self . base_element = self . elements_dict [ first_element_name ] break # so now the element to be activated is found and taken care of, # let's move on to give the base element the config to set if config : # remove name unnamed_config = {} for config_key , config_value in config . items (): key_split = config_key . split ( '__' ) unnamed_config [ '__' . join ( key_split [ 1 ::])] = config_value self . base_element . set_params ( ** unnamed_config ) return self def copy_me ( self ): ps = Switch ( self . name ) ps . _random_state = self . _random_state for element in self . elements : new_element = element . copy_me () ps += new_element ps . _current_element = self . _current_element return ps def prettify_config_output ( self , config_name , config_value , return_dict = False ) -> str : \"\"\" Makes the sklearn configuration dictionary human readable. Returns: Configuration as prettified string or configuration as dict with prettified keys. \"\"\" if isinstance ( config_value , tuple ): output = self . pipeline_element_configurations [ config_value [ 0 ]][ config_value [ 1 ]] if not output : if return_dict : return { self . elements [ config_value [ 0 ]] . name : None } else : return self . elements [ config_value [ 0 ]] . name else : if return_dict : return output return str ( output ) else : return super ( Switch , self ) . prettify_config_output ( config_name , config_value ) def predict_proba ( self , X : np . ndarray , ** kwargs ) -> Union [ np . ndarray , None ]: \"\"\" Predict probabilities. Base element needs predict_proba() function, otherwise return None. Parameters: X: The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. **kwargs: Keyword arguments, not in use yet. Returns: Probabilities. \"\"\" if not self . disabled : if hasattr ( self . base_element . base_element , 'predict_proba' ): return self . base_element . predict_proba ( X ) else : return None return X def _check_hyper ( self , BaseEstimator ): pass def inverse_transform ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ): \"\"\" Calls inverse_transform on the base element. For a dimension preserving transformer without inverse, the value is returned untreated. Parameters: X: The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. y: The truth array-like values with shape=[N], where N is the number of samples. **kwargs: Keyword arguments, passed to base_element.transform. Returns: (X, y, kwargs) in back-transformed version if possible. \"\"\" if hasattr ( self . base_element , 'inverse_transform' ): # todo: check this X , y , kwargs = self . adjusted_delegate_call ( self . base_element . inverse_transform , X , y , ** kwargs ) return X , y , kwargs @property def _estimator_type ( self ): estimator_types = list () for element in self . elements : estimator_types . append ( getattr ( element , '_estimator_type' )) unique_types = set ( estimator_types ) if len ( unique_types ) > 1 : raise NotImplementedError ( \"Switch should only contain elements of a single type (transformer, classifier, \" \"regressor). Found multiple types: {} \" . format ( unique_types )) elif len ( unique_types ) == 1 : return list ( unique_types )[ 0 ] else : return @property def feature_importances_ ( self ): if hasattr ( self . base_element , 'feature_importances_' ): return getattr ( self . base_element , 'feature_importances_' ) __iadd__ ( self , pipeline_element ) special Add a new estimator or transformer object to the switch container. All items change their positions during testing. Parameters: Name Type Description Default pipeline_element PipelineElement Item that should be tested against other competing elements at that position in the pipeline. required Source code in photonai/base/photon_elements.py def __iadd__ ( self , pipeline_element : PipelineElement ): \"\"\" Add a new estimator or transformer object to the switch container. All items change their positions during testing. Parameters: pipeline_element: Item that should be tested against other competing elements at that position in the pipeline. \"\"\" super ( Switch , self ) . __iadd__ ( pipeline_element ) self . elements_dict [ pipeline_element . name ] = pipeline_element self . generate_private_config_grid () return self __init__ ( self , name , elements = None , estimator_name = '' ) special Creates a new Switch object and generated the hyperparameter combination grid. Parameters: Name Type Description Default name str How the element is called in the pipeline. required elements List[photonai.base.photon_elements.PipelineElement] The competing pipeline elements. None estimator_name str - '' Source code in photonai/base/photon_elements.py def __init__ ( self , name : str , elements : List [ PipelineElement ] = None , estimator_name : str = '' ): \"\"\" Creates a new Switch object and generated the hyperparameter combination grid. Parameters: name: How the element is called in the pipeline. elements: The competing pipeline elements. estimator_name: - \"\"\" self . _name = name self . initial_name = self . _name self . sklearn_name = self . name + \"__current_element\" self . _hyperparameters = {} self . _current_element = ( 1 , 1 ) self . pipeline_element_configurations = [] self . base_element = None self . disabled = False self . test_disabled = False self . batch_size = 0 self . estimator_name = estimator_name self . needs_y = True self . needs_covariates = True # we assume we test models against each other, but only guessing self . is_estimator = True self . is_transformer = True self . identifier = \"SWITCH:\" self . _random_state = False self . elements_dict = {} if elements : self . elements = elements self . generate_private_config_grid () for p_element in elements : self . elements_dict [ p_element . name ] = p_element else : self . elements = [] add ( self , pipeline_element ) Add a new estimator or transformer object to the switch container. All items change their positions during testing. Parameters: Name Type Description Default pipeline_element PipelineElement Item that should be tested against other competing elements at that position in the pipeline. required Source code in photonai/base/photon_elements.py def add ( self , pipeline_element : PipelineElement ): \"\"\" Add a new estimator or transformer object to the switch container. All items change their positions during testing. Parameters: pipeline_element: Item that should be tested against other competing elements at that position in the pipeline. \"\"\" self . __iadd__ ( pipeline_element ) predict_proba ( self , X , ** kwargs ) Predict probabilities. Base element needs predict_proba() function, otherwise return None. Parameters: Name Type Description Default X ndarray The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. required **kwargs Keyword arguments, not in use yet. {} Returns: Type Description Optional[numpy.ndarray] Probabilities. Source code in photonai/base/photon_elements.py def predict_proba ( self , X : np . ndarray , ** kwargs ) -> Union [ np . ndarray , None ]: \"\"\" Predict probabilities. Base element needs predict_proba() function, otherwise return None. Parameters: X: The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. **kwargs: Keyword arguments, not in use yet. Returns: Probabilities. \"\"\" if not self . disabled : if hasattr ( self . base_element . base_element , 'predict_proba' ): return self . base_element . predict_proba ( X ) else : return None return X","title":"Switch"},{"location":"api/base/switch/#documentation-for-switch","text":"This class encapsulates several PipelineElements that belong at the same step of the pipeline, competing for being the best choice. If for example you want to find out if Preprocessing A or Preprocessing B is better at this position in the pipe. Or you want to test if a random forest outperforms the good old SVM. ATTENTION: This class is a construct that may be convenient but is not suitable for any complex optimizations. Currently optimization works for grid_search, random search and smac and the specializes switch optimizer. Examples: 1 2 3 4 5 6 7 8 9 10 11 12 from photonai.base import PipelineElement , Switch from photonai.optimization import IntegerRange # Estimator Switch svm = PipelineElement ( 'SVC' , hyperparameters = { 'kernel' : [ 'rbf' , 'linear' ]}) tree = PipelineElement ( 'DecisionTreeClassifier' , hyperparameters = { 'min_samples_split' : IntegerRange ( 2 , 5 ), 'min_samples_leaf' : IntegerRange ( 1 , 5 ), 'criterion' : [ 'gini' , 'entropy' ]}) my_pipe += Switch ( 'EstimatorSwitch' , [ svm , tree ]) Source code in photonai/base/photon_elements.py class Switch ( PipelineElement ): \"\"\" This class encapsulates several PipelineElements that belong at the same step of the pipeline, competing for being the best choice. If for example you want to find out if Preprocessing A or Preprocessing B is better at this position in the pipe. Or you want to test if a random forest outperforms the good old SVM. ATTENTION: This class is a construct that may be convenient but is not suitable for any complex optimizations. Currently optimization works for grid_search, random search and smac and the specializes switch optimizer. Example: ``` python from photonai.base import PipelineElement, Switch from photonai.optimization import IntegerRange # Estimator Switch svm = PipelineElement('SVC', hyperparameters={'kernel': ['rbf', 'linear']}) tree = PipelineElement('DecisionTreeClassifier', hyperparameters={'min_samples_split': IntegerRange(2, 5), 'min_samples_leaf': IntegerRange(1, 5), 'criterion': ['gini', 'entropy']}) my_pipe += Switch('EstimatorSwitch', [svm, tree]) ``` \"\"\" def __init__ ( self , name : str , elements : List [ PipelineElement ] = None , estimator_name : str = '' ): \"\"\" Creates a new Switch object and generated the hyperparameter combination grid. Parameters: name: How the element is called in the pipeline. elements: The competing pipeline elements. estimator_name: - \"\"\" self . _name = name self . initial_name = self . _name self . sklearn_name = self . name + \"__current_element\" self . _hyperparameters = {} self . _current_element = ( 1 , 1 ) self . pipeline_element_configurations = [] self . base_element = None self . disabled = False self . test_disabled = False self . batch_size = 0 self . estimator_name = estimator_name self . needs_y = True self . needs_covariates = True # we assume we test models against each other, but only guessing self . is_estimator = True self . is_transformer = True self . identifier = \"SWITCH:\" self . _random_state = False self . elements_dict = {} if elements : self . elements = elements self . generate_private_config_grid () for p_element in elements : self . elements_dict [ p_element . name ] = p_element else : self . elements = [] def __iadd__ ( self , pipeline_element : PipelineElement ): \"\"\" Add a new estimator or transformer object to the switch container. All items change their positions during testing. Parameters: pipeline_element: Item that should be tested against other competing elements at that position in the pipeline. \"\"\" super ( Switch , self ) . __iadd__ ( pipeline_element ) self . elements_dict [ pipeline_element . name ] = pipeline_element self . generate_private_config_grid () return self def add ( self , pipeline_element : PipelineElement ): \"\"\" Add a new estimator or transformer object to the switch container. All items change their positions during testing. Parameters: pipeline_element: Item that should be tested against other competing elements at that position in the pipeline. \"\"\" self . __iadd__ ( pipeline_element ) @property def hyperparameters ( self ): # Todo: return actual hyperparameters of all pipeline elements?? return self . _hyperparameters @hyperparameters . setter def hyperparameters ( self , value ): pass def generate_private_config_grid ( self ): # reset self . pipeline_element_configurations = [] # calculate anew hyperparameters = [] # generate possible combinations for each item respectively - do not mix hyperparameters across items for i , pipe_element in enumerate ( self . elements ): # distinct_values_config = create_global_config([pipe_element]) # add pipeline switch name in the config so that the hyperparameters can be set from other classes # pipeline switch will give the hyperparameters to the respective child # distinct_values_config_copy = {} # for config_key, config_value in distinct_values_config.items(): # distinct_values_config_copy[self.name + \"__\" + config_key] = config_value if hasattr ( pipe_element , 'generate_config_grid' ): element_configurations = pipe_element . generate_config_grid () final_configuration_list = [] if len ( element_configurations ) == 0 : final_configuration_list . append ({}) # else: for dict_item in element_configurations : # copy_of_dict_item = {} # for key, value in dict_item.items(): # copy_of_dict_item[self.name + '__' + key] = value final_configuration_list . append ( dict ( dict_item )) self . pipeline_element_configurations . append ( final_configuration_list ) hyperparameters += [( i , nr ) for nr in range ( len ( final_configuration_list ))] self . _hyperparameters = { self . sklearn_name : hyperparameters } @property def current_element ( self ): return self . _current_element @current_element . setter def current_element ( self , value ): self . _current_element = value self . base_element = self . elements [ self . current_element [ 0 ]] def get_params ( self , deep : bool = True ): if self . base_element : return self . base_element . get_params ( deep ) else : return {} def set_params ( self , ** kwargs ): \"\"\" The optimization process sees the amount of possible combinations and chooses one of them. Then this class activates the belonging element and prepared the element with the particular chosen configuration. \"\"\" config_nr = None config = None self . estimator_name = '' # copy dict for adaptations params = dict ( kwargs ) # in case we are operating with grid search if self . sklearn_name in params : config_nr = params [ self . sklearn_name ] elif 'current_element' in params : config_nr = params [ 'current_element' ] if \"estimator_name\" in kwargs : self . estimator_name = params [ \"estimator_name\" ] del params [ \"estimator_name\" ] self . base_element = self . elements_dict [ self . estimator_name ] if params is not None : config = params # todo: raise Warning that Switch could not identify which estimator to set when estimator # has no params to optimize # in case we are operating with grid search or any derivates if config_nr is not None : if not isinstance ( config_nr , ( tuple , list )): logger . error ( 'ValueError: current_element must be of type Tuple' ) raise ValueError ( 'current_element must be of type Tuple' ) # grid search hack self . current_element = config_nr config = self . pipeline_element_configurations [ config_nr [ 0 ]][ config_nr [ 1 ]] # if we don't use the specialized switch optimizer # we need to identify the element to activate by checking for which element the optimizer gave params elif not self . estimator_name : # ugly hack because subscription is somehow not possible, we use the for loop but break for kwargs_key , kwargs_value in params . items (): first_element_name = kwargs_key . split ( \"__\" )[ 0 ] self . base_element = self . elements_dict [ first_element_name ] break # so now the element to be activated is found and taken care of, # let's move on to give the base element the config to set if config : # remove name unnamed_config = {} for config_key , config_value in config . items (): key_split = config_key . split ( '__' ) unnamed_config [ '__' . join ( key_split [ 1 ::])] = config_value self . base_element . set_params ( ** unnamed_config ) return self def copy_me ( self ): ps = Switch ( self . name ) ps . _random_state = self . _random_state for element in self . elements : new_element = element . copy_me () ps += new_element ps . _current_element = self . _current_element return ps def prettify_config_output ( self , config_name , config_value , return_dict = False ) -> str : \"\"\" Makes the sklearn configuration dictionary human readable. Returns: Configuration as prettified string or configuration as dict with prettified keys. \"\"\" if isinstance ( config_value , tuple ): output = self . pipeline_element_configurations [ config_value [ 0 ]][ config_value [ 1 ]] if not output : if return_dict : return { self . elements [ config_value [ 0 ]] . name : None } else : return self . elements [ config_value [ 0 ]] . name else : if return_dict : return output return str ( output ) else : return super ( Switch , self ) . prettify_config_output ( config_name , config_value ) def predict_proba ( self , X : np . ndarray , ** kwargs ) -> Union [ np . ndarray , None ]: \"\"\" Predict probabilities. Base element needs predict_proba() function, otherwise return None. Parameters: X: The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. **kwargs: Keyword arguments, not in use yet. Returns: Probabilities. \"\"\" if not self . disabled : if hasattr ( self . base_element . base_element , 'predict_proba' ): return self . base_element . predict_proba ( X ) else : return None return X def _check_hyper ( self , BaseEstimator ): pass def inverse_transform ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ): \"\"\" Calls inverse_transform on the base element. For a dimension preserving transformer without inverse, the value is returned untreated. Parameters: X: The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. y: The truth array-like values with shape=[N], where N is the number of samples. **kwargs: Keyword arguments, passed to base_element.transform. Returns: (X, y, kwargs) in back-transformed version if possible. \"\"\" if hasattr ( self . base_element , 'inverse_transform' ): # todo: check this X , y , kwargs = self . adjusted_delegate_call ( self . base_element . inverse_transform , X , y , ** kwargs ) return X , y , kwargs @property def _estimator_type ( self ): estimator_types = list () for element in self . elements : estimator_types . append ( getattr ( element , '_estimator_type' )) unique_types = set ( estimator_types ) if len ( unique_types ) > 1 : raise NotImplementedError ( \"Switch should only contain elements of a single type (transformer, classifier, \" \"regressor). Found multiple types: {} \" . format ( unique_types )) elif len ( unique_types ) == 1 : return list ( unique_types )[ 0 ] else : return @property def feature_importances_ ( self ): if hasattr ( self . base_element , 'feature_importances_' ): return getattr ( self . base_element , 'feature_importances_' )","title":"Documentation for Switch"},{"location":"api/base/switch/#photonai.base.photon_elements.Switch.__iadd__","text":"Add a new estimator or transformer object to the switch container. All items change their positions during testing. Parameters: Name Type Description Default pipeline_element PipelineElement Item that should be tested against other competing elements at that position in the pipeline. required Source code in photonai/base/photon_elements.py def __iadd__ ( self , pipeline_element : PipelineElement ): \"\"\" Add a new estimator or transformer object to the switch container. All items change their positions during testing. Parameters: pipeline_element: Item that should be tested against other competing elements at that position in the pipeline. \"\"\" super ( Switch , self ) . __iadd__ ( pipeline_element ) self . elements_dict [ pipeline_element . name ] = pipeline_element self . generate_private_config_grid () return self","title":"__iadd__()"},{"location":"api/base/switch/#photonai.base.photon_elements.Switch.__init__","text":"Creates a new Switch object and generated the hyperparameter combination grid. Parameters: Name Type Description Default name str How the element is called in the pipeline. required elements List[photonai.base.photon_elements.PipelineElement] The competing pipeline elements. None estimator_name str - '' Source code in photonai/base/photon_elements.py def __init__ ( self , name : str , elements : List [ PipelineElement ] = None , estimator_name : str = '' ): \"\"\" Creates a new Switch object and generated the hyperparameter combination grid. Parameters: name: How the element is called in the pipeline. elements: The competing pipeline elements. estimator_name: - \"\"\" self . _name = name self . initial_name = self . _name self . sklearn_name = self . name + \"__current_element\" self . _hyperparameters = {} self . _current_element = ( 1 , 1 ) self . pipeline_element_configurations = [] self . base_element = None self . disabled = False self . test_disabled = False self . batch_size = 0 self . estimator_name = estimator_name self . needs_y = True self . needs_covariates = True # we assume we test models against each other, but only guessing self . is_estimator = True self . is_transformer = True self . identifier = \"SWITCH:\" self . _random_state = False self . elements_dict = {} if elements : self . elements = elements self . generate_private_config_grid () for p_element in elements : self . elements_dict [ p_element . name ] = p_element else : self . elements = []","title":"__init__()"},{"location":"api/base/switch/#photonai.base.photon_elements.Switch.add","text":"Add a new estimator or transformer object to the switch container. All items change their positions during testing. Parameters: Name Type Description Default pipeline_element PipelineElement Item that should be tested against other competing elements at that position in the pipeline. required Source code in photonai/base/photon_elements.py def add ( self , pipeline_element : PipelineElement ): \"\"\" Add a new estimator or transformer object to the switch container. All items change their positions during testing. Parameters: pipeline_element: Item that should be tested against other competing elements at that position in the pipeline. \"\"\" self . __iadd__ ( pipeline_element )","title":"add()"},{"location":"api/base/switch/#photonai.base.photon_elements.Switch.predict_proba","text":"Predict probabilities. Base element needs predict_proba() function, otherwise return None. Parameters: Name Type Description Default X ndarray The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. required **kwargs Keyword arguments, not in use yet. {} Returns: Type Description Optional[numpy.ndarray] Probabilities. Source code in photonai/base/photon_elements.py def predict_proba ( self , X : np . ndarray , ** kwargs ) -> Union [ np . ndarray , None ]: \"\"\" Predict probabilities. Base element needs predict_proba() function, otherwise return None. Parameters: X: The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. **kwargs: Keyword arguments, not in use yet. Returns: Probabilities. \"\"\" if not self . disabled : if hasattr ( self . base_element . base_element , 'predict_proba' ): return self . base_element . predict_proba ( X ) else : return None return X","title":"predict_proba()"},{"location":"api/modelwrapper/base_model_wrapper/","text":"Documentation for BaseModelWrapper The PHOTONAI interface for implementing custom pipeline elements. PHOTONAI works on top of the scikit-learn object API, see documentation . Your class should overwrite the following definitions: fit(data) : learn or adjust to the data. If it is an estimator, which means it has the ability to learn, it should implement predict(data) : using the learned model to generate prediction, should inherit sklearn.base.BaseEstimator ( see here ), inherits get_params and set_params . If it is an transformer, which means it preprocesses or prepares the data, it should implement transform(data) : applying the logic to the data to transform it, should inherit from sklearn.base.TransformerMixin ( see here ), inherits fit_transform as a concatenation of both fit and transform, should inherit sklearn.base.BaseEstimator ( see here ) inherits get_params and set_params . Prepare for hyperparameter optimization PHOTONAI expects a definition for all parameters you want to optimize in the hyperparameter search in the constructor stub , and to be addressable with the same name as class variable . In this way you can define any parameter and it is automatically prepared for the hyperparameter search process. See the scikit-learn object API documentation for more in depth information about the interface. Source code in photonai/modelwrapper/base_model_wrapper.py class BaseModelWrapper ( BaseEstimator ): \"\"\" The PHOTONAI interface for implementing custom pipeline elements. PHOTONAI works on top of the scikit-learn object API, [see documentation](http://scikit-learn.org/stable/developers/contributing.html#apis-of-scikit-learn-objects). Your class should overwrite the following definitions: - `fit(data)`: learn or adjust to the data. If it is an estimator, which means it has the ability to learn, - it should implement `predict(data)`: using the learned model to generate prediction, - should inherit *sklearn.base.BaseEstimator* ([see here]( http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html)), - inherits *get_params* and *set_params*. If it is an transformer, which means it preprocesses or prepares the data, - it should implement `transform(data)`: applying the logic to the data to transform it, - should inherit from *sklearn.base.TransformerMixin* ([see here]( http://scikit-learn.org/stable/modules/generated/sklearn.base.TransformerMixin.html)), - inherits *fit_transform* as a concatenation of both fit and transform, - should inherit *sklearn.base.BaseEstimator* ([see here]( http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html)) - inherits *get_params* and *set_params*. `Prepare for hyperparameter optimization` PHOTONAI expects a `definition for all parameters` you want to optimize in the hyperparameter search in the `constructor stub`, and to be addressable with the `same name as class variable`. In this way you can define any parameter and it is automatically prepared for the hyperparameter search process. See the [scikit-learn object API documentation]( http://scikit-learn.org/stable/developers/contributing.html#apis-of-scikit-learn-objects) for more in depth information about the interface. \"\"\" def __init__ ( self ): pass def fit ( self , data : np . ndarray , targets : np . ndarray = None ): \"\"\" Adjust the underlying model or method to the data. Parameters: data: The input samples of shape [n_samples, n_original_features]. targets: The input targets of shape [n_samples, 1]. Returns: IMPORTANT, must return self! \"\"\" def predict ( self , data : np . ndarray ): \"\"\" Use the learned model to make predictions. Parameters: data: The input samples of shape [n_samples, n_original_features]. \"\"\" def transform ( self , data : np . ndarray , targets : np . ndarray = None ): \"\"\" Apply the method's logic to the data. Parameters: data: The input samples of shape [n_samples, n_original_features]. targets: The input targets of shape [n_samples, 1]. Not necessary. \"\"\" def get_params ( self , deep : bool = True ) -> dict : \"\"\" Get the models parameters. Automatically implemented when inheriting from sklearn.base.BaseEstimator Parameters: deep: If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns: Parameter names mapped to their values. \"\"\" return super ( BaseModelWrapper , self ) . get_params ( deep = deep ) def set_params ( self , ** kwargs ): \"\"\" Takes the given dictionary, with the keys being the variable name, and sets the object's parameters to the given values. Automatically implemented when inheriting from sklearn.base.BaseEstimator. Parameters: **kwargs: Estimator parameters. \"\"\" super ( BaseModelWrapper , self ) . set_params ( ** kwargs ) fit ( self , data , targets = None ) Adjust the underlying model or method to the data. Parameters: Name Type Description Default data ndarray The input samples of shape [n_samples, n_original_features]. required targets ndarray The input targets of shape [n_samples, 1]. None Returns: Type Description IMPORTANT, must return self! Source code in photonai/modelwrapper/base_model_wrapper.py def fit ( self , data : np . ndarray , targets : np . ndarray = None ): \"\"\" Adjust the underlying model or method to the data. Parameters: data: The input samples of shape [n_samples, n_original_features]. targets: The input targets of shape [n_samples, 1]. Returns: IMPORTANT, must return self! \"\"\" get_params ( self , deep = True ) Get the models parameters. Automatically implemented when inheriting from sklearn.base.BaseEstimator Parameters: Name Type Description Default deep bool If True, will return the parameters for this estimator and contained subobjects that are estimators. True Returns: Type Description dict Parameter names mapped to their values. Source code in photonai/modelwrapper/base_model_wrapper.py def get_params ( self , deep : bool = True ) -> dict : \"\"\" Get the models parameters. Automatically implemented when inheriting from sklearn.base.BaseEstimator Parameters: deep: If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns: Parameter names mapped to their values. \"\"\" return super ( BaseModelWrapper , self ) . get_params ( deep = deep ) predict ( self , data ) Use the learned model to make predictions. Parameters: Name Type Description Default data ndarray The input samples of shape [n_samples, n_original_features]. required Source code in photonai/modelwrapper/base_model_wrapper.py def predict ( self , data : np . ndarray ): \"\"\" Use the learned model to make predictions. Parameters: data: The input samples of shape [n_samples, n_original_features]. \"\"\" set_params ( self , ** kwargs ) Takes the given dictionary, with the keys being the variable name, and sets the object's parameters to the given values. Automatically implemented when inheriting from sklearn.base.BaseEstimator. Parameters: Name Type Description Default **kwargs Estimator parameters. {} Source code in photonai/modelwrapper/base_model_wrapper.py def set_params ( self , ** kwargs ): \"\"\" Takes the given dictionary, with the keys being the variable name, and sets the object's parameters to the given values. Automatically implemented when inheriting from sklearn.base.BaseEstimator. Parameters: **kwargs: Estimator parameters. \"\"\" super ( BaseModelWrapper , self ) . set_params ( ** kwargs ) transform ( self , data , targets = None ) Apply the method's logic to the data. Parameters: Name Type Description Default data ndarray The input samples of shape [n_samples, n_original_features]. required targets ndarray The input targets of shape [n_samples, 1]. Not necessary. None Source code in photonai/modelwrapper/base_model_wrapper.py def transform ( self , data : np . ndarray , targets : np . ndarray = None ): \"\"\" Apply the method's logic to the data. Parameters: data: The input samples of shape [n_samples, n_original_features]. targets: The input targets of shape [n_samples, 1]. Not necessary. \"\"\"","title":"BaseModelWrapper"},{"location":"api/modelwrapper/base_model_wrapper/#documentation-for-basemodelwrapper","text":"The PHOTONAI interface for implementing custom pipeline elements. PHOTONAI works on top of the scikit-learn object API, see documentation . Your class should overwrite the following definitions: fit(data) : learn or adjust to the data. If it is an estimator, which means it has the ability to learn, it should implement predict(data) : using the learned model to generate prediction, should inherit sklearn.base.BaseEstimator ( see here ), inherits get_params and set_params . If it is an transformer, which means it preprocesses or prepares the data, it should implement transform(data) : applying the logic to the data to transform it, should inherit from sklearn.base.TransformerMixin ( see here ), inherits fit_transform as a concatenation of both fit and transform, should inherit sklearn.base.BaseEstimator ( see here ) inherits get_params and set_params . Prepare for hyperparameter optimization PHOTONAI expects a definition for all parameters you want to optimize in the hyperparameter search in the constructor stub , and to be addressable with the same name as class variable . In this way you can define any parameter and it is automatically prepared for the hyperparameter search process. See the scikit-learn object API documentation for more in depth information about the interface. Source code in photonai/modelwrapper/base_model_wrapper.py class BaseModelWrapper ( BaseEstimator ): \"\"\" The PHOTONAI interface for implementing custom pipeline elements. PHOTONAI works on top of the scikit-learn object API, [see documentation](http://scikit-learn.org/stable/developers/contributing.html#apis-of-scikit-learn-objects). Your class should overwrite the following definitions: - `fit(data)`: learn or adjust to the data. If it is an estimator, which means it has the ability to learn, - it should implement `predict(data)`: using the learned model to generate prediction, - should inherit *sklearn.base.BaseEstimator* ([see here]( http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html)), - inherits *get_params* and *set_params*. If it is an transformer, which means it preprocesses or prepares the data, - it should implement `transform(data)`: applying the logic to the data to transform it, - should inherit from *sklearn.base.TransformerMixin* ([see here]( http://scikit-learn.org/stable/modules/generated/sklearn.base.TransformerMixin.html)), - inherits *fit_transform* as a concatenation of both fit and transform, - should inherit *sklearn.base.BaseEstimator* ([see here]( http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html)) - inherits *get_params* and *set_params*. `Prepare for hyperparameter optimization` PHOTONAI expects a `definition for all parameters` you want to optimize in the hyperparameter search in the `constructor stub`, and to be addressable with the `same name as class variable`. In this way you can define any parameter and it is automatically prepared for the hyperparameter search process. See the [scikit-learn object API documentation]( http://scikit-learn.org/stable/developers/contributing.html#apis-of-scikit-learn-objects) for more in depth information about the interface. \"\"\" def __init__ ( self ): pass def fit ( self , data : np . ndarray , targets : np . ndarray = None ): \"\"\" Adjust the underlying model or method to the data. Parameters: data: The input samples of shape [n_samples, n_original_features]. targets: The input targets of shape [n_samples, 1]. Returns: IMPORTANT, must return self! \"\"\" def predict ( self , data : np . ndarray ): \"\"\" Use the learned model to make predictions. Parameters: data: The input samples of shape [n_samples, n_original_features]. \"\"\" def transform ( self , data : np . ndarray , targets : np . ndarray = None ): \"\"\" Apply the method's logic to the data. Parameters: data: The input samples of shape [n_samples, n_original_features]. targets: The input targets of shape [n_samples, 1]. Not necessary. \"\"\" def get_params ( self , deep : bool = True ) -> dict : \"\"\" Get the models parameters. Automatically implemented when inheriting from sklearn.base.BaseEstimator Parameters: deep: If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns: Parameter names mapped to their values. \"\"\" return super ( BaseModelWrapper , self ) . get_params ( deep = deep ) def set_params ( self , ** kwargs ): \"\"\" Takes the given dictionary, with the keys being the variable name, and sets the object's parameters to the given values. Automatically implemented when inheriting from sklearn.base.BaseEstimator. Parameters: **kwargs: Estimator parameters. \"\"\" super ( BaseModelWrapper , self ) . set_params ( ** kwargs )","title":"Documentation for BaseModelWrapper"},{"location":"api/modelwrapper/base_model_wrapper/#photonai.modelwrapper.base_model_wrapper.BaseModelWrapper.fit","text":"Adjust the underlying model or method to the data. Parameters: Name Type Description Default data ndarray The input samples of shape [n_samples, n_original_features]. required targets ndarray The input targets of shape [n_samples, 1]. None Returns: Type Description IMPORTANT, must return self! Source code in photonai/modelwrapper/base_model_wrapper.py def fit ( self , data : np . ndarray , targets : np . ndarray = None ): \"\"\" Adjust the underlying model or method to the data. Parameters: data: The input samples of shape [n_samples, n_original_features]. targets: The input targets of shape [n_samples, 1]. Returns: IMPORTANT, must return self! \"\"\"","title":"fit()"},{"location":"api/modelwrapper/base_model_wrapper/#photonai.modelwrapper.base_model_wrapper.BaseModelWrapper.get_params","text":"Get the models parameters. Automatically implemented when inheriting from sklearn.base.BaseEstimator Parameters: Name Type Description Default deep bool If True, will return the parameters for this estimator and contained subobjects that are estimators. True Returns: Type Description dict Parameter names mapped to their values. Source code in photonai/modelwrapper/base_model_wrapper.py def get_params ( self , deep : bool = True ) -> dict : \"\"\" Get the models parameters. Automatically implemented when inheriting from sklearn.base.BaseEstimator Parameters: deep: If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns: Parameter names mapped to their values. \"\"\" return super ( BaseModelWrapper , self ) . get_params ( deep = deep )","title":"get_params()"},{"location":"api/modelwrapper/base_model_wrapper/#photonai.modelwrapper.base_model_wrapper.BaseModelWrapper.predict","text":"Use the learned model to make predictions. Parameters: Name Type Description Default data ndarray The input samples of shape [n_samples, n_original_features]. required Source code in photonai/modelwrapper/base_model_wrapper.py def predict ( self , data : np . ndarray ): \"\"\" Use the learned model to make predictions. Parameters: data: The input samples of shape [n_samples, n_original_features]. \"\"\"","title":"predict()"},{"location":"api/modelwrapper/base_model_wrapper/#photonai.modelwrapper.base_model_wrapper.BaseModelWrapper.set_params","text":"Takes the given dictionary, with the keys being the variable name, and sets the object's parameters to the given values. Automatically implemented when inheriting from sklearn.base.BaseEstimator. Parameters: Name Type Description Default **kwargs Estimator parameters. {} Source code in photonai/modelwrapper/base_model_wrapper.py def set_params ( self , ** kwargs ): \"\"\" Takes the given dictionary, with the keys being the variable name, and sets the object's parameters to the given values. Automatically implemented when inheriting from sklearn.base.BaseEstimator. Parameters: **kwargs: Estimator parameters. \"\"\" super ( BaseModelWrapper , self ) . set_params ( ** kwargs )","title":"set_params()"},{"location":"api/modelwrapper/base_model_wrapper/#photonai.modelwrapper.base_model_wrapper.BaseModelWrapper.transform","text":"Apply the method's logic to the data. Parameters: Name Type Description Default data ndarray The input samples of shape [n_samples, n_original_features]. required targets ndarray The input targets of shape [n_samples, 1]. Not necessary. None Source code in photonai/modelwrapper/base_model_wrapper.py def transform ( self , data : np . ndarray , targets : np . ndarray = None ): \"\"\" Apply the method's logic to the data. Parameters: data: The input samples of shape [n_samples, n_original_features]. targets: The input targets of shape [n_samples, 1]. Not necessary. \"\"\"","title":"transform()"},{"location":"api/modelwrapper/imblearn/","text":"Documentation for ImbalancedDataTransformer Applies the chosen strategy to the data in order to balance the input data. Instantiates the strategy filter object according to the name given as string literal. Underlying architecture: Imbalanced-Learning. More information on their documentation . Examples: 1 2 3 4 5 6 7 from photonai.optimization import Categorical tested_methods = Categorical ([ 'RandomOverSampler' , 'SMOTEENN' , 'SVMSMOTE' , 'BorderlineSMOTE' , 'SMOTE' , 'ClusterCentroids' ]) PipelineElement ( 'ImbalancedDataTransformer' , hyperparameters = { 'method_name' : tested_methods }, test_disabled = True ) Source code in photonai/modelwrapper/imbalanced_data_transformer.py class ImbalancedDataTransformer ( BaseEstimator , TransformerMixin ): \"\"\" Applies the chosen strategy to the data in order to balance the input data. Instantiates the strategy filter object according to the name given as string literal. Underlying architecture: Imbalanced-Learning. More information on their [documentation](https://imbalanced-learn.org/stable/). Example: ``` python from photonai.optimization import Categorical tested_methods = Categorical(['RandomOverSampler', 'SMOTEENN', 'SVMSMOTE', 'BorderlineSMOTE', 'SMOTE', 'ClusterCentroids']) PipelineElement('ImbalancedDataTransformer', hyperparameters={'method_name': tested_methods}, test_disabled=True) ``` \"\"\" IMBALANCED_DICT = { 'oversampling' : [ \"ADASYN\" , \"BorderlineSMOTE\" , \"KMeansSMOTE\" , \"RandomOverSampler\" , \"SMOTE\" , \"SMOTENC\" , \"SVMSMOTE\" ], 'undersampling' : [ \"AllKNN\" , \"ClusterCentroids\" , \"CondensedNearestNeighbour\" , \"EditedNearestNeighbours\" , \"InstanceHardnessThreshold\" , \"NearMiss\" , \"NeighbourhoodCleaningRule\" , \"OneSidedSelection\" , \"TomekLinks\" , \"RandomUnderSampler\" , \"RepeatedEditedNearestNeighbours\" ], 'combine' : [ \"SMOTEENN\" , \"SMOTETomek\" ], } def __init__ ( self , method_name : str = 'RandomUnderSampler' , config : dict = None ): \"\"\" Instantiates an object that transforms the data into balanced groups according to the given method. Parameters: method_name: Imbalanced learning strategy. Possible values with - an oversampling strategy are: - ADASYN, - BorderlineSMOTE, - KMeansSMOTE, - RandomOverSampler, - SMOTE, - SMOTENC, - SVMSMOTE, - an undersampling strategy are: - ClusterCentroids, - RandomUnderSampler, - NearMiss, - InstanceHardnessThreshold, - CondensedNearestNeighbour, - EditedNearestNeighbours, - RepeatedEditedNearestNeighbours, - AllKNN, - NeighbourhoodCleaningRule, - OneSidedSelection, - a combined strategy are: - SMOTEENN, - SMOTETomek. config: Each strategy has a set of presets. This parameter is necessary to select the appropriate settings for the selected method. It is important that the key exactly matches the method_name. If no key is found for a method, it will be started with the default settings. Please do not use this parameter inside the 'hyperparmeters' to optimize it. \"\"\" if not __found__ : raise ModuleNotFoundError ( \"Module imblearn not found or not installed as expected. \" \"Please install the requirements.txt in PHOTON main folder.\" ) self . config = config self . _method_name = None self . method_name = method_name self . needs_y = True @property def method_name ( self ): return self . _method_name @method_name . setter def method_name ( self , value ): imbalance_type = '' for group , possible_strategies in ImbalancedDataTransformer . IMBALANCED_DICT . items (): if value in possible_strategies : imbalance_type = group if imbalance_type == \"oversampling\" : home = over_sampling elif imbalance_type == \"undersampling\" : home = under_sampling elif imbalance_type == \"combine\" or imbalance_type == \"combination\" : home = combine else : msg = \"Imbalance Type not found. Can be oversampling, undersampling or combine. \" \\ \"Oversampling: method_name one of {} . Undersampling: method_name one of {} .\" \\ \"Combine: method_name one of {} .\" . format ( str ( self . IMBALANCED_DICT [ \"oversampling\" ]), str ( self . IMBALANCED_DICT [ \"undersampling\" ]), str ( self . IMBALANCED_DICT [ \"combine\" ])) logger . error ( msg ) raise ValueError ( msg ) desired_class = getattr ( home , value ) self . _method_name = value if self . config is not None and value in self . config : if not isinstance ( self . config [ value ], dict ): msg = \"Please use for the imbalanced config a format like: \" \\ \"config={'SMOTE': {'sampling_strategy': {0: 9, 1: 12}}}.\" logger . error ( msg ) raise ValueError ( msg ) self . method = desired_class ( ** self . config [ value ]) else : self . method = desired_class () def fit_transform ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ) -> ( np . ndarray , np . ndarray ): \"\"\" Call of the underlying imblearn.fit_resample(X, y). Parameters: X: The input samples of shape [n_samples, n_features]. y: The input targets of shape [n_samples, 1]. **kwargs: Ignored input. Returns: Transformed data. \"\"\" return self . method . fit_resample ( X , y ) # define an alias for imblearn consistency fit_sample = fit_transform fit_resample = fit_transform def fit ( self , X , y , ** kwargs ): \"\"\"Empty method required in PHOTONAI.\"\"\" return def transform ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ) -> ( np . ndarray , np . ndarray ): \"\"\" Forwarding to the self.fit_transform method. Parameters: X: The input samples of shape [n_samples, n_features]. y: The input targets of shape [n_samples, 1]. **kwargs: Ignored input. Returns: Transformed data. \"\"\" return self . fit_transform ( X , y ) __init__ ( self , method_name = 'RandomUnderSampler' , config = None ) special Instantiates an object that transforms the data into balanced groups according to the given method. Parameters: Name Type Description Default method_name str Imbalanced learning strategy. Possible values with an oversampling strategy are: ADASYN, BorderlineSMOTE, KMeansSMOTE, RandomOverSampler, SMOTE, SMOTENC, SVMSMOTE, an undersampling strategy are: ClusterCentroids, RandomUnderSampler, NearMiss, InstanceHardnessThreshold, CondensedNearestNeighbour, EditedNearestNeighbours, RepeatedEditedNearestNeighbours, AllKNN, NeighbourhoodCleaningRule, OneSidedSelection, a combined strategy are: SMOTEENN, SMOTETomek. 'RandomUnderSampler' config dict Each strategy has a set of presets. This parameter is necessary to select the appropriate settings for the selected method. It is important that the key exactly matches the method_name. If no key is found for a method, it will be started with the default settings. Please do not use this parameter inside the 'hyperparmeters' to optimize it. None Source code in photonai/modelwrapper/imbalanced_data_transformer.py def __init__ ( self , method_name : str = 'RandomUnderSampler' , config : dict = None ): \"\"\" Instantiates an object that transforms the data into balanced groups according to the given method. Parameters: method_name: Imbalanced learning strategy. Possible values with - an oversampling strategy are: - ADASYN, - BorderlineSMOTE, - KMeansSMOTE, - RandomOverSampler, - SMOTE, - SMOTENC, - SVMSMOTE, - an undersampling strategy are: - ClusterCentroids, - RandomUnderSampler, - NearMiss, - InstanceHardnessThreshold, - CondensedNearestNeighbour, - EditedNearestNeighbours, - RepeatedEditedNearestNeighbours, - AllKNN, - NeighbourhoodCleaningRule, - OneSidedSelection, - a combined strategy are: - SMOTEENN, - SMOTETomek. config: Each strategy has a set of presets. This parameter is necessary to select the appropriate settings for the selected method. It is important that the key exactly matches the method_name. If no key is found for a method, it will be started with the default settings. Please do not use this parameter inside the 'hyperparmeters' to optimize it. \"\"\" if not __found__ : raise ModuleNotFoundError ( \"Module imblearn not found or not installed as expected. \" \"Please install the requirements.txt in PHOTON main folder.\" ) self . config = config self . _method_name = None self . method_name = method_name self . needs_y = True fit_transform ( self , X , y = None , ** kwargs ) Call of the underlying imblearn.fit_resample(X, y). Parameters: Name Type Description Default X ndarray The input samples of shape [n_samples, n_features]. required y ndarray The input targets of shape [n_samples, 1]. None **kwargs Ignored input. {} Returns: Type Description (<class 'numpy.ndarray'>, <class 'numpy.ndarray'>) Transformed data. Source code in photonai/modelwrapper/imbalanced_data_transformer.py def fit_transform ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ) -> ( np . ndarray , np . ndarray ): \"\"\" Call of the underlying imblearn.fit_resample(X, y). Parameters: X: The input samples of shape [n_samples, n_features]. y: The input targets of shape [n_samples, 1]. **kwargs: Ignored input. Returns: Transformed data. \"\"\" return self . method . fit_resample ( X , y ) transform ( self , X , y = None , ** kwargs ) Forwarding to the self.fit_transform method. Parameters: Name Type Description Default X ndarray The input samples of shape [n_samples, n_features]. required y ndarray The input targets of shape [n_samples, 1]. None **kwargs Ignored input. {} Returns: Type Description (<class 'numpy.ndarray'>, <class 'numpy.ndarray'>) Transformed data. Source code in photonai/modelwrapper/imbalanced_data_transformer.py def transform ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ) -> ( np . ndarray , np . ndarray ): \"\"\" Forwarding to the self.fit_transform method. Parameters: X: The input samples of shape [n_samples, n_features]. y: The input targets of shape [n_samples, 1]. **kwargs: Ignored input. Returns: Transformed data. \"\"\" return self . fit_transform ( X , y )","title":"ImbalancedDataTransformer"},{"location":"api/modelwrapper/imblearn/#documentation-for-imbalanceddatatransformer","text":"Applies the chosen strategy to the data in order to balance the input data. Instantiates the strategy filter object according to the name given as string literal. Underlying architecture: Imbalanced-Learning. More information on their documentation . Examples: 1 2 3 4 5 6 7 from photonai.optimization import Categorical tested_methods = Categorical ([ 'RandomOverSampler' , 'SMOTEENN' , 'SVMSMOTE' , 'BorderlineSMOTE' , 'SMOTE' , 'ClusterCentroids' ]) PipelineElement ( 'ImbalancedDataTransformer' , hyperparameters = { 'method_name' : tested_methods }, test_disabled = True ) Source code in photonai/modelwrapper/imbalanced_data_transformer.py class ImbalancedDataTransformer ( BaseEstimator , TransformerMixin ): \"\"\" Applies the chosen strategy to the data in order to balance the input data. Instantiates the strategy filter object according to the name given as string literal. Underlying architecture: Imbalanced-Learning. More information on their [documentation](https://imbalanced-learn.org/stable/). Example: ``` python from photonai.optimization import Categorical tested_methods = Categorical(['RandomOverSampler', 'SMOTEENN', 'SVMSMOTE', 'BorderlineSMOTE', 'SMOTE', 'ClusterCentroids']) PipelineElement('ImbalancedDataTransformer', hyperparameters={'method_name': tested_methods}, test_disabled=True) ``` \"\"\" IMBALANCED_DICT = { 'oversampling' : [ \"ADASYN\" , \"BorderlineSMOTE\" , \"KMeansSMOTE\" , \"RandomOverSampler\" , \"SMOTE\" , \"SMOTENC\" , \"SVMSMOTE\" ], 'undersampling' : [ \"AllKNN\" , \"ClusterCentroids\" , \"CondensedNearestNeighbour\" , \"EditedNearestNeighbours\" , \"InstanceHardnessThreshold\" , \"NearMiss\" , \"NeighbourhoodCleaningRule\" , \"OneSidedSelection\" , \"TomekLinks\" , \"RandomUnderSampler\" , \"RepeatedEditedNearestNeighbours\" ], 'combine' : [ \"SMOTEENN\" , \"SMOTETomek\" ], } def __init__ ( self , method_name : str = 'RandomUnderSampler' , config : dict = None ): \"\"\" Instantiates an object that transforms the data into balanced groups according to the given method. Parameters: method_name: Imbalanced learning strategy. Possible values with - an oversampling strategy are: - ADASYN, - BorderlineSMOTE, - KMeansSMOTE, - RandomOverSampler, - SMOTE, - SMOTENC, - SVMSMOTE, - an undersampling strategy are: - ClusterCentroids, - RandomUnderSampler, - NearMiss, - InstanceHardnessThreshold, - CondensedNearestNeighbour, - EditedNearestNeighbours, - RepeatedEditedNearestNeighbours, - AllKNN, - NeighbourhoodCleaningRule, - OneSidedSelection, - a combined strategy are: - SMOTEENN, - SMOTETomek. config: Each strategy has a set of presets. This parameter is necessary to select the appropriate settings for the selected method. It is important that the key exactly matches the method_name. If no key is found for a method, it will be started with the default settings. Please do not use this parameter inside the 'hyperparmeters' to optimize it. \"\"\" if not __found__ : raise ModuleNotFoundError ( \"Module imblearn not found or not installed as expected. \" \"Please install the requirements.txt in PHOTON main folder.\" ) self . config = config self . _method_name = None self . method_name = method_name self . needs_y = True @property def method_name ( self ): return self . _method_name @method_name . setter def method_name ( self , value ): imbalance_type = '' for group , possible_strategies in ImbalancedDataTransformer . IMBALANCED_DICT . items (): if value in possible_strategies : imbalance_type = group if imbalance_type == \"oversampling\" : home = over_sampling elif imbalance_type == \"undersampling\" : home = under_sampling elif imbalance_type == \"combine\" or imbalance_type == \"combination\" : home = combine else : msg = \"Imbalance Type not found. Can be oversampling, undersampling or combine. \" \\ \"Oversampling: method_name one of {} . Undersampling: method_name one of {} .\" \\ \"Combine: method_name one of {} .\" . format ( str ( self . IMBALANCED_DICT [ \"oversampling\" ]), str ( self . IMBALANCED_DICT [ \"undersampling\" ]), str ( self . IMBALANCED_DICT [ \"combine\" ])) logger . error ( msg ) raise ValueError ( msg ) desired_class = getattr ( home , value ) self . _method_name = value if self . config is not None and value in self . config : if not isinstance ( self . config [ value ], dict ): msg = \"Please use for the imbalanced config a format like: \" \\ \"config={'SMOTE': {'sampling_strategy': {0: 9, 1: 12}}}.\" logger . error ( msg ) raise ValueError ( msg ) self . method = desired_class ( ** self . config [ value ]) else : self . method = desired_class () def fit_transform ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ) -> ( np . ndarray , np . ndarray ): \"\"\" Call of the underlying imblearn.fit_resample(X, y). Parameters: X: The input samples of shape [n_samples, n_features]. y: The input targets of shape [n_samples, 1]. **kwargs: Ignored input. Returns: Transformed data. \"\"\" return self . method . fit_resample ( X , y ) # define an alias for imblearn consistency fit_sample = fit_transform fit_resample = fit_transform def fit ( self , X , y , ** kwargs ): \"\"\"Empty method required in PHOTONAI.\"\"\" return def transform ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ) -> ( np . ndarray , np . ndarray ): \"\"\" Forwarding to the self.fit_transform method. Parameters: X: The input samples of shape [n_samples, n_features]. y: The input targets of shape [n_samples, 1]. **kwargs: Ignored input. Returns: Transformed data. \"\"\" return self . fit_transform ( X , y )","title":"Documentation for ImbalancedDataTransformer"},{"location":"api/modelwrapper/imblearn/#photonai.modelwrapper.imbalanced_data_transformer.ImbalancedDataTransformer.__init__","text":"Instantiates an object that transforms the data into balanced groups according to the given method. Parameters: Name Type Description Default method_name str Imbalanced learning strategy. Possible values with an oversampling strategy are: ADASYN, BorderlineSMOTE, KMeansSMOTE, RandomOverSampler, SMOTE, SMOTENC, SVMSMOTE, an undersampling strategy are: ClusterCentroids, RandomUnderSampler, NearMiss, InstanceHardnessThreshold, CondensedNearestNeighbour, EditedNearestNeighbours, RepeatedEditedNearestNeighbours, AllKNN, NeighbourhoodCleaningRule, OneSidedSelection, a combined strategy are: SMOTEENN, SMOTETomek. 'RandomUnderSampler' config dict Each strategy has a set of presets. This parameter is necessary to select the appropriate settings for the selected method. It is important that the key exactly matches the method_name. If no key is found for a method, it will be started with the default settings. Please do not use this parameter inside the 'hyperparmeters' to optimize it. None Source code in photonai/modelwrapper/imbalanced_data_transformer.py def __init__ ( self , method_name : str = 'RandomUnderSampler' , config : dict = None ): \"\"\" Instantiates an object that transforms the data into balanced groups according to the given method. Parameters: method_name: Imbalanced learning strategy. Possible values with - an oversampling strategy are: - ADASYN, - BorderlineSMOTE, - KMeansSMOTE, - RandomOverSampler, - SMOTE, - SMOTENC, - SVMSMOTE, - an undersampling strategy are: - ClusterCentroids, - RandomUnderSampler, - NearMiss, - InstanceHardnessThreshold, - CondensedNearestNeighbour, - EditedNearestNeighbours, - RepeatedEditedNearestNeighbours, - AllKNN, - NeighbourhoodCleaningRule, - OneSidedSelection, - a combined strategy are: - SMOTEENN, - SMOTETomek. config: Each strategy has a set of presets. This parameter is necessary to select the appropriate settings for the selected method. It is important that the key exactly matches the method_name. If no key is found for a method, it will be started with the default settings. Please do not use this parameter inside the 'hyperparmeters' to optimize it. \"\"\" if not __found__ : raise ModuleNotFoundError ( \"Module imblearn not found or not installed as expected. \" \"Please install the requirements.txt in PHOTON main folder.\" ) self . config = config self . _method_name = None self . method_name = method_name self . needs_y = True","title":"__init__()"},{"location":"api/modelwrapper/imblearn/#photonai.modelwrapper.imbalanced_data_transformer.ImbalancedDataTransformer.fit_transform","text":"Call of the underlying imblearn.fit_resample(X, y). Parameters: Name Type Description Default X ndarray The input samples of shape [n_samples, n_features]. required y ndarray The input targets of shape [n_samples, 1]. None **kwargs Ignored input. {} Returns: Type Description (<class 'numpy.ndarray'>, <class 'numpy.ndarray'>) Transformed data. Source code in photonai/modelwrapper/imbalanced_data_transformer.py def fit_transform ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ) -> ( np . ndarray , np . ndarray ): \"\"\" Call of the underlying imblearn.fit_resample(X, y). Parameters: X: The input samples of shape [n_samples, n_features]. y: The input targets of shape [n_samples, 1]. **kwargs: Ignored input. Returns: Transformed data. \"\"\" return self . method . fit_resample ( X , y )","title":"fit_transform()"},{"location":"api/modelwrapper/imblearn/#photonai.modelwrapper.imbalanced_data_transformer.ImbalancedDataTransformer.transform","text":"Forwarding to the self.fit_transform method. Parameters: Name Type Description Default X ndarray The input samples of shape [n_samples, n_features]. required y ndarray The input targets of shape [n_samples, 1]. None **kwargs Ignored input. {} Returns: Type Description (<class 'numpy.ndarray'>, <class 'numpy.ndarray'>) Transformed data. Source code in photonai/modelwrapper/imbalanced_data_transformer.py def transform ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ) -> ( np . ndarray , np . ndarray ): \"\"\" Forwarding to the self.fit_transform method. Parameters: X: The input samples of shape [n_samples, n_features]. y: The input targets of shape [n_samples, 1]. **kwargs: Ignored input. Returns: Transformed data. \"\"\" return self . fit_transform ( X , y )","title":"transform()"},{"location":"api/modelwrapper/label_encoder/","text":"Documentation for LabelEncoder Suitable version of the scikit-learn LabelEncoder for PHOTONAI. Since the pipeline process streams the underlying samples to every transformer, this class is required. Source code in photonai/modelwrapper/label_encoder.py class LabelEncoder ( SKLabelEncoder ): \"\"\" Suitable version of the scikit-learn LabelEncoder for PHOTONAI. Since the pipeline process streams the underlying samples to every transformer, this class is required. \"\"\" def __init__ ( self ): \"\"\"Initialize the object.\"\"\" super ( LabelEncoder , self ) . __init__ () self . needs_y = True def fit ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ): \"\"\" Call of the underlying sklearn.fit(y) method. Parameters: X: The input samples of shape [n_samples, n_features]. y: The input targets of shape [n_samples, 1]. **kwargs: Ignored input. \"\"\" super ( LabelEncoder , self ) . fit ( y ) return self def transform ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ) -> ( np . ndarray , np . ndarray ): \"\"\" Call of the underlying sklearn.transform(y) method. Parameters: X: The input samples of shape [n_samples, n_features]. y: The input targets of shape [n_samples, 1]. **kwargs: Ignored input. Returns: Original X and encoded y. \"\"\" yt = super ( LabelEncoder , self ) . transform ( y ) return X , yt def fit_transform ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ) -> ( np . ndarray , np . ndarray ): \"\"\" Call of the underlying sklearn.fit_transform(y) method. Parameters: X: The input samples of shape [n_samples, n_features]. y: The input targets of shape [n_samples, 1]. **kwargs: Ignored input. Returns: Original X and encoded y. \"\"\" return super ( LabelEncoder , self ) . fit_transform ( y ) __init__ ( self ) special Initialize the object. Source code in photonai/modelwrapper/label_encoder.py def __init__ ( self ): \"\"\"Initialize the object.\"\"\" super ( LabelEncoder , self ) . __init__ () self . needs_y = True fit ( self , X , y = None , ** kwargs ) Call of the underlying sklearn.fit(y) method. Parameters: Name Type Description Default X ndarray The input samples of shape [n_samples, n_features]. required y ndarray The input targets of shape [n_samples, 1]. None **kwargs Ignored input. {} Source code in photonai/modelwrapper/label_encoder.py def fit ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ): \"\"\" Call of the underlying sklearn.fit(y) method. Parameters: X: The input samples of shape [n_samples, n_features]. y: The input targets of shape [n_samples, 1]. **kwargs: Ignored input. \"\"\" super ( LabelEncoder , self ) . fit ( y ) return self fit_transform ( self , X , y = None , ** kwargs ) Call of the underlying sklearn.fit_transform(y) method. Parameters: Name Type Description Default X ndarray The input samples of shape [n_samples, n_features]. required y ndarray The input targets of shape [n_samples, 1]. None **kwargs Ignored input. {} Returns: Type Description (<class 'numpy.ndarray'>, <class 'numpy.ndarray'>) Original X and encoded y. Source code in photonai/modelwrapper/label_encoder.py def fit_transform ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ) -> ( np . ndarray , np . ndarray ): \"\"\" Call of the underlying sklearn.fit_transform(y) method. Parameters: X: The input samples of shape [n_samples, n_features]. y: The input targets of shape [n_samples, 1]. **kwargs: Ignored input. Returns: Original X and encoded y. \"\"\" return super ( LabelEncoder , self ) . fit_transform ( y ) transform ( self , X , y = None , ** kwargs ) Call of the underlying sklearn.transform(y) method. Parameters: Name Type Description Default X ndarray The input samples of shape [n_samples, n_features]. required y ndarray The input targets of shape [n_samples, 1]. None **kwargs Ignored input. {} Returns: Type Description (<class 'numpy.ndarray'>, <class 'numpy.ndarray'>) Original X and encoded y. Source code in photonai/modelwrapper/label_encoder.py def transform ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ) -> ( np . ndarray , np . ndarray ): \"\"\" Call of the underlying sklearn.transform(y) method. Parameters: X: The input samples of shape [n_samples, n_features]. y: The input targets of shape [n_samples, 1]. **kwargs: Ignored input. Returns: Original X and encoded y. \"\"\" yt = super ( LabelEncoder , self ) . transform ( y ) return X , yt","title":"LabelEncoder"},{"location":"api/modelwrapper/label_encoder/#documentation-for-labelencoder","text":"Suitable version of the scikit-learn LabelEncoder for PHOTONAI. Since the pipeline process streams the underlying samples to every transformer, this class is required. Source code in photonai/modelwrapper/label_encoder.py class LabelEncoder ( SKLabelEncoder ): \"\"\" Suitable version of the scikit-learn LabelEncoder for PHOTONAI. Since the pipeline process streams the underlying samples to every transformer, this class is required. \"\"\" def __init__ ( self ): \"\"\"Initialize the object.\"\"\" super ( LabelEncoder , self ) . __init__ () self . needs_y = True def fit ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ): \"\"\" Call of the underlying sklearn.fit(y) method. Parameters: X: The input samples of shape [n_samples, n_features]. y: The input targets of shape [n_samples, 1]. **kwargs: Ignored input. \"\"\" super ( LabelEncoder , self ) . fit ( y ) return self def transform ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ) -> ( np . ndarray , np . ndarray ): \"\"\" Call of the underlying sklearn.transform(y) method. Parameters: X: The input samples of shape [n_samples, n_features]. y: The input targets of shape [n_samples, 1]. **kwargs: Ignored input. Returns: Original X and encoded y. \"\"\" yt = super ( LabelEncoder , self ) . transform ( y ) return X , yt def fit_transform ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ) -> ( np . ndarray , np . ndarray ): \"\"\" Call of the underlying sklearn.fit_transform(y) method. Parameters: X: The input samples of shape [n_samples, n_features]. y: The input targets of shape [n_samples, 1]. **kwargs: Ignored input. Returns: Original X and encoded y. \"\"\" return super ( LabelEncoder , self ) . fit_transform ( y )","title":"Documentation for LabelEncoder"},{"location":"api/modelwrapper/label_encoder/#photonai.modelwrapper.label_encoder.LabelEncoder.__init__","text":"Initialize the object. Source code in photonai/modelwrapper/label_encoder.py def __init__ ( self ): \"\"\"Initialize the object.\"\"\" super ( LabelEncoder , self ) . __init__ () self . needs_y = True","title":"__init__()"},{"location":"api/modelwrapper/label_encoder/#photonai.modelwrapper.label_encoder.LabelEncoder.fit","text":"Call of the underlying sklearn.fit(y) method. Parameters: Name Type Description Default X ndarray The input samples of shape [n_samples, n_features]. required y ndarray The input targets of shape [n_samples, 1]. None **kwargs Ignored input. {} Source code in photonai/modelwrapper/label_encoder.py def fit ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ): \"\"\" Call of the underlying sklearn.fit(y) method. Parameters: X: The input samples of shape [n_samples, n_features]. y: The input targets of shape [n_samples, 1]. **kwargs: Ignored input. \"\"\" super ( LabelEncoder , self ) . fit ( y ) return self","title":"fit()"},{"location":"api/modelwrapper/label_encoder/#photonai.modelwrapper.label_encoder.LabelEncoder.fit_transform","text":"Call of the underlying sklearn.fit_transform(y) method. Parameters: Name Type Description Default X ndarray The input samples of shape [n_samples, n_features]. required y ndarray The input targets of shape [n_samples, 1]. None **kwargs Ignored input. {} Returns: Type Description (<class 'numpy.ndarray'>, <class 'numpy.ndarray'>) Original X and encoded y. Source code in photonai/modelwrapper/label_encoder.py def fit_transform ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ) -> ( np . ndarray , np . ndarray ): \"\"\" Call of the underlying sklearn.fit_transform(y) method. Parameters: X: The input samples of shape [n_samples, n_features]. y: The input targets of shape [n_samples, 1]. **kwargs: Ignored input. Returns: Original X and encoded y. \"\"\" return super ( LabelEncoder , self ) . fit_transform ( y )","title":"fit_transform()"},{"location":"api/modelwrapper/label_encoder/#photonai.modelwrapper.label_encoder.LabelEncoder.transform","text":"Call of the underlying sklearn.transform(y) method. Parameters: Name Type Description Default X ndarray The input samples of shape [n_samples, n_features]. required y ndarray The input targets of shape [n_samples, 1]. None **kwargs Ignored input. {} Returns: Type Description (<class 'numpy.ndarray'>, <class 'numpy.ndarray'>) Original X and encoded y. Source code in photonai/modelwrapper/label_encoder.py def transform ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ) -> ( np . ndarray , np . ndarray ): \"\"\" Call of the underlying sklearn.transform(y) method. Parameters: X: The input samples of shape [n_samples, n_features]. y: The input targets of shape [n_samples, 1]. **kwargs: Ignored input. Returns: Original X and encoded y. \"\"\" yt = super ( LabelEncoder , self ) . transform ( y ) return X , yt","title":"transform()"},{"location":"api/modelwrapper/feature_selection/FClassifSelectPercentile/","text":"Documentation for FClassifSelectPercentile Feature Selection for classification data - percentile based. Apply VarianceThreshold -> SelectPercentile to data. SelectPercentile based on f_classif and parameter percentile. Source code in photonai/modelwrapper/feature_selection.py class FClassifSelectPercentile ( BaseEstimator , TransformerMixin ): \"\"\"Feature Selection for classification data - percentile based. Apply VarianceThreshold -> SelectPercentile to data. SelectPercentile based on f_classif and parameter percentile. \"\"\" _estimator_type = \"transformer\" def __init__ ( self , percentile : float = 10 ): \"\"\" Initialize the object. Parameters: percentile: Percent of features to keep. \"\"\" self . var_thres = VarianceThreshold () self . percentile = percentile self . my_fs = None def fit ( self , X , y ): X = self . var_thres . fit_transform ( X ) self . my_fs = SelectPercentile ( score_func = f_classif , percentile = self . percentile ) self . my_fs . fit ( X , y ) return self def transform ( self , X ): X = self . var_thres . transform ( X ) return self . my_fs . transform ( X ) def inverse_transform ( self , X : np . ndarray ) -> np . ndarray : \"\"\"Reverse to original dimension. 1. SelectPercentile.inverse_transform 2. VarianceThreshold.inverse_transform Parameters: X: The input samples of shape [n_samples, n_selected_features]. Returns: Array of shape [n_samples, n_original_features] with columns of zeros inserted where features would have been removed. \"\"\" Xt = self . my_fs . inverse_transform ( X ) return self . var_thres . inverse_transform ( Xt ) __init__ ( self , percentile = 10 ) special Initialize the object. Parameters: Name Type Description Default percentile float Percent of features to keep. 10 Source code in photonai/modelwrapper/feature_selection.py def __init__ ( self , percentile : float = 10 ): \"\"\" Initialize the object. Parameters: percentile: Percent of features to keep. \"\"\" self . var_thres = VarianceThreshold () self . percentile = percentile self . my_fs = None","title":"FClassifSelectPercentile"},{"location":"api/modelwrapper/feature_selection/FClassifSelectPercentile/#documentation-for-fclassifselectpercentile","text":"Feature Selection for classification data - percentile based. Apply VarianceThreshold -> SelectPercentile to data. SelectPercentile based on f_classif and parameter percentile. Source code in photonai/modelwrapper/feature_selection.py class FClassifSelectPercentile ( BaseEstimator , TransformerMixin ): \"\"\"Feature Selection for classification data - percentile based. Apply VarianceThreshold -> SelectPercentile to data. SelectPercentile based on f_classif and parameter percentile. \"\"\" _estimator_type = \"transformer\" def __init__ ( self , percentile : float = 10 ): \"\"\" Initialize the object. Parameters: percentile: Percent of features to keep. \"\"\" self . var_thres = VarianceThreshold () self . percentile = percentile self . my_fs = None def fit ( self , X , y ): X = self . var_thres . fit_transform ( X ) self . my_fs = SelectPercentile ( score_func = f_classif , percentile = self . percentile ) self . my_fs . fit ( X , y ) return self def transform ( self , X ): X = self . var_thres . transform ( X ) return self . my_fs . transform ( X ) def inverse_transform ( self , X : np . ndarray ) -> np . ndarray : \"\"\"Reverse to original dimension. 1. SelectPercentile.inverse_transform 2. VarianceThreshold.inverse_transform Parameters: X: The input samples of shape [n_samples, n_selected_features]. Returns: Array of shape [n_samples, n_original_features] with columns of zeros inserted where features would have been removed. \"\"\" Xt = self . my_fs . inverse_transform ( X ) return self . var_thres . inverse_transform ( Xt )","title":"Documentation for FClassifSelectPercentile"},{"location":"api/modelwrapper/feature_selection/FClassifSelectPercentile/#photonai.modelwrapper.feature_selection.FClassifSelectPercentile.__init__","text":"Initialize the object. Parameters: Name Type Description Default percentile float Percent of features to keep. 10 Source code in photonai/modelwrapper/feature_selection.py def __init__ ( self , percentile : float = 10 ): \"\"\" Initialize the object. Parameters: percentile: Percent of features to keep. \"\"\" self . var_thres = VarianceThreshold () self . percentile = percentile self . my_fs = None","title":"__init__()"},{"location":"api/modelwrapper/feature_selection/FRegressionFilterPValue/","text":"Documentation for FRegressionFilterPValue Feature Selection for Regression - p-value based. Fit f_regression and select all columns when p_value of column < p_threshold. Source code in photonai/modelwrapper/feature_selection.py class FRegressionFilterPValue ( BaseEstimator , TransformerMixin ): \"\"\"Feature Selection for Regression - p-value based. Fit f_regression and select all columns when p_value of column < p_threshold. \"\"\" _estimator_type = \"transformer\" def __init__ ( self , p_threshold : float = .05 ): \"\"\" Initialize the object. Parameters: p_threshold: Upper bound for p_values. \"\"\" self . p_threshold = p_threshold self . selected_indices = [] self . n_original_features = None def fit ( self , X : np . ndarray , y : np . ndarray ): \"\"\"Calculation of the important columns. Apply f_regression on input X, y to generate p_values. selected_indices = all p_value(columns) < p_threshold. Parameters: X: The input samples of shape [n_samples, n_original_features] y: The input targets of shape [n_samples, 1] \"\"\" self . n_original_features = X . shape [ 1 ] _ , p_values = f_regression ( X , y ) self . selected_indices = np . where ( p_values < self . p_threshold )[ 0 ] return self def transform ( self , X : np . ndarray ) -> np . ndarray : \"\"\"Reduced input X to selected_columns. Parameters: X The input samples of shape [n_samples, n_original_features] Returns: Column-filtered array of shape [n_samples, n_selected_features]. \"\"\" return X [:, self . selected_indices ] def inverse_transform ( self , X : np . ndarray ) -> np . ndarray : \"\"\"Reverse to original dimension. Parameters: X: The input samples of shape [n_samples, n_selected_features]. Raises: ValueError: If input X has a different shape than during fitting. Returns: Array of shape [n_samples, n_original_features] with columns of zeros inserted where features would have been removed. \"\"\" if X . shape [ 1 ] != len ( self . selected_indices ): msg = \"X has a different shape than during fitting.\" logger . error ( msg ) raise ValueError ( msg ) Xt = np . zeros (( X . shape [ 0 ], self . n_original_features )) Xt [:, self . selected_indices ] = X return Xt __init__ ( self , p_threshold = 0.05 ) special Initialize the object. Parameters: Name Type Description Default p_threshold float Upper bound for p_values. 0.05 Source code in photonai/modelwrapper/feature_selection.py def __init__ ( self , p_threshold : float = .05 ): \"\"\" Initialize the object. Parameters: p_threshold: Upper bound for p_values. \"\"\" self . p_threshold = p_threshold self . selected_indices = [] self . n_original_features = None fit ( self , X , y ) Calculation of the important columns. Apply f_regression on input X, y to generate p_values. selected_indices = all p_value(columns) < p_threshold. Parameters: Name Type Description Default X ndarray The input samples of shape [n_samples, n_original_features] required y ndarray The input targets of shape [n_samples, 1] required Source code in photonai/modelwrapper/feature_selection.py def fit ( self , X : np . ndarray , y : np . ndarray ): \"\"\"Calculation of the important columns. Apply f_regression on input X, y to generate p_values. selected_indices = all p_value(columns) < p_threshold. Parameters: X: The input samples of shape [n_samples, n_original_features] y: The input targets of shape [n_samples, 1] \"\"\" self . n_original_features = X . shape [ 1 ] _ , p_values = f_regression ( X , y ) self . selected_indices = np . where ( p_values < self . p_threshold )[ 0 ] return self inverse_transform ( self , X ) Reverse to original dimension. Parameters: Name Type Description Default X ndarray The input samples of shape [n_samples, n_selected_features]. required Exceptions: Type Description ValueError If input X has a different shape than during fitting. Returns: Type Description ndarray Array of shape [n_samples, n_original_features] with columns of zeros inserted where features would have been removed. Source code in photonai/modelwrapper/feature_selection.py def inverse_transform ( self , X : np . ndarray ) -> np . ndarray : \"\"\"Reverse to original dimension. Parameters: X: The input samples of shape [n_samples, n_selected_features]. Raises: ValueError: If input X has a different shape than during fitting. Returns: Array of shape [n_samples, n_original_features] with columns of zeros inserted where features would have been removed. \"\"\" if X . shape [ 1 ] != len ( self . selected_indices ): msg = \"X has a different shape than during fitting.\" logger . error ( msg ) raise ValueError ( msg ) Xt = np . zeros (( X . shape [ 0 ], self . n_original_features )) Xt [:, self . selected_indices ] = X return Xt transform ( self , X ) Reduced input X to selected_columns. Returns: Type Description ndarray Column-filtered array of shape [n_samples, n_selected_features]. Source code in photonai/modelwrapper/feature_selection.py def transform ( self , X : np . ndarray ) -> np . ndarray : \"\"\"Reduced input X to selected_columns. Parameters: X The input samples of shape [n_samples, n_original_features] Returns: Column-filtered array of shape [n_samples, n_selected_features]. \"\"\" return X [:, self . selected_indices ]","title":"FRegressionFilterPValue"},{"location":"api/modelwrapper/feature_selection/FRegressionFilterPValue/#documentation-for-fregressionfilterpvalue","text":"Feature Selection for Regression - p-value based. Fit f_regression and select all columns when p_value of column < p_threshold. Source code in photonai/modelwrapper/feature_selection.py class FRegressionFilterPValue ( BaseEstimator , TransformerMixin ): \"\"\"Feature Selection for Regression - p-value based. Fit f_regression and select all columns when p_value of column < p_threshold. \"\"\" _estimator_type = \"transformer\" def __init__ ( self , p_threshold : float = .05 ): \"\"\" Initialize the object. Parameters: p_threshold: Upper bound for p_values. \"\"\" self . p_threshold = p_threshold self . selected_indices = [] self . n_original_features = None def fit ( self , X : np . ndarray , y : np . ndarray ): \"\"\"Calculation of the important columns. Apply f_regression on input X, y to generate p_values. selected_indices = all p_value(columns) < p_threshold. Parameters: X: The input samples of shape [n_samples, n_original_features] y: The input targets of shape [n_samples, 1] \"\"\" self . n_original_features = X . shape [ 1 ] _ , p_values = f_regression ( X , y ) self . selected_indices = np . where ( p_values < self . p_threshold )[ 0 ] return self def transform ( self , X : np . ndarray ) -> np . ndarray : \"\"\"Reduced input X to selected_columns. Parameters: X The input samples of shape [n_samples, n_original_features] Returns: Column-filtered array of shape [n_samples, n_selected_features]. \"\"\" return X [:, self . selected_indices ] def inverse_transform ( self , X : np . ndarray ) -> np . ndarray : \"\"\"Reverse to original dimension. Parameters: X: The input samples of shape [n_samples, n_selected_features]. Raises: ValueError: If input X has a different shape than during fitting. Returns: Array of shape [n_samples, n_original_features] with columns of zeros inserted where features would have been removed. \"\"\" if X . shape [ 1 ] != len ( self . selected_indices ): msg = \"X has a different shape than during fitting.\" logger . error ( msg ) raise ValueError ( msg ) Xt = np . zeros (( X . shape [ 0 ], self . n_original_features )) Xt [:, self . selected_indices ] = X return Xt","title":"Documentation for FRegressionFilterPValue"},{"location":"api/modelwrapper/feature_selection/FRegressionFilterPValue/#photonai.modelwrapper.feature_selection.FRegressionFilterPValue.__init__","text":"Initialize the object. Parameters: Name Type Description Default p_threshold float Upper bound for p_values. 0.05 Source code in photonai/modelwrapper/feature_selection.py def __init__ ( self , p_threshold : float = .05 ): \"\"\" Initialize the object. Parameters: p_threshold: Upper bound for p_values. \"\"\" self . p_threshold = p_threshold self . selected_indices = [] self . n_original_features = None","title":"__init__()"},{"location":"api/modelwrapper/feature_selection/FRegressionFilterPValue/#photonai.modelwrapper.feature_selection.FRegressionFilterPValue.fit","text":"Calculation of the important columns. Apply f_regression on input X, y to generate p_values. selected_indices = all p_value(columns) < p_threshold. Parameters: Name Type Description Default X ndarray The input samples of shape [n_samples, n_original_features] required y ndarray The input targets of shape [n_samples, 1] required Source code in photonai/modelwrapper/feature_selection.py def fit ( self , X : np . ndarray , y : np . ndarray ): \"\"\"Calculation of the important columns. Apply f_regression on input X, y to generate p_values. selected_indices = all p_value(columns) < p_threshold. Parameters: X: The input samples of shape [n_samples, n_original_features] y: The input targets of shape [n_samples, 1] \"\"\" self . n_original_features = X . shape [ 1 ] _ , p_values = f_regression ( X , y ) self . selected_indices = np . where ( p_values < self . p_threshold )[ 0 ] return self","title":"fit()"},{"location":"api/modelwrapper/feature_selection/FRegressionFilterPValue/#photonai.modelwrapper.feature_selection.FRegressionFilterPValue.inverse_transform","text":"Reverse to original dimension. Parameters: Name Type Description Default X ndarray The input samples of shape [n_samples, n_selected_features]. required Exceptions: Type Description ValueError If input X has a different shape than during fitting. Returns: Type Description ndarray Array of shape [n_samples, n_original_features] with columns of zeros inserted where features would have been removed. Source code in photonai/modelwrapper/feature_selection.py def inverse_transform ( self , X : np . ndarray ) -> np . ndarray : \"\"\"Reverse to original dimension. Parameters: X: The input samples of shape [n_samples, n_selected_features]. Raises: ValueError: If input X has a different shape than during fitting. Returns: Array of shape [n_samples, n_original_features] with columns of zeros inserted where features would have been removed. \"\"\" if X . shape [ 1 ] != len ( self . selected_indices ): msg = \"X has a different shape than during fitting.\" logger . error ( msg ) raise ValueError ( msg ) Xt = np . zeros (( X . shape [ 0 ], self . n_original_features )) Xt [:, self . selected_indices ] = X return Xt","title":"inverse_transform()"},{"location":"api/modelwrapper/feature_selection/FRegressionFilterPValue/#photonai.modelwrapper.feature_selection.FRegressionFilterPValue.transform","text":"Reduced input X to selected_columns. Returns: Type Description ndarray Column-filtered array of shape [n_samples, n_selected_features]. Source code in photonai/modelwrapper/feature_selection.py def transform ( self , X : np . ndarray ) -> np . ndarray : \"\"\"Reduced input X to selected_columns. Parameters: X The input samples of shape [n_samples, n_original_features] Returns: Column-filtered array of shape [n_samples, n_selected_features]. \"\"\" return X [:, self . selected_indices ]","title":"transform()"},{"location":"api/modelwrapper/feature_selection/FRegressionSelectPercentile/","text":"Documentation for FRegressionSelectPercentile Feature Selection for regression data - percentile based. Apply VarianceThreshold -> SelectPercentile to data. SelectPercentile based on f_regression and parameter percentile. Source code in photonai/modelwrapper/feature_selection.py class FRegressionSelectPercentile ( BaseEstimator , TransformerMixin ): \"\"\"Feature Selection for regression data - percentile based. Apply VarianceThreshold -> SelectPercentile to data. SelectPercentile based on f_regression and parameter percentile. \"\"\" _estimator_type = \"transformer\" def __init__ ( self , percentile : float = 10 ): \"\"\" Initialize the object. Parameters: percentile: Percent of features to keep. \"\"\" self . var_thres = VarianceThreshold () self . percentile = percentile self . my_fs = None def fit ( self , X , y ): X = self . var_thres . fit_transform ( X ) self . my_fs = SelectPercentile ( score_func = f_regression , percentile = self . percentile ) self . my_fs . fit ( X , y ) return self def transform ( self , X ): X = self . var_thres . transform ( X ) return self . my_fs . transform ( X ) def inverse_transform ( self , X ): Xt = self . my_fs . inverse_transform ( X ) return self . var_thres . inverse_transform ( Xt ) __init__ ( self , percentile = 10 ) special Initialize the object. Parameters: Name Type Description Default percentile float Percent of features to keep. 10 Source code in photonai/modelwrapper/feature_selection.py def __init__ ( self , percentile : float = 10 ): \"\"\" Initialize the object. Parameters: percentile: Percent of features to keep. \"\"\" self . var_thres = VarianceThreshold () self . percentile = percentile self . my_fs = None","title":"FRegressionSelectPercentile"},{"location":"api/modelwrapper/feature_selection/FRegressionSelectPercentile/#documentation-for-fregressionselectpercentile","text":"Feature Selection for regression data - percentile based. Apply VarianceThreshold -> SelectPercentile to data. SelectPercentile based on f_regression and parameter percentile. Source code in photonai/modelwrapper/feature_selection.py class FRegressionSelectPercentile ( BaseEstimator , TransformerMixin ): \"\"\"Feature Selection for regression data - percentile based. Apply VarianceThreshold -> SelectPercentile to data. SelectPercentile based on f_regression and parameter percentile. \"\"\" _estimator_type = \"transformer\" def __init__ ( self , percentile : float = 10 ): \"\"\" Initialize the object. Parameters: percentile: Percent of features to keep. \"\"\" self . var_thres = VarianceThreshold () self . percentile = percentile self . my_fs = None def fit ( self , X , y ): X = self . var_thres . fit_transform ( X ) self . my_fs = SelectPercentile ( score_func = f_regression , percentile = self . percentile ) self . my_fs . fit ( X , y ) return self def transform ( self , X ): X = self . var_thres . transform ( X ) return self . my_fs . transform ( X ) def inverse_transform ( self , X ): Xt = self . my_fs . inverse_transform ( X ) return self . var_thres . inverse_transform ( Xt )","title":"Documentation for FRegressionSelectPercentile"},{"location":"api/modelwrapper/feature_selection/FRegressionSelectPercentile/#photonai.modelwrapper.feature_selection.FRegressionSelectPercentile.__init__","text":"Initialize the object. Parameters: Name Type Description Default percentile float Percent of features to keep. 10 Source code in photonai/modelwrapper/feature_selection.py def __init__ ( self , percentile : float = 10 ): \"\"\" Initialize the object. Parameters: percentile: Percent of features to keep. \"\"\" self . var_thres = VarianceThreshold () self . percentile = percentile self . my_fs = None","title":"__init__()"},{"location":"api/modelwrapper/feature_selection/LassoFeatureSelection/","text":"Documentation for LassoFeatureSelection Lasso based feature selection - based on feature_importance. Apply Lasso to ModelSelection. Source code in photonai/modelwrapper/feature_selection.py class LassoFeatureSelection ( BaseEstimator , TransformerMixin ): \"\"\"Lasso based feature selection - based on feature_importance. Apply Lasso to ModelSelection. \"\"\" def __init__ ( self , percentile : float = 0.3 , alpha : float = 1. , ** kwargs ): \"\"\" Initialize the object. Parameters: percentile: bool, default=False Percent of features to keep. alpha: float, default=1. Weighting parameter for Lasso. **kwargs: Passed to Lasso object. \"\"\" self . percentile = percentile self . alpha = alpha self . model_selector = None self . Lasso_kwargs = kwargs self . needs_covariates = False self . needs_y = False def fit ( self , X , y = None , ** kwargs ): self . model_selector = ModelSelector ( Lasso ( alpha = self . alpha , ** self . Lasso_kwargs ), threshold = self . percentile , percentile = True ) self . model_selector . fit ( X , y , ** kwargs ) return self def transform ( self , X , y = None , ** kwargs ): selected_features = self . model_selector . transform ( X , y , ** kwargs ) return selected_features def set_params ( self , ** params ): super ( LassoFeatureSelection , self ) . set_params ( ** params ) def inverse_transform ( self , X , y = None , ** kwargs ): return self . model_selector . inverse_transform ( X ) __init__ ( self , percentile = 0.3 , alpha = 1.0 , ** kwargs ) special Initialize the object. Parameters: Name Type Description Default percentile float bool, default=False Percent of features to keep. 0.3 alpha float float, default=1. Weighting parameter for Lasso. 1.0 **kwargs Passed to Lasso object. {} Source code in photonai/modelwrapper/feature_selection.py def __init__ ( self , percentile : float = 0.3 , alpha : float = 1. , ** kwargs ): \"\"\" Initialize the object. Parameters: percentile: bool, default=False Percent of features to keep. alpha: float, default=1. Weighting parameter for Lasso. **kwargs: Passed to Lasso object. \"\"\" self . percentile = percentile self . alpha = alpha self . model_selector = None self . Lasso_kwargs = kwargs self . needs_covariates = False self . needs_y = False","title":"LassoFeatureSelection"},{"location":"api/modelwrapper/feature_selection/LassoFeatureSelection/#documentation-for-lassofeatureselection","text":"Lasso based feature selection - based on feature_importance. Apply Lasso to ModelSelection. Source code in photonai/modelwrapper/feature_selection.py class LassoFeatureSelection ( BaseEstimator , TransformerMixin ): \"\"\"Lasso based feature selection - based on feature_importance. Apply Lasso to ModelSelection. \"\"\" def __init__ ( self , percentile : float = 0.3 , alpha : float = 1. , ** kwargs ): \"\"\" Initialize the object. Parameters: percentile: bool, default=False Percent of features to keep. alpha: float, default=1. Weighting parameter for Lasso. **kwargs: Passed to Lasso object. \"\"\" self . percentile = percentile self . alpha = alpha self . model_selector = None self . Lasso_kwargs = kwargs self . needs_covariates = False self . needs_y = False def fit ( self , X , y = None , ** kwargs ): self . model_selector = ModelSelector ( Lasso ( alpha = self . alpha , ** self . Lasso_kwargs ), threshold = self . percentile , percentile = True ) self . model_selector . fit ( X , y , ** kwargs ) return self def transform ( self , X , y = None , ** kwargs ): selected_features = self . model_selector . transform ( X , y , ** kwargs ) return selected_features def set_params ( self , ** params ): super ( LassoFeatureSelection , self ) . set_params ( ** params ) def inverse_transform ( self , X , y = None , ** kwargs ): return self . model_selector . inverse_transform ( X )","title":"Documentation for LassoFeatureSelection"},{"location":"api/modelwrapper/feature_selection/LassoFeatureSelection/#photonai.modelwrapper.feature_selection.LassoFeatureSelection.__init__","text":"Initialize the object. Parameters: Name Type Description Default percentile float bool, default=False Percent of features to keep. 0.3 alpha float float, default=1. Weighting parameter for Lasso. 1.0 **kwargs Passed to Lasso object. {} Source code in photonai/modelwrapper/feature_selection.py def __init__ ( self , percentile : float = 0.3 , alpha : float = 1. , ** kwargs ): \"\"\" Initialize the object. Parameters: percentile: bool, default=False Percent of features to keep. alpha: float, default=1. Weighting parameter for Lasso. **kwargs: Passed to Lasso object. \"\"\" self . percentile = percentile self . alpha = alpha self . model_selector = None self . Lasso_kwargs = kwargs self . needs_covariates = False self . needs_y = False","title":"__init__()"},{"location":"api/modelwrapper/feature_selection/ModelSelector/","text":"Documentation for ModelSelector Model Selector - based on feature_importance. Apply feature selection on specific estimator and its importance scores. Source code in photonai/modelwrapper/feature_selection.py class ModelSelector ( BaseEstimator , TransformerMixin ): \"\"\"Model Selector - based on feature_importance. Apply feature selection on specific estimator and its importance scores. \"\"\" _estimator_type = \"transformer\" def __init__ ( self , estimator_obj : BaseEstimator , threshold : float = 1e-5 , percentile : bool = False ): \"\"\" Initialize the object. Parameters: estimator_obj: Estimator with fit/tranform and possibility of feature_importance. threshold: If percentile == True: Lower Bound for required importance score to keep. If percentile == True: percentage to keep (ordered features by feature_importance) percentile: Percent of features to keep. \"\"\" self . threshold = threshold self . estimator_obj = estimator_obj self . selected_indices = [] self . percentile = percentile self . importance_scores = [] self . n_original_features = None def _get_feature_importances ( self , estimator , norm_order = 1 ): \"\"\"Retrieve or aggregate feature importances from estimator\"\"\" importances = getattr ( estimator , \"feature_importances_\" , None ) if importances is None and hasattr ( estimator , \"coef_\" ): if estimator . coef_ . ndim == 1 : importances = np . abs ( estimator . coef_ ) else : importances = np . linalg . norm ( estimator . coef_ , axis = 0 , ord = norm_order ) elif importances is None : raise ValueError ( \"The underlying estimator %s has no `coef_` or \" \"`feature_importances_` attribute. Either pass a fitted estimator\" \" to SelectFromModel or call fit before calling transform.\" % estimator . __class__ . __name__ ) return importances def fit ( self , X , y = None , ** kwargs ): self . n_original_features = X . shape [ 1 ] # 1. fit estimator self . estimator_obj . fit ( X , y ) # penalty = \"l1\" self . importance_scores = self . _get_feature_importances ( self . estimator_obj ) if not self . percentile : self . selected_indices = np . where ( self . importance_scores >= self . threshold )[ 0 ] else : # Todo: works only for binary classification, not for multiclass if self . threshold > 1 : raise ValueError ( \"Threshold should not be greater than 1\" ) ordered_importances = np . sort ( self . importance_scores ) if isinstance ( X , list ): X = np . array ( X ) index = int ( np . floor (( 1 - self . threshold ) * X . shape [ 1 ])) percentile_thres = ordered_importances [ index ] self . selected_indices = np . where ( self . importance_scores >= percentile_thres )[ 0 ] # Todo: sortieren und Grenze definieren und dann np.where pass return self def transform ( self , X , y = None , ** kwargs ): if isinstance ( X , list ): X = np . array ( X ) X_new = X [:, self . selected_indices ] # if no features were selected raise error if X_new . shape [ 1 ] == 0 : print ( \"No Features were selected from model, using all features\" ) return X return X_new def inverse_transform ( self , X ): if X . shape [ 1 ] != len ( self . selected_indices ): msg = \"X has a different shape than during fitting.\" logger . error ( msg ) raise ValueError ( msg ) Xt = np . zeros (( X . shape [ 0 ], self . n_original_features )) Xt [:, self . selected_indices ] = X return Xt def set_params ( self , ** params ): if 'threshold' in params : self . threshold = params [ 'threshold' ] params . pop ( 'threshold' ) self . estimator_obj . set_params ( ** params ) def get_params ( self , deep = True ): return self . estimator_obj . get_params ( deep ) __init__ ( self , estimator_obj , threshold = 1e-05 , percentile = False ) special Initialize the object. Parameters: Name Type Description Default estimator_obj BaseEstimator Estimator with fit/tranform and possibility of feature_importance. required threshold float If percentile == True: Lower Bound for required importance score to keep. If percentile == True: percentage to keep (ordered features by feature_importance) 1e-05 percentile bool Percent of features to keep. False Source code in photonai/modelwrapper/feature_selection.py def __init__ ( self , estimator_obj : BaseEstimator , threshold : float = 1e-5 , percentile : bool = False ): \"\"\" Initialize the object. Parameters: estimator_obj: Estimator with fit/tranform and possibility of feature_importance. threshold: If percentile == True: Lower Bound for required importance score to keep. If percentile == True: percentage to keep (ordered features by feature_importance) percentile: Percent of features to keep. \"\"\" self . threshold = threshold self . estimator_obj = estimator_obj self . selected_indices = [] self . percentile = percentile self . importance_scores = [] self . n_original_features = None","title":"ModelSelector"},{"location":"api/modelwrapper/feature_selection/ModelSelector/#documentation-for-modelselector","text":"Model Selector - based on feature_importance. Apply feature selection on specific estimator and its importance scores. Source code in photonai/modelwrapper/feature_selection.py class ModelSelector ( BaseEstimator , TransformerMixin ): \"\"\"Model Selector - based on feature_importance. Apply feature selection on specific estimator and its importance scores. \"\"\" _estimator_type = \"transformer\" def __init__ ( self , estimator_obj : BaseEstimator , threshold : float = 1e-5 , percentile : bool = False ): \"\"\" Initialize the object. Parameters: estimator_obj: Estimator with fit/tranform and possibility of feature_importance. threshold: If percentile == True: Lower Bound for required importance score to keep. If percentile == True: percentage to keep (ordered features by feature_importance) percentile: Percent of features to keep. \"\"\" self . threshold = threshold self . estimator_obj = estimator_obj self . selected_indices = [] self . percentile = percentile self . importance_scores = [] self . n_original_features = None def _get_feature_importances ( self , estimator , norm_order = 1 ): \"\"\"Retrieve or aggregate feature importances from estimator\"\"\" importances = getattr ( estimator , \"feature_importances_\" , None ) if importances is None and hasattr ( estimator , \"coef_\" ): if estimator . coef_ . ndim == 1 : importances = np . abs ( estimator . coef_ ) else : importances = np . linalg . norm ( estimator . coef_ , axis = 0 , ord = norm_order ) elif importances is None : raise ValueError ( \"The underlying estimator %s has no `coef_` or \" \"`feature_importances_` attribute. Either pass a fitted estimator\" \" to SelectFromModel or call fit before calling transform.\" % estimator . __class__ . __name__ ) return importances def fit ( self , X , y = None , ** kwargs ): self . n_original_features = X . shape [ 1 ] # 1. fit estimator self . estimator_obj . fit ( X , y ) # penalty = \"l1\" self . importance_scores = self . _get_feature_importances ( self . estimator_obj ) if not self . percentile : self . selected_indices = np . where ( self . importance_scores >= self . threshold )[ 0 ] else : # Todo: works only for binary classification, not for multiclass if self . threshold > 1 : raise ValueError ( \"Threshold should not be greater than 1\" ) ordered_importances = np . sort ( self . importance_scores ) if isinstance ( X , list ): X = np . array ( X ) index = int ( np . floor (( 1 - self . threshold ) * X . shape [ 1 ])) percentile_thres = ordered_importances [ index ] self . selected_indices = np . where ( self . importance_scores >= percentile_thres )[ 0 ] # Todo: sortieren und Grenze definieren und dann np.where pass return self def transform ( self , X , y = None , ** kwargs ): if isinstance ( X , list ): X = np . array ( X ) X_new = X [:, self . selected_indices ] # if no features were selected raise error if X_new . shape [ 1 ] == 0 : print ( \"No Features were selected from model, using all features\" ) return X return X_new def inverse_transform ( self , X ): if X . shape [ 1 ] != len ( self . selected_indices ): msg = \"X has a different shape than during fitting.\" logger . error ( msg ) raise ValueError ( msg ) Xt = np . zeros (( X . shape [ 0 ], self . n_original_features )) Xt [:, self . selected_indices ] = X return Xt def set_params ( self , ** params ): if 'threshold' in params : self . threshold = params [ 'threshold' ] params . pop ( 'threshold' ) self . estimator_obj . set_params ( ** params ) def get_params ( self , deep = True ): return self . estimator_obj . get_params ( deep )","title":"Documentation for ModelSelector"},{"location":"api/modelwrapper/feature_selection/ModelSelector/#photonai.modelwrapper.feature_selection.ModelSelector.__init__","text":"Initialize the object. Parameters: Name Type Description Default estimator_obj BaseEstimator Estimator with fit/tranform and possibility of feature_importance. required threshold float If percentile == True: Lower Bound for required importance score to keep. If percentile == True: percentage to keep (ordered features by feature_importance) 1e-05 percentile bool Percent of features to keep. False Source code in photonai/modelwrapper/feature_selection.py def __init__ ( self , estimator_obj : BaseEstimator , threshold : float = 1e-5 , percentile : bool = False ): \"\"\" Initialize the object. Parameters: estimator_obj: Estimator with fit/tranform and possibility of feature_importance. threshold: If percentile == True: Lower Bound for required importance score to keep. If percentile == True: percentage to keep (ordered features by feature_importance) percentile: Percent of features to keep. \"\"\" self . threshold = threshold self . estimator_obj = estimator_obj self . selected_indices = [] self . percentile = percentile self . importance_scores = [] self . n_original_features = None","title":"__init__()"},{"location":"api/modelwrapper/keras/dnn_classifier/","text":"Documentation for KerasDnnClassifier Wrapper class for a classification-based Keras model. See Keras API . Examples: 1 2 3 4 5 6 7 PipelineElement ( 'KerasDnnClassifier' , hyperparameters = { 'hidden_layer_sizes' : Categorical ([[ 10 , 8 , 4 ], [ 20 , 15 , 5 ]]), 'dropout_rate' : Categorical ([ 0.5 , [ 0.5 , 0.2 , 0.1 ]])}, activations = 'relu' , nn_batch_size = 32 , multi_class = True , verbosity = 1 ) Source code in photonai/modelwrapper/keras_dnn_classifier.py class KerasDnnClassifier ( KerasDnnBaseModel , KerasBaseClassifier ): \"\"\"Wrapper class for a classification-based Keras model. See [Keras API](https://keras.io/api/). Example: ``` python PipelineElement('KerasDnnClassifier', hyperparameters={'hidden_layer_sizes': Categorical([[10, 8, 4], [20, 15, 5]]), 'dropout_rate': Categorical([0.5, [0.5, 0.2, 0.1]])}, activations='relu', nn_batch_size=32, multi_class=True, verbosity=1) ``` \"\"\" def __init__ ( self , multi_class : bool = True , hidden_layer_sizes : list = None , learning_rate : float = 0.01 , loss : str = \"\" , epochs : int = 100 , nn_batch_size : int = 64 , metrics : list = None , callbacks : list = None , validation_split : float = 0.1 , verbosity : int = 1 , dropout_rate : Union [ float , list ] = 0.2 , activations : Union [ str , list ] = 'relu' , optimizer : Union [ Optimizer , str ] = \"adam\" ): \"\"\" Initialize the object. Parameters: multi_class: Enables multi_target learning. hidden_layer_sizes: Number of perceptrons per layer. learning_rate: Step size of the learning adjustment. loss: Loss function. epochs: Number of arbitrary cutoffs, generally defined as \"one pass over the entire dataset\", used to separate training into distinct phases, which is useful for logging and periodic evaluation. nn_batch_size: Typically the batch_size. A batch is a set of nn_batch_size samples. The samples in a batch are processed independently, in parallel. If training, a batch results in only one update to the model. metrics: List of evaluate metrics. callbacks: Within Keras, there is the ability to add callbacks specifically designed to be run at the end of an epoch. Examples of these are learning rate changes and model checkpointing (saving). validation_split: Split size of validation set. verbosity: The level of verbosity, 0 is least talkative and gives only warn and error, 1 gives adds info and 2 adds debug. dropout_rate: A Dropout layer applies random dropout and rescales the output. In inference mode, the same layer does nothing. Float -> added behind each layer List -> Same size as hidden_layer_size activations: Activation function. optimizer: Optimization algorithm. \"\"\" self . _loss = \"\" self . _multi_class = None self . loss = loss self . multi_class = multi_class self . epochs = epochs self . nn_batch_size = nn_batch_size self . validation_split = validation_split if callbacks : self . callbacks = callbacks else : self . callbacks = [] if not metrics : metrics = [ 'accuracy' ] super ( KerasDnnClassifier , self ) . __init__ ( hidden_layer_sizes = hidden_layer_sizes , target_activation = \"softmax\" , learning_rate = learning_rate , loss = loss , metrics = metrics , dropout_rate = dropout_rate , activations = activations , optimizer = optimizer , verbosity = verbosity ) @property def multi_class ( self ): return self . _multi_class @multi_class . setter def multi_class ( self , value ): self . _multi_class = value if not self . loss or self . loss in [ \"categorical_crossentropy\" , \"binary_crossentropy\" ]: if value : self . loss = \"categorical_crossentropy\" else : self . loss = \"binary_crossentropy\" @property def target_activation ( self ): return self . _target_activation @target_activation . setter def target_activation ( self , value ): if value == \"softmax\" : self . _target_activation = value else : raise ValueError ( \"The Classifcation subclass of KerasDnnBaseModel does not allow to use another \" \"target_activation. Please use 'softmax' like default.\" ) @property def loss ( self ): return self . _loss @loss . setter def loss ( self , value ): if value == \"\" : if self . _multi_class : self . _loss = \"categorical_crossentropy\" else : self . _loss = \"binary_crossentropy\" elif self . _multi_class and value in keras_dnn_base_model . get_loss_allocation ()[ \"multi_classification\" ]: self . _loss = value elif ( not self . _multi_class ) and value in keras_dnn_base_model . get_loss_allocation ()[ \"binary_classification\" ]: self . _loss = value else : raise ValueError ( \"Loss function is not supported. Feel free to use upperclass without restrictions.\" ) def _calc_target_dimension ( self , y ): class_nums = len ( np . unique ( y )) if not self . multi_class : if class_nums != 2 and self . loss == 'binary_crossentropy' : raise ValueError ( \"Can not use binary classification with more or less than two target classes.\" ) self . target_dimension = 1 else : self . target_dimension = len ( np . unique ( y )) def fit ( self , X : np . ndarray , y : np . ndarray ): \"\"\" Starting the learning process of the neural network. Parameters: X: The input samples with shape [n_samples, n_features]. y: The input targets with shape [n_samples, 1]. \"\"\" self . _calc_target_dimension ( y ) self . create_model ( X . shape [ 1 ]) super ( KerasDnnClassifier , self ) . fit ( X , y ) return self __init__ ( self , multi_class = True , hidden_layer_sizes = None , learning_rate = 0.01 , loss = '' , epochs = 100 , nn_batch_size = 64 , metrics = None , callbacks = None , validation_split = 0.1 , verbosity = 1 , dropout_rate = 0.2 , activations = 'relu' , optimizer = 'adam' ) special Initialize the object. Parameters: Name Type Description Default multi_class bool Enables multi_target learning. True hidden_layer_sizes list Number of perceptrons per layer. None learning_rate float Step size of the learning adjustment. 0.01 loss str Loss function. '' epochs int Number of arbitrary cutoffs, generally defined as \"one pass over the entire dataset\", used to separate training into distinct phases, which is useful for logging and periodic evaluation. 100 nn_batch_size int Typically the batch_size. A batch is a set of nn_batch_size samples. The samples in a batch are processed independently, in parallel. If training, a batch results in only one update to the model. 64 metrics list List of evaluate metrics. None callbacks list Within Keras, there is the ability to add callbacks specifically designed to be run at the end of an epoch. Examples of these are learning rate changes and model checkpointing (saving). None validation_split float Split size of validation set. 0.1 verbosity int The level of verbosity, 0 is least talkative and gives only warn and error, 1 gives adds info and 2 adds debug. 1 dropout_rate Union[float, list] A Dropout layer applies random dropout and rescales the output. In inference mode, the same layer does nothing. Float -> added behind each layer List -> Same size as hidden_layer_size 0.2 activations Union[str, list] Activation function. 'relu' optimizer Union[tensorflow.python.keras.optimizer_v2.optimizer_v2.OptimizerV2, str] Optimization algorithm. 'adam' Source code in photonai/modelwrapper/keras_dnn_classifier.py def __init__ ( self , multi_class : bool = True , hidden_layer_sizes : list = None , learning_rate : float = 0.01 , loss : str = \"\" , epochs : int = 100 , nn_batch_size : int = 64 , metrics : list = None , callbacks : list = None , validation_split : float = 0.1 , verbosity : int = 1 , dropout_rate : Union [ float , list ] = 0.2 , activations : Union [ str , list ] = 'relu' , optimizer : Union [ Optimizer , str ] = \"adam\" ): \"\"\" Initialize the object. Parameters: multi_class: Enables multi_target learning. hidden_layer_sizes: Number of perceptrons per layer. learning_rate: Step size of the learning adjustment. loss: Loss function. epochs: Number of arbitrary cutoffs, generally defined as \"one pass over the entire dataset\", used to separate training into distinct phases, which is useful for logging and periodic evaluation. nn_batch_size: Typically the batch_size. A batch is a set of nn_batch_size samples. The samples in a batch are processed independently, in parallel. If training, a batch results in only one update to the model. metrics: List of evaluate metrics. callbacks: Within Keras, there is the ability to add callbacks specifically designed to be run at the end of an epoch. Examples of these are learning rate changes and model checkpointing (saving). validation_split: Split size of validation set. verbosity: The level of verbosity, 0 is least talkative and gives only warn and error, 1 gives adds info and 2 adds debug. dropout_rate: A Dropout layer applies random dropout and rescales the output. In inference mode, the same layer does nothing. Float -> added behind each layer List -> Same size as hidden_layer_size activations: Activation function. optimizer: Optimization algorithm. \"\"\" self . _loss = \"\" self . _multi_class = None self . loss = loss self . multi_class = multi_class self . epochs = epochs self . nn_batch_size = nn_batch_size self . validation_split = validation_split if callbacks : self . callbacks = callbacks else : self . callbacks = [] if not metrics : metrics = [ 'accuracy' ] super ( KerasDnnClassifier , self ) . __init__ ( hidden_layer_sizes = hidden_layer_sizes , target_activation = \"softmax\" , learning_rate = learning_rate , loss = loss , metrics = metrics , dropout_rate = dropout_rate , activations = activations , optimizer = optimizer , verbosity = verbosity ) fit ( self , X , y ) Starting the learning process of the neural network. Parameters: Name Type Description Default X ndarray The input samples with shape [n_samples, n_features]. required y ndarray The input targets with shape [n_samples, 1]. required Source code in photonai/modelwrapper/keras_dnn_classifier.py def fit ( self , X : np . ndarray , y : np . ndarray ): \"\"\" Starting the learning process of the neural network. Parameters: X: The input samples with shape [n_samples, n_features]. y: The input targets with shape [n_samples, 1]. \"\"\" self . _calc_target_dimension ( y ) self . create_model ( X . shape [ 1 ]) super ( KerasDnnClassifier , self ) . fit ( X , y ) return self","title":"KerasDnnClassifier"},{"location":"api/modelwrapper/keras/dnn_classifier/#documentation-for-kerasdnnclassifier","text":"Wrapper class for a classification-based Keras model. See Keras API . Examples: 1 2 3 4 5 6 7 PipelineElement ( 'KerasDnnClassifier' , hyperparameters = { 'hidden_layer_sizes' : Categorical ([[ 10 , 8 , 4 ], [ 20 , 15 , 5 ]]), 'dropout_rate' : Categorical ([ 0.5 , [ 0.5 , 0.2 , 0.1 ]])}, activations = 'relu' , nn_batch_size = 32 , multi_class = True , verbosity = 1 ) Source code in photonai/modelwrapper/keras_dnn_classifier.py class KerasDnnClassifier ( KerasDnnBaseModel , KerasBaseClassifier ): \"\"\"Wrapper class for a classification-based Keras model. See [Keras API](https://keras.io/api/). Example: ``` python PipelineElement('KerasDnnClassifier', hyperparameters={'hidden_layer_sizes': Categorical([[10, 8, 4], [20, 15, 5]]), 'dropout_rate': Categorical([0.5, [0.5, 0.2, 0.1]])}, activations='relu', nn_batch_size=32, multi_class=True, verbosity=1) ``` \"\"\" def __init__ ( self , multi_class : bool = True , hidden_layer_sizes : list = None , learning_rate : float = 0.01 , loss : str = \"\" , epochs : int = 100 , nn_batch_size : int = 64 , metrics : list = None , callbacks : list = None , validation_split : float = 0.1 , verbosity : int = 1 , dropout_rate : Union [ float , list ] = 0.2 , activations : Union [ str , list ] = 'relu' , optimizer : Union [ Optimizer , str ] = \"adam\" ): \"\"\" Initialize the object. Parameters: multi_class: Enables multi_target learning. hidden_layer_sizes: Number of perceptrons per layer. learning_rate: Step size of the learning adjustment. loss: Loss function. epochs: Number of arbitrary cutoffs, generally defined as \"one pass over the entire dataset\", used to separate training into distinct phases, which is useful for logging and periodic evaluation. nn_batch_size: Typically the batch_size. A batch is a set of nn_batch_size samples. The samples in a batch are processed independently, in parallel. If training, a batch results in only one update to the model. metrics: List of evaluate metrics. callbacks: Within Keras, there is the ability to add callbacks specifically designed to be run at the end of an epoch. Examples of these are learning rate changes and model checkpointing (saving). validation_split: Split size of validation set. verbosity: The level of verbosity, 0 is least talkative and gives only warn and error, 1 gives adds info and 2 adds debug. dropout_rate: A Dropout layer applies random dropout and rescales the output. In inference mode, the same layer does nothing. Float -> added behind each layer List -> Same size as hidden_layer_size activations: Activation function. optimizer: Optimization algorithm. \"\"\" self . _loss = \"\" self . _multi_class = None self . loss = loss self . multi_class = multi_class self . epochs = epochs self . nn_batch_size = nn_batch_size self . validation_split = validation_split if callbacks : self . callbacks = callbacks else : self . callbacks = [] if not metrics : metrics = [ 'accuracy' ] super ( KerasDnnClassifier , self ) . __init__ ( hidden_layer_sizes = hidden_layer_sizes , target_activation = \"softmax\" , learning_rate = learning_rate , loss = loss , metrics = metrics , dropout_rate = dropout_rate , activations = activations , optimizer = optimizer , verbosity = verbosity ) @property def multi_class ( self ): return self . _multi_class @multi_class . setter def multi_class ( self , value ): self . _multi_class = value if not self . loss or self . loss in [ \"categorical_crossentropy\" , \"binary_crossentropy\" ]: if value : self . loss = \"categorical_crossentropy\" else : self . loss = \"binary_crossentropy\" @property def target_activation ( self ): return self . _target_activation @target_activation . setter def target_activation ( self , value ): if value == \"softmax\" : self . _target_activation = value else : raise ValueError ( \"The Classifcation subclass of KerasDnnBaseModel does not allow to use another \" \"target_activation. Please use 'softmax' like default.\" ) @property def loss ( self ): return self . _loss @loss . setter def loss ( self , value ): if value == \"\" : if self . _multi_class : self . _loss = \"categorical_crossentropy\" else : self . _loss = \"binary_crossentropy\" elif self . _multi_class and value in keras_dnn_base_model . get_loss_allocation ()[ \"multi_classification\" ]: self . _loss = value elif ( not self . _multi_class ) and value in keras_dnn_base_model . get_loss_allocation ()[ \"binary_classification\" ]: self . _loss = value else : raise ValueError ( \"Loss function is not supported. Feel free to use upperclass without restrictions.\" ) def _calc_target_dimension ( self , y ): class_nums = len ( np . unique ( y )) if not self . multi_class : if class_nums != 2 and self . loss == 'binary_crossentropy' : raise ValueError ( \"Can not use binary classification with more or less than two target classes.\" ) self . target_dimension = 1 else : self . target_dimension = len ( np . unique ( y )) def fit ( self , X : np . ndarray , y : np . ndarray ): \"\"\" Starting the learning process of the neural network. Parameters: X: The input samples with shape [n_samples, n_features]. y: The input targets with shape [n_samples, 1]. \"\"\" self . _calc_target_dimension ( y ) self . create_model ( X . shape [ 1 ]) super ( KerasDnnClassifier , self ) . fit ( X , y ) return self","title":"Documentation for KerasDnnClassifier"},{"location":"api/modelwrapper/keras/dnn_classifier/#photonai.modelwrapper.keras_dnn_classifier.KerasDnnClassifier.__init__","text":"Initialize the object. Parameters: Name Type Description Default multi_class bool Enables multi_target learning. True hidden_layer_sizes list Number of perceptrons per layer. None learning_rate float Step size of the learning adjustment. 0.01 loss str Loss function. '' epochs int Number of arbitrary cutoffs, generally defined as \"one pass over the entire dataset\", used to separate training into distinct phases, which is useful for logging and periodic evaluation. 100 nn_batch_size int Typically the batch_size. A batch is a set of nn_batch_size samples. The samples in a batch are processed independently, in parallel. If training, a batch results in only one update to the model. 64 metrics list List of evaluate metrics. None callbacks list Within Keras, there is the ability to add callbacks specifically designed to be run at the end of an epoch. Examples of these are learning rate changes and model checkpointing (saving). None validation_split float Split size of validation set. 0.1 verbosity int The level of verbosity, 0 is least talkative and gives only warn and error, 1 gives adds info and 2 adds debug. 1 dropout_rate Union[float, list] A Dropout layer applies random dropout and rescales the output. In inference mode, the same layer does nothing. Float -> added behind each layer List -> Same size as hidden_layer_size 0.2 activations Union[str, list] Activation function. 'relu' optimizer Union[tensorflow.python.keras.optimizer_v2.optimizer_v2.OptimizerV2, str] Optimization algorithm. 'adam' Source code in photonai/modelwrapper/keras_dnn_classifier.py def __init__ ( self , multi_class : bool = True , hidden_layer_sizes : list = None , learning_rate : float = 0.01 , loss : str = \"\" , epochs : int = 100 , nn_batch_size : int = 64 , metrics : list = None , callbacks : list = None , validation_split : float = 0.1 , verbosity : int = 1 , dropout_rate : Union [ float , list ] = 0.2 , activations : Union [ str , list ] = 'relu' , optimizer : Union [ Optimizer , str ] = \"adam\" ): \"\"\" Initialize the object. Parameters: multi_class: Enables multi_target learning. hidden_layer_sizes: Number of perceptrons per layer. learning_rate: Step size of the learning adjustment. loss: Loss function. epochs: Number of arbitrary cutoffs, generally defined as \"one pass over the entire dataset\", used to separate training into distinct phases, which is useful for logging and periodic evaluation. nn_batch_size: Typically the batch_size. A batch is a set of nn_batch_size samples. The samples in a batch are processed independently, in parallel. If training, a batch results in only one update to the model. metrics: List of evaluate metrics. callbacks: Within Keras, there is the ability to add callbacks specifically designed to be run at the end of an epoch. Examples of these are learning rate changes and model checkpointing (saving). validation_split: Split size of validation set. verbosity: The level of verbosity, 0 is least talkative and gives only warn and error, 1 gives adds info and 2 adds debug. dropout_rate: A Dropout layer applies random dropout and rescales the output. In inference mode, the same layer does nothing. Float -> added behind each layer List -> Same size as hidden_layer_size activations: Activation function. optimizer: Optimization algorithm. \"\"\" self . _loss = \"\" self . _multi_class = None self . loss = loss self . multi_class = multi_class self . epochs = epochs self . nn_batch_size = nn_batch_size self . validation_split = validation_split if callbacks : self . callbacks = callbacks else : self . callbacks = [] if not metrics : metrics = [ 'accuracy' ] super ( KerasDnnClassifier , self ) . __init__ ( hidden_layer_sizes = hidden_layer_sizes , target_activation = \"softmax\" , learning_rate = learning_rate , loss = loss , metrics = metrics , dropout_rate = dropout_rate , activations = activations , optimizer = optimizer , verbosity = verbosity )","title":"__init__()"},{"location":"api/modelwrapper/keras/dnn_classifier/#photonai.modelwrapper.keras_dnn_classifier.KerasDnnClassifier.fit","text":"Starting the learning process of the neural network. Parameters: Name Type Description Default X ndarray The input samples with shape [n_samples, n_features]. required y ndarray The input targets with shape [n_samples, 1]. required Source code in photonai/modelwrapper/keras_dnn_classifier.py def fit ( self , X : np . ndarray , y : np . ndarray ): \"\"\" Starting the learning process of the neural network. Parameters: X: The input samples with shape [n_samples, n_features]. y: The input targets with shape [n_samples, 1]. \"\"\" self . _calc_target_dimension ( y ) self . create_model ( X . shape [ 1 ]) super ( KerasDnnClassifier , self ) . fit ( X , y ) return self","title":"fit()"},{"location":"api/modelwrapper/keras/dnn_regressor/","text":"Documentation for KerasDnnRegressor Wrapper class for a regression-based Keras model. See Keras API . Examples: 1 2 3 4 5 6 7 PipelineElement ( 'KerasDnnRegressor' , hyperparameters = { 'hidden_layer_sizes' : Categorical ([[ 18 , 14 ], [ 30 , 5 ]]), 'dropout_rate' : Categorical ([ 0.01 , 0.2 ])}, activations = 'relu' , epochs = 50 , nn_batch_size = 64 , verbosity = 1 ) Source code in photonai/modelwrapper/keras_dnn_regressor.py class KerasDnnRegressor ( KerasDnnBaseModel , KerasBaseRegressor ): \"\"\"Wrapper class for a regression-based Keras model. See [Keras API](https://keras.io/api/). Example: ``` python PipelineElement('KerasDnnRegressor', hyperparameters={'hidden_layer_sizes': Categorical([[18, 14], [30, 5]]), 'dropout_rate': Categorical([0.01, 0.2])}, activations='relu', epochs=50, nn_batch_size=64, verbosity=1) ``` \"\"\" def __init__ ( self , hidden_layer_sizes : int = None , learning_rate : float = 0.01 , loss : str = \"mean_squared_error\" , epochs : int = 10 , nn_batch_size : int = 64 , metrics : list = None , validation_split : float = 0.1 , callbacks : list = None , batch_normalization : bool = True , verbosity : int = 0 , dropout_rate : Union [ float , list ] = 0.2 , activations : Union [ str , list ] = 'relu' , optimizer : Union [ Optimizer , str ] = \"adam\" ): \"\"\" Initialize the object. Parameters: hidden_layer_sizes: Number of perceptrons per layer. learning_rate: Step size of the learning adjustment. loss: Loss function. epochs: Number of arbitrary cutoffs, generally defined as \"one pass over the entire dataset\", used to separate training into distinct phases, which is useful for logging and periodic evaluation. nn_batch_size: Typically the batch_size. A batch is a set of nn_batch_size samples. The samples in a batch are processed independently, in parallel. If training, a batch results in only one update to the model. metrics: List of evaluate metrics. callbacks: Within Keras, there is the ability to add callbacks specifically designed to be run at the end of an epoch. Examples of these are learning rate changes and model checkpointing (saving). validation_split: Split size of validation set. batch_normalization: Batch normalization applies a transformation that maintains the mean output close to 0 and the output standard deviation close to 1. verbosity: The level of verbosity, 0 is least talkative and gives only warn and error, 1 gives adds info and 2 adds debug. dropout_rate: A Dropout layer applies random dropout and rescales the output. In inference mode, the same layer does nothing. Float -> added behind each layer List -> Same size as hidden_layer_size activations: Activation function. optimizer: Optimization algorithm. \"\"\" self . _loss = \"\" self . _multi_class = None self . loss = loss self . epochs = epochs self . nn_batch_size = nn_batch_size self . validation_split = validation_split if callbacks : self . callbacks = callbacks else : self . callbacks = [] if not metrics : metrics = [ 'mean_squared_error' ] super ( KerasDnnRegressor , self ) . __init__ ( hidden_layer_sizes = hidden_layer_sizes , target_activation = \"linear\" , target_dimension = 1 , learning_rate = learning_rate , loss = loss , metrics = metrics , batch_normalization = batch_normalization , verbosity = verbosity , dropout_rate = dropout_rate , activations = activations , optimizer = optimizer ) @property def target_activation ( self ): return \"linear\" @target_activation . setter def target_activation ( self , value ): if value != \"linear\" : msg = \"The subclass of KerasBaseRegressor does not allow to use another \" \\ \"target_activation. Please use 'linear' like default.\" logger . error ( msg ) raise ValueError ( msg ) @property def loss ( self ): return self . _loss @loss . setter def loss ( self , value ): if value in keras_dnn_base_model . get_loss_allocation ()[ \"regression\" ]: self . _loss = value else : raise ValueError ( \"Loss function is not supported. Feel free to use upperclass without restrictions.\" ) def fit ( self , X : np . ndarray , y : np . ndarray ): \"\"\" Starting the learning. Parameters: X: The input samples with shape [n_samples, n_features]. y: The input targets with shape [n_samples, 1]. \"\"\" self . create_model ( X . shape [ 1 ]) super ( KerasDnnBaseModel , self ) . fit ( X , y ) return self __init__ ( self , hidden_layer_sizes = None , learning_rate = 0.01 , loss = 'mean_squared_error' , epochs = 10 , nn_batch_size = 64 , metrics = None , validation_split = 0.1 , callbacks = None , batch_normalization = True , verbosity = 0 , dropout_rate = 0.2 , activations = 'relu' , optimizer = 'adam' ) special Initialize the object. Parameters: Name Type Description Default hidden_layer_sizes int Number of perceptrons per layer. None learning_rate float Step size of the learning adjustment. 0.01 loss str Loss function. 'mean_squared_error' epochs int Number of arbitrary cutoffs, generally defined as \"one pass over the entire dataset\", used to separate training into distinct phases, which is useful for logging and periodic evaluation. 10 nn_batch_size int Typically the batch_size. A batch is a set of nn_batch_size samples. The samples in a batch are processed independently, in parallel. If training, a batch results in only one update to the model. 64 metrics list List of evaluate metrics. None callbacks list Within Keras, there is the ability to add callbacks specifically designed to be run at the end of an epoch. Examples of these are learning rate changes and model checkpointing (saving). None validation_split float Split size of validation set. 0.1 batch_normalization bool Batch normalization applies a transformation that maintains the mean output close to 0 and the output standard deviation close to 1. True verbosity int The level of verbosity, 0 is least talkative and gives only warn and error, 1 gives adds info and 2 adds debug. 0 dropout_rate Union[float, list] A Dropout layer applies random dropout and rescales the output. In inference mode, the same layer does nothing. Float -> added behind each layer List -> Same size as hidden_layer_size 0.2 activations Union[str, list] Activation function. 'relu' optimizer Union[tensorflow.python.keras.optimizer_v2.optimizer_v2.OptimizerV2, str] Optimization algorithm. 'adam' Source code in photonai/modelwrapper/keras_dnn_regressor.py def __init__ ( self , hidden_layer_sizes : int = None , learning_rate : float = 0.01 , loss : str = \"mean_squared_error\" , epochs : int = 10 , nn_batch_size : int = 64 , metrics : list = None , validation_split : float = 0.1 , callbacks : list = None , batch_normalization : bool = True , verbosity : int = 0 , dropout_rate : Union [ float , list ] = 0.2 , activations : Union [ str , list ] = 'relu' , optimizer : Union [ Optimizer , str ] = \"adam\" ): \"\"\" Initialize the object. Parameters: hidden_layer_sizes: Number of perceptrons per layer. learning_rate: Step size of the learning adjustment. loss: Loss function. epochs: Number of arbitrary cutoffs, generally defined as \"one pass over the entire dataset\", used to separate training into distinct phases, which is useful for logging and periodic evaluation. nn_batch_size: Typically the batch_size. A batch is a set of nn_batch_size samples. The samples in a batch are processed independently, in parallel. If training, a batch results in only one update to the model. metrics: List of evaluate metrics. callbacks: Within Keras, there is the ability to add callbacks specifically designed to be run at the end of an epoch. Examples of these are learning rate changes and model checkpointing (saving). validation_split: Split size of validation set. batch_normalization: Batch normalization applies a transformation that maintains the mean output close to 0 and the output standard deviation close to 1. verbosity: The level of verbosity, 0 is least talkative and gives only warn and error, 1 gives adds info and 2 adds debug. dropout_rate: A Dropout layer applies random dropout and rescales the output. In inference mode, the same layer does nothing. Float -> added behind each layer List -> Same size as hidden_layer_size activations: Activation function. optimizer: Optimization algorithm. \"\"\" self . _loss = \"\" self . _multi_class = None self . loss = loss self . epochs = epochs self . nn_batch_size = nn_batch_size self . validation_split = validation_split if callbacks : self . callbacks = callbacks else : self . callbacks = [] if not metrics : metrics = [ 'mean_squared_error' ] super ( KerasDnnRegressor , self ) . __init__ ( hidden_layer_sizes = hidden_layer_sizes , target_activation = \"linear\" , target_dimension = 1 , learning_rate = learning_rate , loss = loss , metrics = metrics , batch_normalization = batch_normalization , verbosity = verbosity , dropout_rate = dropout_rate , activations = activations , optimizer = optimizer ) fit ( self , X , y ) Starting the learning. Parameters: Name Type Description Default X ndarray The input samples with shape [n_samples, n_features]. required y ndarray The input targets with shape [n_samples, 1]. required Source code in photonai/modelwrapper/keras_dnn_regressor.py def fit ( self , X : np . ndarray , y : np . ndarray ): \"\"\" Starting the learning. Parameters: X: The input samples with shape [n_samples, n_features]. y: The input targets with shape [n_samples, 1]. \"\"\" self . create_model ( X . shape [ 1 ]) super ( KerasDnnBaseModel , self ) . fit ( X , y ) return self","title":"KerasDnnRegressor"},{"location":"api/modelwrapper/keras/dnn_regressor/#documentation-for-kerasdnnregressor","text":"Wrapper class for a regression-based Keras model. See Keras API . Examples: 1 2 3 4 5 6 7 PipelineElement ( 'KerasDnnRegressor' , hyperparameters = { 'hidden_layer_sizes' : Categorical ([[ 18 , 14 ], [ 30 , 5 ]]), 'dropout_rate' : Categorical ([ 0.01 , 0.2 ])}, activations = 'relu' , epochs = 50 , nn_batch_size = 64 , verbosity = 1 ) Source code in photonai/modelwrapper/keras_dnn_regressor.py class KerasDnnRegressor ( KerasDnnBaseModel , KerasBaseRegressor ): \"\"\"Wrapper class for a regression-based Keras model. See [Keras API](https://keras.io/api/). Example: ``` python PipelineElement('KerasDnnRegressor', hyperparameters={'hidden_layer_sizes': Categorical([[18, 14], [30, 5]]), 'dropout_rate': Categorical([0.01, 0.2])}, activations='relu', epochs=50, nn_batch_size=64, verbosity=1) ``` \"\"\" def __init__ ( self , hidden_layer_sizes : int = None , learning_rate : float = 0.01 , loss : str = \"mean_squared_error\" , epochs : int = 10 , nn_batch_size : int = 64 , metrics : list = None , validation_split : float = 0.1 , callbacks : list = None , batch_normalization : bool = True , verbosity : int = 0 , dropout_rate : Union [ float , list ] = 0.2 , activations : Union [ str , list ] = 'relu' , optimizer : Union [ Optimizer , str ] = \"adam\" ): \"\"\" Initialize the object. Parameters: hidden_layer_sizes: Number of perceptrons per layer. learning_rate: Step size of the learning adjustment. loss: Loss function. epochs: Number of arbitrary cutoffs, generally defined as \"one pass over the entire dataset\", used to separate training into distinct phases, which is useful for logging and periodic evaluation. nn_batch_size: Typically the batch_size. A batch is a set of nn_batch_size samples. The samples in a batch are processed independently, in parallel. If training, a batch results in only one update to the model. metrics: List of evaluate metrics. callbacks: Within Keras, there is the ability to add callbacks specifically designed to be run at the end of an epoch. Examples of these are learning rate changes and model checkpointing (saving). validation_split: Split size of validation set. batch_normalization: Batch normalization applies a transformation that maintains the mean output close to 0 and the output standard deviation close to 1. verbosity: The level of verbosity, 0 is least talkative and gives only warn and error, 1 gives adds info and 2 adds debug. dropout_rate: A Dropout layer applies random dropout and rescales the output. In inference mode, the same layer does nothing. Float -> added behind each layer List -> Same size as hidden_layer_size activations: Activation function. optimizer: Optimization algorithm. \"\"\" self . _loss = \"\" self . _multi_class = None self . loss = loss self . epochs = epochs self . nn_batch_size = nn_batch_size self . validation_split = validation_split if callbacks : self . callbacks = callbacks else : self . callbacks = [] if not metrics : metrics = [ 'mean_squared_error' ] super ( KerasDnnRegressor , self ) . __init__ ( hidden_layer_sizes = hidden_layer_sizes , target_activation = \"linear\" , target_dimension = 1 , learning_rate = learning_rate , loss = loss , metrics = metrics , batch_normalization = batch_normalization , verbosity = verbosity , dropout_rate = dropout_rate , activations = activations , optimizer = optimizer ) @property def target_activation ( self ): return \"linear\" @target_activation . setter def target_activation ( self , value ): if value != \"linear\" : msg = \"The subclass of KerasBaseRegressor does not allow to use another \" \\ \"target_activation. Please use 'linear' like default.\" logger . error ( msg ) raise ValueError ( msg ) @property def loss ( self ): return self . _loss @loss . setter def loss ( self , value ): if value in keras_dnn_base_model . get_loss_allocation ()[ \"regression\" ]: self . _loss = value else : raise ValueError ( \"Loss function is not supported. Feel free to use upperclass without restrictions.\" ) def fit ( self , X : np . ndarray , y : np . ndarray ): \"\"\" Starting the learning. Parameters: X: The input samples with shape [n_samples, n_features]. y: The input targets with shape [n_samples, 1]. \"\"\" self . create_model ( X . shape [ 1 ]) super ( KerasDnnBaseModel , self ) . fit ( X , y ) return self","title":"Documentation for KerasDnnRegressor"},{"location":"api/modelwrapper/keras/dnn_regressor/#photonai.modelwrapper.keras_dnn_regressor.KerasDnnRegressor.__init__","text":"Initialize the object. Parameters: Name Type Description Default hidden_layer_sizes int Number of perceptrons per layer. None learning_rate float Step size of the learning adjustment. 0.01 loss str Loss function. 'mean_squared_error' epochs int Number of arbitrary cutoffs, generally defined as \"one pass over the entire dataset\", used to separate training into distinct phases, which is useful for logging and periodic evaluation. 10 nn_batch_size int Typically the batch_size. A batch is a set of nn_batch_size samples. The samples in a batch are processed independently, in parallel. If training, a batch results in only one update to the model. 64 metrics list List of evaluate metrics. None callbacks list Within Keras, there is the ability to add callbacks specifically designed to be run at the end of an epoch. Examples of these are learning rate changes and model checkpointing (saving). None validation_split float Split size of validation set. 0.1 batch_normalization bool Batch normalization applies a transformation that maintains the mean output close to 0 and the output standard deviation close to 1. True verbosity int The level of verbosity, 0 is least talkative and gives only warn and error, 1 gives adds info and 2 adds debug. 0 dropout_rate Union[float, list] A Dropout layer applies random dropout and rescales the output. In inference mode, the same layer does nothing. Float -> added behind each layer List -> Same size as hidden_layer_size 0.2 activations Union[str, list] Activation function. 'relu' optimizer Union[tensorflow.python.keras.optimizer_v2.optimizer_v2.OptimizerV2, str] Optimization algorithm. 'adam' Source code in photonai/modelwrapper/keras_dnn_regressor.py def __init__ ( self , hidden_layer_sizes : int = None , learning_rate : float = 0.01 , loss : str = \"mean_squared_error\" , epochs : int = 10 , nn_batch_size : int = 64 , metrics : list = None , validation_split : float = 0.1 , callbacks : list = None , batch_normalization : bool = True , verbosity : int = 0 , dropout_rate : Union [ float , list ] = 0.2 , activations : Union [ str , list ] = 'relu' , optimizer : Union [ Optimizer , str ] = \"adam\" ): \"\"\" Initialize the object. Parameters: hidden_layer_sizes: Number of perceptrons per layer. learning_rate: Step size of the learning adjustment. loss: Loss function. epochs: Number of arbitrary cutoffs, generally defined as \"one pass over the entire dataset\", used to separate training into distinct phases, which is useful for logging and periodic evaluation. nn_batch_size: Typically the batch_size. A batch is a set of nn_batch_size samples. The samples in a batch are processed independently, in parallel. If training, a batch results in only one update to the model. metrics: List of evaluate metrics. callbacks: Within Keras, there is the ability to add callbacks specifically designed to be run at the end of an epoch. Examples of these are learning rate changes and model checkpointing (saving). validation_split: Split size of validation set. batch_normalization: Batch normalization applies a transformation that maintains the mean output close to 0 and the output standard deviation close to 1. verbosity: The level of verbosity, 0 is least talkative and gives only warn and error, 1 gives adds info and 2 adds debug. dropout_rate: A Dropout layer applies random dropout and rescales the output. In inference mode, the same layer does nothing. Float -> added behind each layer List -> Same size as hidden_layer_size activations: Activation function. optimizer: Optimization algorithm. \"\"\" self . _loss = \"\" self . _multi_class = None self . loss = loss self . epochs = epochs self . nn_batch_size = nn_batch_size self . validation_split = validation_split if callbacks : self . callbacks = callbacks else : self . callbacks = [] if not metrics : metrics = [ 'mean_squared_error' ] super ( KerasDnnRegressor , self ) . __init__ ( hidden_layer_sizes = hidden_layer_sizes , target_activation = \"linear\" , target_dimension = 1 , learning_rate = learning_rate , loss = loss , metrics = metrics , batch_normalization = batch_normalization , verbosity = verbosity , dropout_rate = dropout_rate , activations = activations , optimizer = optimizer )","title":"__init__()"},{"location":"api/modelwrapper/keras/dnn_regressor/#photonai.modelwrapper.keras_dnn_regressor.KerasDnnRegressor.fit","text":"Starting the learning. Parameters: Name Type Description Default X ndarray The input samples with shape [n_samples, n_features]. required y ndarray The input targets with shape [n_samples, 1]. required Source code in photonai/modelwrapper/keras_dnn_regressor.py def fit ( self , X : np . ndarray , y : np . ndarray ): \"\"\" Starting the learning. Parameters: X: The input samples with shape [n_samples, n_features]. y: The input targets with shape [n_samples, 1]. \"\"\" self . create_model ( X . shape [ 1 ]) super ( KerasDnnBaseModel , self ) . fit ( X , y ) return self","title":"fit()"},{"location":"api/optimization/grid_search/","text":"Documentation for GridSearchOptimizer Grid search optimizer. Searches for the best configuration by iteratively testing a grid of possible hyperparameter combinations. Examples: 1 2 3 4 5 my_pipe = Hyperpipe ( name = 'grid_based_pipe' , optimizer = 'grid_search' , ... ) my_pipe . fit ( X , y ) Source code in photonai/optimization/grid_search/grid_search.py class GridSearchOptimizer ( PhotonSlaveOptimizer ): \"\"\"Grid search optimizer. Searches for the best configuration by iteratively testing a grid of possible hyperparameter combinations. Example: ``` python my_pipe = Hyperpipe(name='grid_based_pipe', optimizer='grid_search', ... ) my_pipe.fit(X, y) ``` \"\"\" def __init__ ( self ): \"\"\"Initialize the object.\"\"\" self . param_grid = [] self . pipeline_elements = None self . parameter_iterable = None self . ask = self . next_config_generator () def prepare ( self , pipeline_elements : list , maximize_metric : bool ) -> None : \"\"\" Creates a grid from a list of PipelineElements. Hyperparameters can be accessed via pipe_element.hyperparameters. Parameters: pipeline_elements: List of all PipelineElements to create the hyperparameter space. maximize_metric: Boolean to distinguish between score and error. \"\"\" self . pipeline_elements = pipeline_elements self . ask = self . next_config_generator () self . param_grid = create_global_config_grid ( self . pipeline_elements ) logger . info ( \"Grid Search generated \" + str ( len ( self . param_grid )) + \" configurations\" ) def next_config_generator ( self ) -> Generator : \"\"\" Generator for new configs - ask method. Returns: Yields the next config. \"\"\" for parameters in self . param_grid : yield parameters __init__ ( self ) special Initialize the object. Source code in photonai/optimization/grid_search/grid_search.py def __init__ ( self ): \"\"\"Initialize the object.\"\"\" self . param_grid = [] self . pipeline_elements = None self . parameter_iterable = None self . ask = self . next_config_generator () next_config_generator ( self ) Generator for new configs - ask method. Returns: Type Description Generator Yields the next config. Source code in photonai/optimization/grid_search/grid_search.py def next_config_generator ( self ) -> Generator : \"\"\" Generator for new configs - ask method. Returns: Yields the next config. \"\"\" for parameters in self . param_grid : yield parameters prepare ( self , pipeline_elements , maximize_metric ) Creates a grid from a list of PipelineElements. Hyperparameters can be accessed via pipe_element.hyperparameters. Parameters: Name Type Description Default pipeline_elements list List of all PipelineElements to create the hyperparameter space. required maximize_metric bool Boolean to distinguish between score and error. required Source code in photonai/optimization/grid_search/grid_search.py def prepare ( self , pipeline_elements : list , maximize_metric : bool ) -> None : \"\"\" Creates a grid from a list of PipelineElements. Hyperparameters can be accessed via pipe_element.hyperparameters. Parameters: pipeline_elements: List of all PipelineElements to create the hyperparameter space. maximize_metric: Boolean to distinguish between score and error. \"\"\" self . pipeline_elements = pipeline_elements self . ask = self . next_config_generator () self . param_grid = create_global_config_grid ( self . pipeline_elements ) logger . info ( \"Grid Search generated \" + str ( len ( self . param_grid )) + \" configurations\" )","title":"GridSearch"},{"location":"api/optimization/grid_search/#documentation-for-gridsearchoptimizer","text":"Grid search optimizer. Searches for the best configuration by iteratively testing a grid of possible hyperparameter combinations. Examples: 1 2 3 4 5 my_pipe = Hyperpipe ( name = 'grid_based_pipe' , optimizer = 'grid_search' , ... ) my_pipe . fit ( X , y ) Source code in photonai/optimization/grid_search/grid_search.py class GridSearchOptimizer ( PhotonSlaveOptimizer ): \"\"\"Grid search optimizer. Searches for the best configuration by iteratively testing a grid of possible hyperparameter combinations. Example: ``` python my_pipe = Hyperpipe(name='grid_based_pipe', optimizer='grid_search', ... ) my_pipe.fit(X, y) ``` \"\"\" def __init__ ( self ): \"\"\"Initialize the object.\"\"\" self . param_grid = [] self . pipeline_elements = None self . parameter_iterable = None self . ask = self . next_config_generator () def prepare ( self , pipeline_elements : list , maximize_metric : bool ) -> None : \"\"\" Creates a grid from a list of PipelineElements. Hyperparameters can be accessed via pipe_element.hyperparameters. Parameters: pipeline_elements: List of all PipelineElements to create the hyperparameter space. maximize_metric: Boolean to distinguish between score and error. \"\"\" self . pipeline_elements = pipeline_elements self . ask = self . next_config_generator () self . param_grid = create_global_config_grid ( self . pipeline_elements ) logger . info ( \"Grid Search generated \" + str ( len ( self . param_grid )) + \" configurations\" ) def next_config_generator ( self ) -> Generator : \"\"\" Generator for new configs - ask method. Returns: Yields the next config. \"\"\" for parameters in self . param_grid : yield parameters","title":"Documentation for GridSearchOptimizer"},{"location":"api/optimization/grid_search/#photonai.optimization.grid_search.grid_search.GridSearchOptimizer.__init__","text":"Initialize the object. Source code in photonai/optimization/grid_search/grid_search.py def __init__ ( self ): \"\"\"Initialize the object.\"\"\" self . param_grid = [] self . pipeline_elements = None self . parameter_iterable = None self . ask = self . next_config_generator ()","title":"__init__()"},{"location":"api/optimization/grid_search/#photonai.optimization.grid_search.grid_search.GridSearchOptimizer.next_config_generator","text":"Generator for new configs - ask method. Returns: Type Description Generator Yields the next config. Source code in photonai/optimization/grid_search/grid_search.py def next_config_generator ( self ) -> Generator : \"\"\" Generator for new configs - ask method. Returns: Yields the next config. \"\"\" for parameters in self . param_grid : yield parameters","title":"next_config_generator()"},{"location":"api/optimization/grid_search/#photonai.optimization.grid_search.grid_search.GridSearchOptimizer.prepare","text":"Creates a grid from a list of PipelineElements. Hyperparameters can be accessed via pipe_element.hyperparameters. Parameters: Name Type Description Default pipeline_elements list List of all PipelineElements to create the hyperparameter space. required maximize_metric bool Boolean to distinguish between score and error. required Source code in photonai/optimization/grid_search/grid_search.py def prepare ( self , pipeline_elements : list , maximize_metric : bool ) -> None : \"\"\" Creates a grid from a list of PipelineElements. Hyperparameters can be accessed via pipe_element.hyperparameters. Parameters: pipeline_elements: List of all PipelineElements to create the hyperparameter space. maximize_metric: Boolean to distinguish between score and error. \"\"\" self . pipeline_elements = pipeline_elements self . ask = self . next_config_generator () self . param_grid = create_global_config_grid ( self . pipeline_elements ) logger . info ( \"Grid Search generated \" + str ( len ( self . param_grid )) + \" configurations\" )","title":"prepare()"},{"location":"api/optimization/nevergrad/","text":"Documentation for NevergradOptimizer Nevergrad Wrapper for PHOTONAI. Nevergrad is a gradient-free optimization platform. Nevergrad usage and implementation details . Examples: 1 2 3 4 5 6 7 8 9 import nevergrad as ng # list of all available nevergrad optimizer print ( list ( ng . optimizers . registry . values ())) my_pipe = Hyperpipe ( 'nevergrad_example' , optimizer = 'nevergrad' , optimizer_params = { 'facade' : 'NGO' , 'n_configurations' : 30 }, ... ) Source code in photonai/optimization/nevergrad/nevergrad.py class NevergradOptimizer ( PhotonMasterOptimizer ): \"\"\"Nevergrad Wrapper for PHOTONAI. Nevergrad is a gradient-free optimization platform. Nevergrad [usage and implementation details]( https://facebookresearch.github.io/nevergrad/). Example: ``` python import nevergrad as ng # list of all available nevergrad optimizer print(list(ng.optimizers.registry.values())) my_pipe = Hyperpipe('nevergrad_example', optimizer='nevergrad', optimizer_params={'facade': 'NGO', 'n_configurations': 30}, ... ) ``` \"\"\" def __init__ ( self , facade = 'NGO' , n_configurations : int = 100 , rng : int = 42 ): \"\"\" Initialize the object. Parameters: facade: Choice of the Nevergrad backend strategy, e.g. [NGO, ...]. n_configurations: Number of runs. rng: Random Seed. \"\"\" if not __found__ : msg = \"Module nevergrad not found or not installed as expected. \" \\ \"Please install the nevergrad/requirements.txt PHOTONAI provides.\" logger . error ( msg ) raise ModuleNotFoundError ( msg ) if facade in list ( ng . optimizers . registry . values ()): self . facade = facade elif facade in list ( ng . optimizers . registry . keys ()): self . facade = ng . optimizers . registry [ facade ] else : msg = \"nevergrad.optimizer {} not known. Check out all available nevergrad optimizers \" \\ \"by nevergrad.optimizers.registry.keys()\" . format ( str ( facade )) logger . error ( msg . format ( str ( facade ))) raise ValueError ( msg . format ( str ( facade ))) self . n_configurations = n_configurations self . space = None # Hyperparameter space for nevergrad self . switch_optiones = {} self . hyperparameters = [] self . rng = rng self . maximize_metric = False self . constant_dictionary = {} self . objective = None self . optimizer = None def prepare ( self , pipeline_elements : list , maximize_metric : bool , objective_function : Callable ) -> None : \"\"\"Prepare Nevergrad Optimizer. Parameters: pipeline_elements: List of all PipelineElements to create the hyperparameter space. maximize_metric: Boolean to distinguish between score and error. objective_function: The cost or objective function. \"\"\" self . space = self . _build_nevergrad_space ( pipeline_elements ) self . space . random_state . seed ( self . rng ) if self . constant_dictionary : msg = \"PHOTONAI has detected some one-valued params in your hyperparameters. Pleas use the kwargs for \" \\ \"constant values. This run ignores following settings: \" + str ( self . constant_dictionary . keys ()) logger . warning ( msg ) warnings . warn ( msg ) self . maximize_metric = maximize_metric def nevergrad_objective_function ( ** current_config ): return objective_function ( current_config ) self . objective = nevergrad_objective_function self . optimizer = self . facade ( parametrization = self . space , budget = self . n_configurations ) def optimize ( self ) -> None : self . optimizer . minimize ( self . objective ) def _build_nevergrad_space ( self , pipeline_elements : list ): \"\"\" Build entire Nevergrad hyperparameter space. Parameters: pipeline_elements: List of all pipeline_elements to create the hyperparameter space. \"\"\" param_dict = {} for pipe_element in pipeline_elements : # build conditions for switch elements if pipe_element . __class__ . __name__ == 'Switch' : raise NotImplementedError ( \"Currently PHOTONAIs Switch is not supported by nevergrad.\" ) if hasattr ( pipe_element , 'hyperparameters' ): for name , value in pipe_element . hyperparameters . items (): self . hyperparameters . append ( name ) # if we only have one value we do not need to optimize if isinstance ( value , list ) and len ( value ) < 2 : self . constant_dictionary [ name ] = value [ 0 ] continue if isinstance ( value , PhotonCategorical ) and len ( value . values ) < 2 : self . constant_dictionary [ name ] = value . values [ 0 ] continue nevergrad_param = self . _convert_photonai_to_nevergrad_param ( value ) if nevergrad_param is not None : param_dict [ name ] = nevergrad_param return ng . p . Instrumentation ( ** param_dict ) @staticmethod def _convert_photonai_to_nevergrad_param ( hyperparam : PhotonHyperparam ): \"\"\" Helper function: Convert PHOTONAI to Nevergrad hyperparameter. Parameters: hyperparam: One of photonai.optimization.hyperparameters. \"\"\" if isinstance ( hyperparam , PhotonCategorical ) or isinstance ( hyperparam , BooleanSwitch ): return ng . p . Choice ( hyperparam . values ) elif isinstance ( hyperparam , list ): return ng . p . Choice ( hyperparam ) elif isinstance ( hyperparam , FloatRange ): if hyperparam . range_type == 'linspace' : return ng . p . Scalar ( lower = hyperparam . start , upper = hyperparam . stop ) elif hyperparam . range_type == 'logspace' : return ng . p . Log ( lower = hyperparam . start , upper = hyperparam . stop ) msg = str ( hyperparam . range_type ) + \"in your float hyperparameter is not implemented yet.\" logger . error ( msg ) raise NotImplementedError ( msg ) elif isinstance ( hyperparam , IntegerRange ): return ng . p . Scalar ( lower = hyperparam . start , upper = hyperparam . stop ) . set_integer_casting () msg = \"Cannot convert hyperparameter \" + str ( hyperparam ) + \". Supported types: Categorical, IntegerRange,\" \\ \"FloatRange, list.\" logger . error ( msg ) raise ValueError ( msg ) __init__ ( self , facade = 'NGO' , n_configurations = 100 , rng = 42 ) special Initialize the object. Parameters: Name Type Description Default facade Choice of the Nevergrad backend strategy, e.g. [NGO, ...]. 'NGO' n_configurations int Number of runs. 100 rng int Random Seed. 42 Source code in photonai/optimization/nevergrad/nevergrad.py def __init__ ( self , facade = 'NGO' , n_configurations : int = 100 , rng : int = 42 ): \"\"\" Initialize the object. Parameters: facade: Choice of the Nevergrad backend strategy, e.g. [NGO, ...]. n_configurations: Number of runs. rng: Random Seed. \"\"\" if not __found__ : msg = \"Module nevergrad not found or not installed as expected. \" \\ \"Please install the nevergrad/requirements.txt PHOTONAI provides.\" logger . error ( msg ) raise ModuleNotFoundError ( msg ) if facade in list ( ng . optimizers . registry . values ()): self . facade = facade elif facade in list ( ng . optimizers . registry . keys ()): self . facade = ng . optimizers . registry [ facade ] else : msg = \"nevergrad.optimizer {} not known. Check out all available nevergrad optimizers \" \\ \"by nevergrad.optimizers.registry.keys()\" . format ( str ( facade )) logger . error ( msg . format ( str ( facade ))) raise ValueError ( msg . format ( str ( facade ))) self . n_configurations = n_configurations self . space = None # Hyperparameter space for nevergrad self . switch_optiones = {} self . hyperparameters = [] self . rng = rng self . maximize_metric = False self . constant_dictionary = {} self . objective = None self . optimizer = None optimize ( self ) Start the optimization process based on the underlying objective function. Source code in photonai/optimization/nevergrad/nevergrad.py def optimize ( self ) -> None : self . optimizer . minimize ( self . objective ) prepare ( self , pipeline_elements , maximize_metric , objective_function ) Prepare Nevergrad Optimizer. Parameters: Name Type Description Default pipeline_elements list List of all PipelineElements to create the hyperparameter space. required maximize_metric bool Boolean to distinguish between score and error. required objective_function Callable The cost or objective function. required Source code in photonai/optimization/nevergrad/nevergrad.py def prepare ( self , pipeline_elements : list , maximize_metric : bool , objective_function : Callable ) -> None : \"\"\"Prepare Nevergrad Optimizer. Parameters: pipeline_elements: List of all PipelineElements to create the hyperparameter space. maximize_metric: Boolean to distinguish between score and error. objective_function: The cost or objective function. \"\"\" self . space = self . _build_nevergrad_space ( pipeline_elements ) self . space . random_state . seed ( self . rng ) if self . constant_dictionary : msg = \"PHOTONAI has detected some one-valued params in your hyperparameters. Pleas use the kwargs for \" \\ \"constant values. This run ignores following settings: \" + str ( self . constant_dictionary . keys ()) logger . warning ( msg ) warnings . warn ( msg ) self . maximize_metric = maximize_metric def nevergrad_objective_function ( ** current_config ): return objective_function ( current_config ) self . objective = nevergrad_objective_function self . optimizer = self . facade ( parametrization = self . space , budget = self . n_configurations )","title":"Nevergrad"},{"location":"api/optimization/nevergrad/#documentation-for-nevergradoptimizer","text":"Nevergrad Wrapper for PHOTONAI. Nevergrad is a gradient-free optimization platform. Nevergrad usage and implementation details . Examples: 1 2 3 4 5 6 7 8 9 import nevergrad as ng # list of all available nevergrad optimizer print ( list ( ng . optimizers . registry . values ())) my_pipe = Hyperpipe ( 'nevergrad_example' , optimizer = 'nevergrad' , optimizer_params = { 'facade' : 'NGO' , 'n_configurations' : 30 }, ... ) Source code in photonai/optimization/nevergrad/nevergrad.py class NevergradOptimizer ( PhotonMasterOptimizer ): \"\"\"Nevergrad Wrapper for PHOTONAI. Nevergrad is a gradient-free optimization platform. Nevergrad [usage and implementation details]( https://facebookresearch.github.io/nevergrad/). Example: ``` python import nevergrad as ng # list of all available nevergrad optimizer print(list(ng.optimizers.registry.values())) my_pipe = Hyperpipe('nevergrad_example', optimizer='nevergrad', optimizer_params={'facade': 'NGO', 'n_configurations': 30}, ... ) ``` \"\"\" def __init__ ( self , facade = 'NGO' , n_configurations : int = 100 , rng : int = 42 ): \"\"\" Initialize the object. Parameters: facade: Choice of the Nevergrad backend strategy, e.g. [NGO, ...]. n_configurations: Number of runs. rng: Random Seed. \"\"\" if not __found__ : msg = \"Module nevergrad not found or not installed as expected. \" \\ \"Please install the nevergrad/requirements.txt PHOTONAI provides.\" logger . error ( msg ) raise ModuleNotFoundError ( msg ) if facade in list ( ng . optimizers . registry . values ()): self . facade = facade elif facade in list ( ng . optimizers . registry . keys ()): self . facade = ng . optimizers . registry [ facade ] else : msg = \"nevergrad.optimizer {} not known. Check out all available nevergrad optimizers \" \\ \"by nevergrad.optimizers.registry.keys()\" . format ( str ( facade )) logger . error ( msg . format ( str ( facade ))) raise ValueError ( msg . format ( str ( facade ))) self . n_configurations = n_configurations self . space = None # Hyperparameter space for nevergrad self . switch_optiones = {} self . hyperparameters = [] self . rng = rng self . maximize_metric = False self . constant_dictionary = {} self . objective = None self . optimizer = None def prepare ( self , pipeline_elements : list , maximize_metric : bool , objective_function : Callable ) -> None : \"\"\"Prepare Nevergrad Optimizer. Parameters: pipeline_elements: List of all PipelineElements to create the hyperparameter space. maximize_metric: Boolean to distinguish between score and error. objective_function: The cost or objective function. \"\"\" self . space = self . _build_nevergrad_space ( pipeline_elements ) self . space . random_state . seed ( self . rng ) if self . constant_dictionary : msg = \"PHOTONAI has detected some one-valued params in your hyperparameters. Pleas use the kwargs for \" \\ \"constant values. This run ignores following settings: \" + str ( self . constant_dictionary . keys ()) logger . warning ( msg ) warnings . warn ( msg ) self . maximize_metric = maximize_metric def nevergrad_objective_function ( ** current_config ): return objective_function ( current_config ) self . objective = nevergrad_objective_function self . optimizer = self . facade ( parametrization = self . space , budget = self . n_configurations ) def optimize ( self ) -> None : self . optimizer . minimize ( self . objective ) def _build_nevergrad_space ( self , pipeline_elements : list ): \"\"\" Build entire Nevergrad hyperparameter space. Parameters: pipeline_elements: List of all pipeline_elements to create the hyperparameter space. \"\"\" param_dict = {} for pipe_element in pipeline_elements : # build conditions for switch elements if pipe_element . __class__ . __name__ == 'Switch' : raise NotImplementedError ( \"Currently PHOTONAIs Switch is not supported by nevergrad.\" ) if hasattr ( pipe_element , 'hyperparameters' ): for name , value in pipe_element . hyperparameters . items (): self . hyperparameters . append ( name ) # if we only have one value we do not need to optimize if isinstance ( value , list ) and len ( value ) < 2 : self . constant_dictionary [ name ] = value [ 0 ] continue if isinstance ( value , PhotonCategorical ) and len ( value . values ) < 2 : self . constant_dictionary [ name ] = value . values [ 0 ] continue nevergrad_param = self . _convert_photonai_to_nevergrad_param ( value ) if nevergrad_param is not None : param_dict [ name ] = nevergrad_param return ng . p . Instrumentation ( ** param_dict ) @staticmethod def _convert_photonai_to_nevergrad_param ( hyperparam : PhotonHyperparam ): \"\"\" Helper function: Convert PHOTONAI to Nevergrad hyperparameter. Parameters: hyperparam: One of photonai.optimization.hyperparameters. \"\"\" if isinstance ( hyperparam , PhotonCategorical ) or isinstance ( hyperparam , BooleanSwitch ): return ng . p . Choice ( hyperparam . values ) elif isinstance ( hyperparam , list ): return ng . p . Choice ( hyperparam ) elif isinstance ( hyperparam , FloatRange ): if hyperparam . range_type == 'linspace' : return ng . p . Scalar ( lower = hyperparam . start , upper = hyperparam . stop ) elif hyperparam . range_type == 'logspace' : return ng . p . Log ( lower = hyperparam . start , upper = hyperparam . stop ) msg = str ( hyperparam . range_type ) + \"in your float hyperparameter is not implemented yet.\" logger . error ( msg ) raise NotImplementedError ( msg ) elif isinstance ( hyperparam , IntegerRange ): return ng . p . Scalar ( lower = hyperparam . start , upper = hyperparam . stop ) . set_integer_casting () msg = \"Cannot convert hyperparameter \" + str ( hyperparam ) + \". Supported types: Categorical, IntegerRange,\" \\ \"FloatRange, list.\" logger . error ( msg ) raise ValueError ( msg )","title":"Documentation for NevergradOptimizer"},{"location":"api/optimization/nevergrad/#photonai.optimization.nevergrad.nevergrad.NevergradOptimizer.__init__","text":"Initialize the object. Parameters: Name Type Description Default facade Choice of the Nevergrad backend strategy, e.g. [NGO, ...]. 'NGO' n_configurations int Number of runs. 100 rng int Random Seed. 42 Source code in photonai/optimization/nevergrad/nevergrad.py def __init__ ( self , facade = 'NGO' , n_configurations : int = 100 , rng : int = 42 ): \"\"\" Initialize the object. Parameters: facade: Choice of the Nevergrad backend strategy, e.g. [NGO, ...]. n_configurations: Number of runs. rng: Random Seed. \"\"\" if not __found__ : msg = \"Module nevergrad not found or not installed as expected. \" \\ \"Please install the nevergrad/requirements.txt PHOTONAI provides.\" logger . error ( msg ) raise ModuleNotFoundError ( msg ) if facade in list ( ng . optimizers . registry . values ()): self . facade = facade elif facade in list ( ng . optimizers . registry . keys ()): self . facade = ng . optimizers . registry [ facade ] else : msg = \"nevergrad.optimizer {} not known. Check out all available nevergrad optimizers \" \\ \"by nevergrad.optimizers.registry.keys()\" . format ( str ( facade )) logger . error ( msg . format ( str ( facade ))) raise ValueError ( msg . format ( str ( facade ))) self . n_configurations = n_configurations self . space = None # Hyperparameter space for nevergrad self . switch_optiones = {} self . hyperparameters = [] self . rng = rng self . maximize_metric = False self . constant_dictionary = {} self . objective = None self . optimizer = None","title":"__init__()"},{"location":"api/optimization/nevergrad/#photonai.optimization.nevergrad.nevergrad.NevergradOptimizer.optimize","text":"Start the optimization process based on the underlying objective function. Source code in photonai/optimization/nevergrad/nevergrad.py def optimize ( self ) -> None : self . optimizer . minimize ( self . objective )","title":"optimize()"},{"location":"api/optimization/nevergrad/#photonai.optimization.nevergrad.nevergrad.NevergradOptimizer.prepare","text":"Prepare Nevergrad Optimizer. Parameters: Name Type Description Default pipeline_elements list List of all PipelineElements to create the hyperparameter space. required maximize_metric bool Boolean to distinguish between score and error. required objective_function Callable The cost or objective function. required Source code in photonai/optimization/nevergrad/nevergrad.py def prepare ( self , pipeline_elements : list , maximize_metric : bool , objective_function : Callable ) -> None : \"\"\"Prepare Nevergrad Optimizer. Parameters: pipeline_elements: List of all PipelineElements to create the hyperparameter space. maximize_metric: Boolean to distinguish between score and error. objective_function: The cost or objective function. \"\"\" self . space = self . _build_nevergrad_space ( pipeline_elements ) self . space . random_state . seed ( self . rng ) if self . constant_dictionary : msg = \"PHOTONAI has detected some one-valued params in your hyperparameters. Pleas use the kwargs for \" \\ \"constant values. This run ignores following settings: \" + str ( self . constant_dictionary . keys ()) logger . warning ( msg ) warnings . warn ( msg ) self . maximize_metric = maximize_metric def nevergrad_objective_function ( ** current_config ): return objective_function ( current_config ) self . objective = nevergrad_objective_function self . optimizer = self . facade ( parametrization = self . space , budget = self . n_configurations )","title":"prepare()"},{"location":"api/optimization/random_grid_search/","text":"Documentation for RandomGridSearchOptimizer Random grid search optimizer. Searches for the best configuration by randomly testing n points of a grid of possible hyperparameters. Examples: 1 2 3 4 5 6 7 my_pipe = Hyperpipe ( name = 'rgrid_based_pipe' , optimizer = 'random_grid_search' , optimizer_params = { 'n_configurations' : 50 , 'limit_in_minutes' : 10 }, ... ) my_pipe . fit ( X , y ) Source code in photonai/optimization/grid_search/grid_search.py class RandomGridSearchOptimizer ( GridSearchOptimizer ): \"\"\"Random grid search optimizer. Searches for the best configuration by randomly testing n points of a grid of possible hyperparameters. Example: ``` python my_pipe = Hyperpipe(name='rgrid_based_pipe', optimizer='random_grid_search', optimizer_params={'n_configurations': 50, 'limit_in_minutes': 10}, ... ) my_pipe.fit(X, y) ``` \"\"\" def __init__ ( self , limit_in_minutes : Union [ float , None ] = None , n_configurations : Union [ int , None ] = 25 ): \"\"\" Initialize the object. Parameters: limit_in_minutes: Total time in minutes. n_configurations: Number of configurations to be calculated. \"\"\" super ( RandomGridSearchOptimizer , self ) . __init__ () self . _k = n_configurations self . n_configurations = self . _k self . limit_in_minutes = limit_in_minutes self . start_time , self . end_time = None , None def prepare ( self , pipeline_elements : list , maximize_metric : bool ) -> None : \"\"\" Prepare hyperparameter search. Parameters: pipeline_elements: List of all PipelineElements to create the hyperparameter space. maximize_metric: Boolean to distinguish between score and error. \"\"\" super ( RandomGridSearchOptimizer , self ) . prepare ( pipeline_elements , maximize_metric ) self . start_time = None self . n_configurations = self . _k self . param_grid = list ( self . param_grid ) # create random order in list np . random . shuffle ( self . param_grid ) if self . n_configurations is not None : # k is maximal all grid items if self . n_configurations > len ( self . param_grid ): self . n_configurations = len ( self . param_grid ) self . param_grid = self . param_grid [ 0 : self . n_configurations ] def next_config_generator ( self ) -> Generator : \"\"\" Generator for new configs - ask method. Returns: Yields the next config. \"\"\" if self . start_time is None and self . limit_in_minutes is not None : self . start_time = datetime . datetime . now () self . end_time = self . start_time + datetime . timedelta ( minutes = self . limit_in_minutes ) for parameters in super ( RandomGridSearchOptimizer , self ) . next_config_generator (): if self . limit_in_minutes is None or datetime . datetime . now () < self . end_time : yield parameters __init__ ( self , limit_in_minutes = None , n_configurations = 25 ) special Initialize the object. Parameters: Name Type Description Default limit_in_minutes Optional[float] Total time in minutes. None n_configurations Optional[int] Number of configurations to be calculated. 25 Source code in photonai/optimization/grid_search/grid_search.py def __init__ ( self , limit_in_minutes : Union [ float , None ] = None , n_configurations : Union [ int , None ] = 25 ): \"\"\" Initialize the object. Parameters: limit_in_minutes: Total time in minutes. n_configurations: Number of configurations to be calculated. \"\"\" super ( RandomGridSearchOptimizer , self ) . __init__ () self . _k = n_configurations self . n_configurations = self . _k self . limit_in_minutes = limit_in_minutes self . start_time , self . end_time = None , None next_config_generator ( self ) Generator for new configs - ask method. Returns: Type Description Generator Yields the next config. Source code in photonai/optimization/grid_search/grid_search.py def next_config_generator ( self ) -> Generator : \"\"\" Generator for new configs - ask method. Returns: Yields the next config. \"\"\" if self . start_time is None and self . limit_in_minutes is not None : self . start_time = datetime . datetime . now () self . end_time = self . start_time + datetime . timedelta ( minutes = self . limit_in_minutes ) for parameters in super ( RandomGridSearchOptimizer , self ) . next_config_generator (): if self . limit_in_minutes is None or datetime . datetime . now () < self . end_time : yield parameters prepare ( self , pipeline_elements , maximize_metric ) Prepare hyperparameter search. Parameters: Name Type Description Default pipeline_elements list List of all PipelineElements to create the hyperparameter space. required maximize_metric bool Boolean to distinguish between score and error. required Source code in photonai/optimization/grid_search/grid_search.py def prepare ( self , pipeline_elements : list , maximize_metric : bool ) -> None : \"\"\" Prepare hyperparameter search. Parameters: pipeline_elements: List of all PipelineElements to create the hyperparameter space. maximize_metric: Boolean to distinguish between score and error. \"\"\" super ( RandomGridSearchOptimizer , self ) . prepare ( pipeline_elements , maximize_metric ) self . start_time = None self . n_configurations = self . _k self . param_grid = list ( self . param_grid ) # create random order in list np . random . shuffle ( self . param_grid ) if self . n_configurations is not None : # k is maximal all grid items if self . n_configurations > len ( self . param_grid ): self . n_configurations = len ( self . param_grid ) self . param_grid = self . param_grid [ 0 : self . n_configurations ]","title":"RandomGridSearch"},{"location":"api/optimization/random_grid_search/#documentation-for-randomgridsearchoptimizer","text":"Random grid search optimizer. Searches for the best configuration by randomly testing n points of a grid of possible hyperparameters. Examples: 1 2 3 4 5 6 7 my_pipe = Hyperpipe ( name = 'rgrid_based_pipe' , optimizer = 'random_grid_search' , optimizer_params = { 'n_configurations' : 50 , 'limit_in_minutes' : 10 }, ... ) my_pipe . fit ( X , y ) Source code in photonai/optimization/grid_search/grid_search.py class RandomGridSearchOptimizer ( GridSearchOptimizer ): \"\"\"Random grid search optimizer. Searches for the best configuration by randomly testing n points of a grid of possible hyperparameters. Example: ``` python my_pipe = Hyperpipe(name='rgrid_based_pipe', optimizer='random_grid_search', optimizer_params={'n_configurations': 50, 'limit_in_minutes': 10}, ... ) my_pipe.fit(X, y) ``` \"\"\" def __init__ ( self , limit_in_minutes : Union [ float , None ] = None , n_configurations : Union [ int , None ] = 25 ): \"\"\" Initialize the object. Parameters: limit_in_minutes: Total time in minutes. n_configurations: Number of configurations to be calculated. \"\"\" super ( RandomGridSearchOptimizer , self ) . __init__ () self . _k = n_configurations self . n_configurations = self . _k self . limit_in_minutes = limit_in_minutes self . start_time , self . end_time = None , None def prepare ( self , pipeline_elements : list , maximize_metric : bool ) -> None : \"\"\" Prepare hyperparameter search. Parameters: pipeline_elements: List of all PipelineElements to create the hyperparameter space. maximize_metric: Boolean to distinguish between score and error. \"\"\" super ( RandomGridSearchOptimizer , self ) . prepare ( pipeline_elements , maximize_metric ) self . start_time = None self . n_configurations = self . _k self . param_grid = list ( self . param_grid ) # create random order in list np . random . shuffle ( self . param_grid ) if self . n_configurations is not None : # k is maximal all grid items if self . n_configurations > len ( self . param_grid ): self . n_configurations = len ( self . param_grid ) self . param_grid = self . param_grid [ 0 : self . n_configurations ] def next_config_generator ( self ) -> Generator : \"\"\" Generator for new configs - ask method. Returns: Yields the next config. \"\"\" if self . start_time is None and self . limit_in_minutes is not None : self . start_time = datetime . datetime . now () self . end_time = self . start_time + datetime . timedelta ( minutes = self . limit_in_minutes ) for parameters in super ( RandomGridSearchOptimizer , self ) . next_config_generator (): if self . limit_in_minutes is None or datetime . datetime . now () < self . end_time : yield parameters","title":"Documentation for RandomGridSearchOptimizer"},{"location":"api/optimization/random_grid_search/#photonai.optimization.grid_search.grid_search.RandomGridSearchOptimizer.__init__","text":"Initialize the object. Parameters: Name Type Description Default limit_in_minutes Optional[float] Total time in minutes. None n_configurations Optional[int] Number of configurations to be calculated. 25 Source code in photonai/optimization/grid_search/grid_search.py def __init__ ( self , limit_in_minutes : Union [ float , None ] = None , n_configurations : Union [ int , None ] = 25 ): \"\"\" Initialize the object. Parameters: limit_in_minutes: Total time in minutes. n_configurations: Number of configurations to be calculated. \"\"\" super ( RandomGridSearchOptimizer , self ) . __init__ () self . _k = n_configurations self . n_configurations = self . _k self . limit_in_minutes = limit_in_minutes self . start_time , self . end_time = None , None","title":"__init__()"},{"location":"api/optimization/random_grid_search/#photonai.optimization.grid_search.grid_search.RandomGridSearchOptimizer.next_config_generator","text":"Generator for new configs - ask method. Returns: Type Description Generator Yields the next config. Source code in photonai/optimization/grid_search/grid_search.py def next_config_generator ( self ) -> Generator : \"\"\" Generator for new configs - ask method. Returns: Yields the next config. \"\"\" if self . start_time is None and self . limit_in_minutes is not None : self . start_time = datetime . datetime . now () self . end_time = self . start_time + datetime . timedelta ( minutes = self . limit_in_minutes ) for parameters in super ( RandomGridSearchOptimizer , self ) . next_config_generator (): if self . limit_in_minutes is None or datetime . datetime . now () < self . end_time : yield parameters","title":"next_config_generator()"},{"location":"api/optimization/random_grid_search/#photonai.optimization.grid_search.grid_search.RandomGridSearchOptimizer.prepare","text":"Prepare hyperparameter search. Parameters: Name Type Description Default pipeline_elements list List of all PipelineElements to create the hyperparameter space. required maximize_metric bool Boolean to distinguish between score and error. required Source code in photonai/optimization/grid_search/grid_search.py def prepare ( self , pipeline_elements : list , maximize_metric : bool ) -> None : \"\"\" Prepare hyperparameter search. Parameters: pipeline_elements: List of all PipelineElements to create the hyperparameter space. maximize_metric: Boolean to distinguish between score and error. \"\"\" super ( RandomGridSearchOptimizer , self ) . prepare ( pipeline_elements , maximize_metric ) self . start_time = None self . n_configurations = self . _k self . param_grid = list ( self . param_grid ) # create random order in list np . random . shuffle ( self . param_grid ) if self . n_configurations is not None : # k is maximal all grid items if self . n_configurations > len ( self . param_grid ): self . n_configurations = len ( self . param_grid ) self . param_grid = self . param_grid [ 0 : self . n_configurations ]","title":"prepare()"},{"location":"api/optimization/random_search/","text":"Documentation for RandomSearchOptimizer Random search optimizer. Searches for the best configuration by randomly testing hyperparameter combinations without any grid. Source code in photonai/optimization/random_search/random_search.py class RandomSearchOptimizer ( PhotonSlaveOptimizer ): \"\"\"Random search optimizer. Searches for the best configuration by randomly testing hyperparameter combinations without any grid. \"\"\" def __init__ ( self , limit_in_minutes : Union [ float , None ] = None , n_configurations : Union [ int , None ] = 10 ): \"\"\" Initialize the object. One of limit_in_minutes or n_configurations must differ from None. Parameters: limit_in_minutes: Total time in minutes. n_configurations: Number of configurations to be calculated. \"\"\" self . pipeline_elements = None self . parameter_iterable = None self . ask = self . next_config_generator () self . n_configurations = None if not limit_in_minutes or limit_in_minutes <= 0 : self . limit_in_minutes = None else : self . limit_in_minutes = limit_in_minutes self . start_time = None self . end_time = None if not n_configurations or n_configurations <= 0 : self . n_configurations = None else : self . n_configurations = n_configurations self . k_configutration = 0 # use k++ until k==n: break if self . n_configurations is None and self . limit_in_minutes is None : msg = \"No stopping criteria for RandomSearchOptimizer.\" logger . error ( msg ) raise ValueError ( msg ) def prepare ( self , pipeline_elements : list , maximize_metric : bool ) -> None : \"\"\" Initializes grid free random hyperparameter search. Parameters: pipeline_elements: List of all PipelineElements to create the hyperparameter space. maximize_metric: Boolean to distinguish between score and error. \"\"\" self . start_time = None self . pipeline_elements = pipeline_elements self . ask = self . next_config_generator () def next_config_generator ( self ) -> Generator : \"\"\" Generator for new configs - ask method. Returns: Yields the next config. \"\"\" while True : self . k_configutration += 1 new_config = True if self . limit_in_minutes : if self . start_time is None : self . start_time = datetime . datetime . now () self . end_time = self . start_time + datetime . timedelta ( minutes = self . limit_in_minutes ) if datetime . datetime . now () >= self . end_time : new_config = False if self . n_configurations : if self . k_configutration >= self . n_configurations + 1 : new_config = False if not new_config : return _ = ( yield self . _generate_config ()) def _generate_config ( self ): config = {} for p_element in self . pipeline_elements : for h_key , h_value in p_element . hyperparameters . items (): if isinstance ( h_value , list ): config [ h_key ] = random . choice ( h_value ) else : config [ h_key ] = h_value . get_random_value () return config __init__ ( self , limit_in_minutes = None , n_configurations = 10 ) special Initialize the object. One of limit_in_minutes or n_configurations must differ from None. Parameters: Name Type Description Default limit_in_minutes Optional[float] Total time in minutes. None n_configurations Optional[int] Number of configurations to be calculated. 10 Source code in photonai/optimization/random_search/random_search.py def __init__ ( self , limit_in_minutes : Union [ float , None ] = None , n_configurations : Union [ int , None ] = 10 ): \"\"\" Initialize the object. One of limit_in_minutes or n_configurations must differ from None. Parameters: limit_in_minutes: Total time in minutes. n_configurations: Number of configurations to be calculated. \"\"\" self . pipeline_elements = None self . parameter_iterable = None self . ask = self . next_config_generator () self . n_configurations = None if not limit_in_minutes or limit_in_minutes <= 0 : self . limit_in_minutes = None else : self . limit_in_minutes = limit_in_minutes self . start_time = None self . end_time = None if not n_configurations or n_configurations <= 0 : self . n_configurations = None else : self . n_configurations = n_configurations self . k_configutration = 0 # use k++ until k==n: break if self . n_configurations is None and self . limit_in_minutes is None : msg = \"No stopping criteria for RandomSearchOptimizer.\" logger . error ( msg ) raise ValueError ( msg ) next_config_generator ( self ) Generator for new configs - ask method. Returns: Type Description Generator Yields the next config. Source code in photonai/optimization/random_search/random_search.py def next_config_generator ( self ) -> Generator : \"\"\" Generator for new configs - ask method. Returns: Yields the next config. \"\"\" while True : self . k_configutration += 1 new_config = True if self . limit_in_minutes : if self . start_time is None : self . start_time = datetime . datetime . now () self . end_time = self . start_time + datetime . timedelta ( minutes = self . limit_in_minutes ) if datetime . datetime . now () >= self . end_time : new_config = False if self . n_configurations : if self . k_configutration >= self . n_configurations + 1 : new_config = False if not new_config : return _ = ( yield self . _generate_config ()) prepare ( self , pipeline_elements , maximize_metric ) Initializes grid free random hyperparameter search. Parameters: Name Type Description Default pipeline_elements list List of all PipelineElements to create the hyperparameter space. required maximize_metric bool Boolean to distinguish between score and error. required Source code in photonai/optimization/random_search/random_search.py def prepare ( self , pipeline_elements : list , maximize_metric : bool ) -> None : \"\"\" Initializes grid free random hyperparameter search. Parameters: pipeline_elements: List of all PipelineElements to create the hyperparameter space. maximize_metric: Boolean to distinguish between score and error. \"\"\" self . start_time = None self . pipeline_elements = pipeline_elements self . ask = self . next_config_generator ()","title":"RandomSearch"},{"location":"api/optimization/random_search/#documentation-for-randomsearchoptimizer","text":"Random search optimizer. Searches for the best configuration by randomly testing hyperparameter combinations without any grid. Source code in photonai/optimization/random_search/random_search.py class RandomSearchOptimizer ( PhotonSlaveOptimizer ): \"\"\"Random search optimizer. Searches for the best configuration by randomly testing hyperparameter combinations without any grid. \"\"\" def __init__ ( self , limit_in_minutes : Union [ float , None ] = None , n_configurations : Union [ int , None ] = 10 ): \"\"\" Initialize the object. One of limit_in_minutes or n_configurations must differ from None. Parameters: limit_in_minutes: Total time in minutes. n_configurations: Number of configurations to be calculated. \"\"\" self . pipeline_elements = None self . parameter_iterable = None self . ask = self . next_config_generator () self . n_configurations = None if not limit_in_minutes or limit_in_minutes <= 0 : self . limit_in_minutes = None else : self . limit_in_minutes = limit_in_minutes self . start_time = None self . end_time = None if not n_configurations or n_configurations <= 0 : self . n_configurations = None else : self . n_configurations = n_configurations self . k_configutration = 0 # use k++ until k==n: break if self . n_configurations is None and self . limit_in_minutes is None : msg = \"No stopping criteria for RandomSearchOptimizer.\" logger . error ( msg ) raise ValueError ( msg ) def prepare ( self , pipeline_elements : list , maximize_metric : bool ) -> None : \"\"\" Initializes grid free random hyperparameter search. Parameters: pipeline_elements: List of all PipelineElements to create the hyperparameter space. maximize_metric: Boolean to distinguish between score and error. \"\"\" self . start_time = None self . pipeline_elements = pipeline_elements self . ask = self . next_config_generator () def next_config_generator ( self ) -> Generator : \"\"\" Generator for new configs - ask method. Returns: Yields the next config. \"\"\" while True : self . k_configutration += 1 new_config = True if self . limit_in_minutes : if self . start_time is None : self . start_time = datetime . datetime . now () self . end_time = self . start_time + datetime . timedelta ( minutes = self . limit_in_minutes ) if datetime . datetime . now () >= self . end_time : new_config = False if self . n_configurations : if self . k_configutration >= self . n_configurations + 1 : new_config = False if not new_config : return _ = ( yield self . _generate_config ()) def _generate_config ( self ): config = {} for p_element in self . pipeline_elements : for h_key , h_value in p_element . hyperparameters . items (): if isinstance ( h_value , list ): config [ h_key ] = random . choice ( h_value ) else : config [ h_key ] = h_value . get_random_value () return config","title":"Documentation for RandomSearchOptimizer"},{"location":"api/optimization/random_search/#photonai.optimization.random_search.random_search.RandomSearchOptimizer.__init__","text":"Initialize the object. One of limit_in_minutes or n_configurations must differ from None. Parameters: Name Type Description Default limit_in_minutes Optional[float] Total time in minutes. None n_configurations Optional[int] Number of configurations to be calculated. 10 Source code in photonai/optimization/random_search/random_search.py def __init__ ( self , limit_in_minutes : Union [ float , None ] = None , n_configurations : Union [ int , None ] = 10 ): \"\"\" Initialize the object. One of limit_in_minutes or n_configurations must differ from None. Parameters: limit_in_minutes: Total time in minutes. n_configurations: Number of configurations to be calculated. \"\"\" self . pipeline_elements = None self . parameter_iterable = None self . ask = self . next_config_generator () self . n_configurations = None if not limit_in_minutes or limit_in_minutes <= 0 : self . limit_in_minutes = None else : self . limit_in_minutes = limit_in_minutes self . start_time = None self . end_time = None if not n_configurations or n_configurations <= 0 : self . n_configurations = None else : self . n_configurations = n_configurations self . k_configutration = 0 # use k++ until k==n: break if self . n_configurations is None and self . limit_in_minutes is None : msg = \"No stopping criteria for RandomSearchOptimizer.\" logger . error ( msg ) raise ValueError ( msg )","title":"__init__()"},{"location":"api/optimization/random_search/#photonai.optimization.random_search.random_search.RandomSearchOptimizer.next_config_generator","text":"Generator for new configs - ask method. Returns: Type Description Generator Yields the next config. Source code in photonai/optimization/random_search/random_search.py def next_config_generator ( self ) -> Generator : \"\"\" Generator for new configs - ask method. Returns: Yields the next config. \"\"\" while True : self . k_configutration += 1 new_config = True if self . limit_in_minutes : if self . start_time is None : self . start_time = datetime . datetime . now () self . end_time = self . start_time + datetime . timedelta ( minutes = self . limit_in_minutes ) if datetime . datetime . now () >= self . end_time : new_config = False if self . n_configurations : if self . k_configutration >= self . n_configurations + 1 : new_config = False if not new_config : return _ = ( yield self . _generate_config ())","title":"next_config_generator()"},{"location":"api/optimization/random_search/#photonai.optimization.random_search.random_search.RandomSearchOptimizer.prepare","text":"Initializes grid free random hyperparameter search. Parameters: Name Type Description Default pipeline_elements list List of all PipelineElements to create the hyperparameter space. required maximize_metric bool Boolean to distinguish between score and error. required Source code in photonai/optimization/random_search/random_search.py def prepare ( self , pipeline_elements : list , maximize_metric : bool ) -> None : \"\"\" Initializes grid free random hyperparameter search. Parameters: pipeline_elements: List of all PipelineElements to create the hyperparameter space. maximize_metric: Boolean to distinguish between score and error. \"\"\" self . start_time = None self . pipeline_elements = pipeline_elements self . ask = self . next_config_generator ()","title":"prepare()"},{"location":"api/optimization/skopt/","text":"Documentation for SkOptOptimizer Wrapper for Scikit-Optimize with PHOTONAI. Scikit-Optimize, or skopt, is a simple and efficient library to minimize (very) expensive and noisy black-box functions. It implements several methods for sequential model-based optimization. skopt aims to be accessible and easy to use in many contexts. Scikit-optimize usage and implementation details A detailed parameter documentation here. Examples: 1 2 3 4 5 6 my_pipe = Hyperpipe ( 'skopt_example' , optimizer = 'sk_opt' , optimizer_params = { 'n_configurations' : 25 , 'acq_func' : 'LCB' , 'acq_func_kwargs' : { 'kappa' : 1.96 }}, ... ) Source code in photonai/optimization/scikit_optimize/sk_opt.py class SkOptOptimizer ( PhotonSlaveOptimizer ): \"\"\"Wrapper for Scikit-Optimize with PHOTONAI. Scikit-Optimize, or skopt, is a simple and efficient library to minimize (very) expensive and noisy black-box functions. It implements several methods for sequential model-based optimization. skopt aims to be accessible and easy to use in many contexts. Scikit-optimize [usage and implementation details](https://scikit-optimize.github.io/stable/) A detailed parameter documentation [here.]( https://scikit-optimize.github.io/stable/modules/generated/skopt.optimizer.Optimizer.html#skopt.optimizer.Optimizer) Example: ``` python my_pipe = Hyperpipe('skopt_example', optimizer='sk_opt', optimizer_params={'n_configurations': 25, 'acq_func': 'LCB', 'acq_func_kwargs': {'kappa': 1.96}}, ...) ``` \"\"\" def __init__ ( self , n_configurations : int = 20 , n_initial_points : int = 10 , limit_in_minutes : Union [ float , None ] = None , base_estimator : Union [ str , sklearn . base . RegressorMixin ] = \"ET\" , initial_point_generator : str = \"random\" , acq_func : str = 'gp_hedge' , acq_func_kwargs : dict = None ): \"\"\" Initialize the object. Parameters: n_configurations: Number of configurations to be calculated. n_initial_points: Number of evaluations with initialization points before approximating it with `base_estimator`. limit_in_minutes: Total time in minutes. base_estimator: Estimator for returning std(Y | x) along with E[Y | x]. initial_point_generator: Generator for initial points. acq_func: Function to minimize over the posterior distribution. acq_func_kwargs: Additional arguments to be passed to the acquisition function. \"\"\" self . metric_to_optimize = '' self . n_configurations = n_configurations self . n_initial_points = n_initial_points self . base_estimator = base_estimator self . initial_point_generator = initial_point_generator self . acq_func = acq_func self . acq_func_kwargs = acq_func_kwargs self . limit_in_minutes = limit_in_minutes self . start_time , self . end_time = None , None self . optimizer = None self . maximize_metric = None self . hyperparameter_list = [] self . constant_dictionary = {} self . ask = self . ask_generator () def ask_generator ( self ) -> Generator : \"\"\" Generator for new configs - ask method. Returns: Yields the next config. \"\"\" if self . start_time is None and self . limit_in_minutes is not None : self . start_time = datetime . datetime . now () self . end_time = self . start_time + datetime . timedelta ( minutes = self . limit_in_minutes ) if self . optimizer is None : yield {} else : for i in range ( self . n_configurations ): next_config_list = self . optimizer . ask () next_config_dict = { self . hyperparameter_list [ number ]: self . _convert_to_native ( value ) for number , value in enumerate ( next_config_list )} if self . limit_in_minutes is None or datetime . datetime . now () < self . end_time : yield next_config_dict else : yield {} break def prepare ( self , pipeline_elements : list , maximize_metric : bool ) -> None : \"\"\" Initializes hyperparameter search with scikit-optimize. Assembles all hyperparameters of the list of PipelineElements in order to prepare the hyperparameter space. Hyperparameters can be accessed via pipe_element.hyperparameters. Parameters: pipeline_elements: List of all PipelineElements to create the hyperparameter space. maximize_metric: Boolean to distinguish between score and error. \"\"\" self . start_time = None self . optimizer = None self . hyperparameter_list = [] self . maximize_metric = maximize_metric # build skopt space space = [] for pipe_element in pipeline_elements : if pipe_element . __class__ . __name__ == 'Switch' : error_msg = 'Scikit-Optimize cannot operate in the specified hyperparameter space with a Switch ' \\ 'element. We recommend the use of SMAC.' logger . error ( error_msg ) raise ValueError ( error_msg ) if hasattr ( pipe_element , 'hyperparameters' ): for name , value in pipe_element . hyperparameters . items (): # if we only have one value we do not need to optimize if isinstance ( value , list ) and len ( value ) < 2 : self . constant_dictionary [ name ] = value [ 0 ] continue if isinstance ( value , PhotonCategorical ) and len ( value . values ) < 2 : self . constant_dictionary [ name ] = value . values [ 0 ] continue skopt_param = self . _convert_photonai_to_skopt_space ( value , name ) if skopt_param is not None : space . append ( skopt_param ) if self . constant_dictionary : msg = \"PHOTONAI has detected some one-valued params in your hyperparameters. Pleas use the kwargs for \" \\ \"constant values. This run ignores following settings: \" + str ( self . constant_dictionary . keys ()) logger . warning ( msg ) warnings . warn ( msg ) if len ( space ) == 0 : msg = \"Did not find any hyperparameter to convert into skopt space.\" logger . warning ( msg ) warnings . warn ( msg ) else : self . optimizer = Optimizer ( space , base_estimator = self . base_estimator , n_initial_points = self . n_initial_points , initial_point_generator = self . initial_point_generator , acq_func = self . acq_func , acq_func_kwargs = self . acq_func_kwargs ) self . ask = self . ask_generator () def tell ( self , config : dict , performance : float ) -> None : \"\"\" Provide a config result to calculate new ones. Parameters: config: The configuration that has been trained and tested. performance: Metrics about the configuration's generalization capabilities. \"\"\" # convert dictionary to list in correct order if self . optimizer is not None : config_values = [ config [ name ] for name in self . hyperparameter_list ] best_config_metric_performance = performance if self . maximize_metric : best_config_metric_performance = - best_config_metric_performance self . optimizer . tell ( config_values , best_config_metric_performance ) def _convert_photonai_to_skopt_space ( self , hyperparam : Union [ PhotonHyperparam , list ], name : str ) -> Dimension : self . hyperparameter_list . append ( name ) if isinstance ( hyperparam , PhotonCategorical ) or isinstance ( hyperparam , BooleanSwitch ): return skoptCategorical ( hyperparam . values , name = name ) elif isinstance ( hyperparam , list ): return skoptCategorical ( hyperparam , name = name ) elif isinstance ( hyperparam , FloatRange ): if hyperparam . range_type == 'linspace' : return Real ( hyperparam . start , hyperparam . stop , name = name , prior = 'uniform' ) elif hyperparam . range_type == 'logspace' : return Real ( hyperparam . start , hyperparam . stop , name = name , prior = 'log-uniform' ) else : msg = \"The hyperparam.range_type \" + hyperparam . range_type + \" is not supported by scikit-optimize.\" logger . error ( msg ) raise ValueError ( msg ) elif isinstance ( hyperparam , IntegerRange ): return Integer ( hyperparam . start , hyperparam . stop , name = name ) msg = \"Cannot convert hyperparameter \" + str ( hyperparam ) + \". \" \\ \"Supported types: Categorical, IntegerRange, FloatRange, list.\" logger . error ( msg ) raise ValueError ( msg ) @staticmethod def _convert_to_native ( obj ): # check if we have a numpy object, if so convert it to python native if type ( obj ) . __module__ == np . __name__ : return obj . item () else : return obj __init__ ( self , n_configurations = 20 , n_initial_points = 10 , limit_in_minutes = None , base_estimator = 'ET' , initial_point_generator = 'random' , acq_func = 'gp_hedge' , acq_func_kwargs = None ) special Initialize the object. Parameters: Name Type Description Default n_configurations int Number of configurations to be calculated. 20 n_initial_points int Number of evaluations with initialization points before approximating it with base_estimator . 10 limit_in_minutes Optional[float] Total time in minutes. None base_estimator Union[str, sklearn.base.RegressorMixin] Estimator for returning std(Y | x) along with E[Y | x]. 'ET' initial_point_generator str Generator for initial points. 'random' acq_func str Function to minimize over the posterior distribution. 'gp_hedge' acq_func_kwargs dict Additional arguments to be passed to the acquisition function. None Source code in photonai/optimization/scikit_optimize/sk_opt.py def __init__ ( self , n_configurations : int = 20 , n_initial_points : int = 10 , limit_in_minutes : Union [ float , None ] = None , base_estimator : Union [ str , sklearn . base . RegressorMixin ] = \"ET\" , initial_point_generator : str = \"random\" , acq_func : str = 'gp_hedge' , acq_func_kwargs : dict = None ): \"\"\" Initialize the object. Parameters: n_configurations: Number of configurations to be calculated. n_initial_points: Number of evaluations with initialization points before approximating it with `base_estimator`. limit_in_minutes: Total time in minutes. base_estimator: Estimator for returning std(Y | x) along with E[Y | x]. initial_point_generator: Generator for initial points. acq_func: Function to minimize over the posterior distribution. acq_func_kwargs: Additional arguments to be passed to the acquisition function. \"\"\" self . metric_to_optimize = '' self . n_configurations = n_configurations self . n_initial_points = n_initial_points self . base_estimator = base_estimator self . initial_point_generator = initial_point_generator self . acq_func = acq_func self . acq_func_kwargs = acq_func_kwargs self . limit_in_minutes = limit_in_minutes self . start_time , self . end_time = None , None self . optimizer = None self . maximize_metric = None self . hyperparameter_list = [] self . constant_dictionary = {} self . ask = self . ask_generator () prepare ( self , pipeline_elements , maximize_metric ) Initializes hyperparameter search with scikit-optimize. Assembles all hyperparameters of the list of PipelineElements in order to prepare the hyperparameter space. Hyperparameters can be accessed via pipe_element.hyperparameters. Parameters: Name Type Description Default pipeline_elements list List of all PipelineElements to create the hyperparameter space. required maximize_metric bool Boolean to distinguish between score and error. required Source code in photonai/optimization/scikit_optimize/sk_opt.py def prepare ( self , pipeline_elements : list , maximize_metric : bool ) -> None : \"\"\" Initializes hyperparameter search with scikit-optimize. Assembles all hyperparameters of the list of PipelineElements in order to prepare the hyperparameter space. Hyperparameters can be accessed via pipe_element.hyperparameters. Parameters: pipeline_elements: List of all PipelineElements to create the hyperparameter space. maximize_metric: Boolean to distinguish between score and error. \"\"\" self . start_time = None self . optimizer = None self . hyperparameter_list = [] self . maximize_metric = maximize_metric # build skopt space space = [] for pipe_element in pipeline_elements : if pipe_element . __class__ . __name__ == 'Switch' : error_msg = 'Scikit-Optimize cannot operate in the specified hyperparameter space with a Switch ' \\ 'element. We recommend the use of SMAC.' logger . error ( error_msg ) raise ValueError ( error_msg ) if hasattr ( pipe_element , 'hyperparameters' ): for name , value in pipe_element . hyperparameters . items (): # if we only have one value we do not need to optimize if isinstance ( value , list ) and len ( value ) < 2 : self . constant_dictionary [ name ] = value [ 0 ] continue if isinstance ( value , PhotonCategorical ) and len ( value . values ) < 2 : self . constant_dictionary [ name ] = value . values [ 0 ] continue skopt_param = self . _convert_photonai_to_skopt_space ( value , name ) if skopt_param is not None : space . append ( skopt_param ) if self . constant_dictionary : msg = \"PHOTONAI has detected some one-valued params in your hyperparameters. Pleas use the kwargs for \" \\ \"constant values. This run ignores following settings: \" + str ( self . constant_dictionary . keys ()) logger . warning ( msg ) warnings . warn ( msg ) if len ( space ) == 0 : msg = \"Did not find any hyperparameter to convert into skopt space.\" logger . warning ( msg ) warnings . warn ( msg ) else : self . optimizer = Optimizer ( space , base_estimator = self . base_estimator , n_initial_points = self . n_initial_points , initial_point_generator = self . initial_point_generator , acq_func = self . acq_func , acq_func_kwargs = self . acq_func_kwargs ) self . ask = self . ask_generator () tell ( self , config , performance ) Provide a config result to calculate new ones. Parameters: Name Type Description Default config dict The configuration that has been trained and tested. required performance float Metrics about the configuration's generalization capabilities. required Source code in photonai/optimization/scikit_optimize/sk_opt.py def tell ( self , config : dict , performance : float ) -> None : \"\"\" Provide a config result to calculate new ones. Parameters: config: The configuration that has been trained and tested. performance: Metrics about the configuration's generalization capabilities. \"\"\" # convert dictionary to list in correct order if self . optimizer is not None : config_values = [ config [ name ] for name in self . hyperparameter_list ] best_config_metric_performance = performance if self . maximize_metric : best_config_metric_performance = - best_config_metric_performance self . optimizer . tell ( config_values , best_config_metric_performance )","title":"Scikit-Optimize"},{"location":"api/optimization/skopt/#documentation-for-skoptoptimizer","text":"Wrapper for Scikit-Optimize with PHOTONAI. Scikit-Optimize, or skopt, is a simple and efficient library to minimize (very) expensive and noisy black-box functions. It implements several methods for sequential model-based optimization. skopt aims to be accessible and easy to use in many contexts. Scikit-optimize usage and implementation details A detailed parameter documentation here. Examples: 1 2 3 4 5 6 my_pipe = Hyperpipe ( 'skopt_example' , optimizer = 'sk_opt' , optimizer_params = { 'n_configurations' : 25 , 'acq_func' : 'LCB' , 'acq_func_kwargs' : { 'kappa' : 1.96 }}, ... ) Source code in photonai/optimization/scikit_optimize/sk_opt.py class SkOptOptimizer ( PhotonSlaveOptimizer ): \"\"\"Wrapper for Scikit-Optimize with PHOTONAI. Scikit-Optimize, or skopt, is a simple and efficient library to minimize (very) expensive and noisy black-box functions. It implements several methods for sequential model-based optimization. skopt aims to be accessible and easy to use in many contexts. Scikit-optimize [usage and implementation details](https://scikit-optimize.github.io/stable/) A detailed parameter documentation [here.]( https://scikit-optimize.github.io/stable/modules/generated/skopt.optimizer.Optimizer.html#skopt.optimizer.Optimizer) Example: ``` python my_pipe = Hyperpipe('skopt_example', optimizer='sk_opt', optimizer_params={'n_configurations': 25, 'acq_func': 'LCB', 'acq_func_kwargs': {'kappa': 1.96}}, ...) ``` \"\"\" def __init__ ( self , n_configurations : int = 20 , n_initial_points : int = 10 , limit_in_minutes : Union [ float , None ] = None , base_estimator : Union [ str , sklearn . base . RegressorMixin ] = \"ET\" , initial_point_generator : str = \"random\" , acq_func : str = 'gp_hedge' , acq_func_kwargs : dict = None ): \"\"\" Initialize the object. Parameters: n_configurations: Number of configurations to be calculated. n_initial_points: Number of evaluations with initialization points before approximating it with `base_estimator`. limit_in_minutes: Total time in minutes. base_estimator: Estimator for returning std(Y | x) along with E[Y | x]. initial_point_generator: Generator for initial points. acq_func: Function to minimize over the posterior distribution. acq_func_kwargs: Additional arguments to be passed to the acquisition function. \"\"\" self . metric_to_optimize = '' self . n_configurations = n_configurations self . n_initial_points = n_initial_points self . base_estimator = base_estimator self . initial_point_generator = initial_point_generator self . acq_func = acq_func self . acq_func_kwargs = acq_func_kwargs self . limit_in_minutes = limit_in_minutes self . start_time , self . end_time = None , None self . optimizer = None self . maximize_metric = None self . hyperparameter_list = [] self . constant_dictionary = {} self . ask = self . ask_generator () def ask_generator ( self ) -> Generator : \"\"\" Generator for new configs - ask method. Returns: Yields the next config. \"\"\" if self . start_time is None and self . limit_in_minutes is not None : self . start_time = datetime . datetime . now () self . end_time = self . start_time + datetime . timedelta ( minutes = self . limit_in_minutes ) if self . optimizer is None : yield {} else : for i in range ( self . n_configurations ): next_config_list = self . optimizer . ask () next_config_dict = { self . hyperparameter_list [ number ]: self . _convert_to_native ( value ) for number , value in enumerate ( next_config_list )} if self . limit_in_minutes is None or datetime . datetime . now () < self . end_time : yield next_config_dict else : yield {} break def prepare ( self , pipeline_elements : list , maximize_metric : bool ) -> None : \"\"\" Initializes hyperparameter search with scikit-optimize. Assembles all hyperparameters of the list of PipelineElements in order to prepare the hyperparameter space. Hyperparameters can be accessed via pipe_element.hyperparameters. Parameters: pipeline_elements: List of all PipelineElements to create the hyperparameter space. maximize_metric: Boolean to distinguish between score and error. \"\"\" self . start_time = None self . optimizer = None self . hyperparameter_list = [] self . maximize_metric = maximize_metric # build skopt space space = [] for pipe_element in pipeline_elements : if pipe_element . __class__ . __name__ == 'Switch' : error_msg = 'Scikit-Optimize cannot operate in the specified hyperparameter space with a Switch ' \\ 'element. We recommend the use of SMAC.' logger . error ( error_msg ) raise ValueError ( error_msg ) if hasattr ( pipe_element , 'hyperparameters' ): for name , value in pipe_element . hyperparameters . items (): # if we only have one value we do not need to optimize if isinstance ( value , list ) and len ( value ) < 2 : self . constant_dictionary [ name ] = value [ 0 ] continue if isinstance ( value , PhotonCategorical ) and len ( value . values ) < 2 : self . constant_dictionary [ name ] = value . values [ 0 ] continue skopt_param = self . _convert_photonai_to_skopt_space ( value , name ) if skopt_param is not None : space . append ( skopt_param ) if self . constant_dictionary : msg = \"PHOTONAI has detected some one-valued params in your hyperparameters. Pleas use the kwargs for \" \\ \"constant values. This run ignores following settings: \" + str ( self . constant_dictionary . keys ()) logger . warning ( msg ) warnings . warn ( msg ) if len ( space ) == 0 : msg = \"Did not find any hyperparameter to convert into skopt space.\" logger . warning ( msg ) warnings . warn ( msg ) else : self . optimizer = Optimizer ( space , base_estimator = self . base_estimator , n_initial_points = self . n_initial_points , initial_point_generator = self . initial_point_generator , acq_func = self . acq_func , acq_func_kwargs = self . acq_func_kwargs ) self . ask = self . ask_generator () def tell ( self , config : dict , performance : float ) -> None : \"\"\" Provide a config result to calculate new ones. Parameters: config: The configuration that has been trained and tested. performance: Metrics about the configuration's generalization capabilities. \"\"\" # convert dictionary to list in correct order if self . optimizer is not None : config_values = [ config [ name ] for name in self . hyperparameter_list ] best_config_metric_performance = performance if self . maximize_metric : best_config_metric_performance = - best_config_metric_performance self . optimizer . tell ( config_values , best_config_metric_performance ) def _convert_photonai_to_skopt_space ( self , hyperparam : Union [ PhotonHyperparam , list ], name : str ) -> Dimension : self . hyperparameter_list . append ( name ) if isinstance ( hyperparam , PhotonCategorical ) or isinstance ( hyperparam , BooleanSwitch ): return skoptCategorical ( hyperparam . values , name = name ) elif isinstance ( hyperparam , list ): return skoptCategorical ( hyperparam , name = name ) elif isinstance ( hyperparam , FloatRange ): if hyperparam . range_type == 'linspace' : return Real ( hyperparam . start , hyperparam . stop , name = name , prior = 'uniform' ) elif hyperparam . range_type == 'logspace' : return Real ( hyperparam . start , hyperparam . stop , name = name , prior = 'log-uniform' ) else : msg = \"The hyperparam.range_type \" + hyperparam . range_type + \" is not supported by scikit-optimize.\" logger . error ( msg ) raise ValueError ( msg ) elif isinstance ( hyperparam , IntegerRange ): return Integer ( hyperparam . start , hyperparam . stop , name = name ) msg = \"Cannot convert hyperparameter \" + str ( hyperparam ) + \". \" \\ \"Supported types: Categorical, IntegerRange, FloatRange, list.\" logger . error ( msg ) raise ValueError ( msg ) @staticmethod def _convert_to_native ( obj ): # check if we have a numpy object, if so convert it to python native if type ( obj ) . __module__ == np . __name__ : return obj . item () else : return obj","title":"Documentation for SkOptOptimizer"},{"location":"api/optimization/skopt/#photonai.optimization.scikit_optimize.sk_opt.SkOptOptimizer.__init__","text":"Initialize the object. Parameters: Name Type Description Default n_configurations int Number of configurations to be calculated. 20 n_initial_points int Number of evaluations with initialization points before approximating it with base_estimator . 10 limit_in_minutes Optional[float] Total time in minutes. None base_estimator Union[str, sklearn.base.RegressorMixin] Estimator for returning std(Y | x) along with E[Y | x]. 'ET' initial_point_generator str Generator for initial points. 'random' acq_func str Function to minimize over the posterior distribution. 'gp_hedge' acq_func_kwargs dict Additional arguments to be passed to the acquisition function. None Source code in photonai/optimization/scikit_optimize/sk_opt.py def __init__ ( self , n_configurations : int = 20 , n_initial_points : int = 10 , limit_in_minutes : Union [ float , None ] = None , base_estimator : Union [ str , sklearn . base . RegressorMixin ] = \"ET\" , initial_point_generator : str = \"random\" , acq_func : str = 'gp_hedge' , acq_func_kwargs : dict = None ): \"\"\" Initialize the object. Parameters: n_configurations: Number of configurations to be calculated. n_initial_points: Number of evaluations with initialization points before approximating it with `base_estimator`. limit_in_minutes: Total time in minutes. base_estimator: Estimator for returning std(Y | x) along with E[Y | x]. initial_point_generator: Generator for initial points. acq_func: Function to minimize over the posterior distribution. acq_func_kwargs: Additional arguments to be passed to the acquisition function. \"\"\" self . metric_to_optimize = '' self . n_configurations = n_configurations self . n_initial_points = n_initial_points self . base_estimator = base_estimator self . initial_point_generator = initial_point_generator self . acq_func = acq_func self . acq_func_kwargs = acq_func_kwargs self . limit_in_minutes = limit_in_minutes self . start_time , self . end_time = None , None self . optimizer = None self . maximize_metric = None self . hyperparameter_list = [] self . constant_dictionary = {} self . ask = self . ask_generator ()","title":"__init__()"},{"location":"api/optimization/skopt/#photonai.optimization.scikit_optimize.sk_opt.SkOptOptimizer.prepare","text":"Initializes hyperparameter search with scikit-optimize. Assembles all hyperparameters of the list of PipelineElements in order to prepare the hyperparameter space. Hyperparameters can be accessed via pipe_element.hyperparameters. Parameters: Name Type Description Default pipeline_elements list List of all PipelineElements to create the hyperparameter space. required maximize_metric bool Boolean to distinguish between score and error. required Source code in photonai/optimization/scikit_optimize/sk_opt.py def prepare ( self , pipeline_elements : list , maximize_metric : bool ) -> None : \"\"\" Initializes hyperparameter search with scikit-optimize. Assembles all hyperparameters of the list of PipelineElements in order to prepare the hyperparameter space. Hyperparameters can be accessed via pipe_element.hyperparameters. Parameters: pipeline_elements: List of all PipelineElements to create the hyperparameter space. maximize_metric: Boolean to distinguish between score and error. \"\"\" self . start_time = None self . optimizer = None self . hyperparameter_list = [] self . maximize_metric = maximize_metric # build skopt space space = [] for pipe_element in pipeline_elements : if pipe_element . __class__ . __name__ == 'Switch' : error_msg = 'Scikit-Optimize cannot operate in the specified hyperparameter space with a Switch ' \\ 'element. We recommend the use of SMAC.' logger . error ( error_msg ) raise ValueError ( error_msg ) if hasattr ( pipe_element , 'hyperparameters' ): for name , value in pipe_element . hyperparameters . items (): # if we only have one value we do not need to optimize if isinstance ( value , list ) and len ( value ) < 2 : self . constant_dictionary [ name ] = value [ 0 ] continue if isinstance ( value , PhotonCategorical ) and len ( value . values ) < 2 : self . constant_dictionary [ name ] = value . values [ 0 ] continue skopt_param = self . _convert_photonai_to_skopt_space ( value , name ) if skopt_param is not None : space . append ( skopt_param ) if self . constant_dictionary : msg = \"PHOTONAI has detected some one-valued params in your hyperparameters. Pleas use the kwargs for \" \\ \"constant values. This run ignores following settings: \" + str ( self . constant_dictionary . keys ()) logger . warning ( msg ) warnings . warn ( msg ) if len ( space ) == 0 : msg = \"Did not find any hyperparameter to convert into skopt space.\" logger . warning ( msg ) warnings . warn ( msg ) else : self . optimizer = Optimizer ( space , base_estimator = self . base_estimator , n_initial_points = self . n_initial_points , initial_point_generator = self . initial_point_generator , acq_func = self . acq_func , acq_func_kwargs = self . acq_func_kwargs ) self . ask = self . ask_generator ()","title":"prepare()"},{"location":"api/optimization/skopt/#photonai.optimization.scikit_optimize.sk_opt.SkOptOptimizer.tell","text":"Provide a config result to calculate new ones. Parameters: Name Type Description Default config dict The configuration that has been trained and tested. required performance float Metrics about the configuration's generalization capabilities. required Source code in photonai/optimization/scikit_optimize/sk_opt.py def tell ( self , config : dict , performance : float ) -> None : \"\"\" Provide a config result to calculate new ones. Parameters: config: The configuration that has been trained and tested. performance: Metrics about the configuration's generalization capabilities. \"\"\" # convert dictionary to list in correct order if self . optimizer is not None : config_values = [ config [ name ] for name in self . hyperparameter_list ] best_config_metric_performance = performance if self . maximize_metric : best_config_metric_performance = - best_config_metric_performance self . optimizer . tell ( config_values , best_config_metric_performance )","title":"tell()"},{"location":"api/optimization/smac/","text":"Documentation for SMACOptimizer SMAC Wrapper for PHOTONAI. SMAC (sequential model-based algorithm configuration) is a versatile tool for optimizing algorithm parameters. The main core consists of Bayesian Optimization in combination with an aggressive racing mechanism to efficiently decide which of two configurations performs better. SMAC usage and implementation details here . References: 1 2 3 Hutter, F. and Hoos, H. H. and Leyton-Brown, K. Sequential Model-Based Optimization for General Algorithm Configuration In: Proceedings of the conference on Learning and Intelligent OptimizatioN (LION 5) Examples: 1 2 3 4 5 6 my_pipe = Hyperpipe ( 'smac_example' , optimizer = 'smac' , optimizer_params = { \"facade\" : \"SMAC4BO\" , \"wallclock_limit\" : 60.0 * 10 , # seconds \"ta_run_limit\" : 100 }, # limit of configurations ... ) Source code in photonai/optimization/smac/smac.py class SMACOptimizer ( PhotonMasterOptimizer ): \"\"\"SMAC Wrapper for PHOTONAI. SMAC (sequential model-based algorithm configuration) is a versatile tool for optimizing algorithm parameters. The main core consists of Bayesian Optimization in combination with an aggressive racing mechanism to efficiently decide which of two configurations performs better. SMAC usage and implementation details [here]( https://automl.github.io/SMAC3/master/quickstart.html). References: Hutter, F. and Hoos, H. H. and Leyton-Brown, K. Sequential Model-Based Optimization for General Algorithm Configuration In: Proceedings of the conference on Learning and Intelligent OptimizatioN (LION 5) Example: ``` python my_pipe = Hyperpipe('smac_example', optimizer='smac', optimizer_params={\"facade\": \"SMAC4BO\", \"wallclock_limit\": 60.0*10, # seconds \"ta_run_limit\": 100}, # limit of configurations ...) ``` \"\"\" def __init__ ( self , facade = 'SMAC4HPO' , run_obj : str = \"quality\" , deterministic : str = \"true\" , wallclock_limit : float = 60.0 , intensifier_kwargs : dict = None , rng : int = 42 , ** kwargs ): \"\"\" Initialize the object. Parameters: facade: Choice of the SMAC backend strategy, [SMAC4BO, SMAC4HPO, SMAC4AC, BOHB4HPO]. run_obj: Defines the optimization metric. When optimizing runtime, cutoff_time is required as well. wallclock_limit: Maximum amount of wallclock-time used for optimization. deterministic: If true, SMAC assumes that the target function or algorithm is deterministic (the same static seed of 0 is always passed to the function/algorithm). If false, different random seeds are passed to the target function/algorithm. intensifier_kwargs: Dict for intensifier settings. rng: Random seed of SMAC.facade. **kwargs: All initial kwargs are passed to SMACs scenario. [List of all a vailable parameters]( https://automl.github.io/SMAC3/master/options.html#scenario). \"\"\" super ( SMACOptimizer , self ) . __init__ () if not __found__ : msg = \"Module smac not found or not installed as expected. \" \\ \"Please install the smac/requirements.txt PHOTONAI provides.\" logger . error ( msg ) raise ModuleNotFoundError ( msg ) self . run_obj = run_obj self . deterministic = deterministic self . wallclock_limit = wallclock_limit self . kwargs = kwargs if facade in [ \"SMAC4BO\" , SMAC4BO , \"SMAC4AC\" , SMAC4AC , \"SMAC4HPO\" , SMAC4HPO , \"BOHB4HPO\" , BOHB4HPO ]: if type ( facade ) == str : self . facade = eval ( facade ) else : self . facade = facade else : msg = \"SMAC.facade {} not known. Please use one of ['SMAC4BO', 'SMAC4AC', 'SMAC4HPO'].\" logger . error ( msg . format ( str ( facade ))) raise ValueError ( msg . format ( str ( facade ))) self . rng = rng if not intensifier_kwargs : self . intensifier_kwargs = {} else : self . intensifier_kwargs = intensifier_kwargs self . cspace = ConfigurationSpace () # hyperparameter space for SMAC self . switch_optiones = {} self . hyperparameters = [] self . maximize_metric = False self . constant_dictionary = {} def prepare ( self , pipeline_elements : list , maximize_metric : bool , objective_function : Callable ): \"\"\" Initializes the SMAC Optimizer. Parameters: pipeline_elements: List of all PipelineElements to create hyperparameter space. maximize_metric: Boolean to distinguish between score and error. objective_function: The cost or objective function. \"\"\" self . cspace = ConfigurationSpace () # build space self . _build_smac_space ( pipeline_elements ) if self . constant_dictionary : msg = \"PHOTONAI has detected some one-valued params in your hyperparameters. Pleas use the kwargs for \" \\ \"constant values. This run ignores following settings: \" + str ( self . constant_dictionary . keys ()) logger . warning ( msg ) warnings . warn ( msg ) self . maximize_metric = maximize_metric scenario_dict = self . kwargs scenario_dict . update ({ \"run_obj\" : self . run_obj , \"deterministic\" : self . deterministic , \"wallclock_limit\" : self . wallclock_limit , \"cs\" : self . cspace , \"limit_resources\" : False }) scenario = Scenario ( scenario_dict ) def smac_objective_function ( current_config ): current_config = { k : current_config [ k ] for k in current_config if ( current_config [ k ] and 'algos' not in k )} return objective_function ( current_config ) self . smac = self . facade ( scenario = scenario , intensifier_kwargs = self . intensifier_kwargs , rng = self . rng , tae_runner = smac_objective_function ) def optimize ( self ): \"\"\"Start optimization process.\"\"\" self . smac . optimize () def _build_smac_space ( self , pipeline_elements : list ): \"\"\" Build the entire SMAC hyperparameter space. Parameters: pipeline_elements: List of all PipelineElements to create the hyperparameter space. \"\"\" for pipe_element in pipeline_elements : # build conditions for switch elements if pipe_element . __class__ . __name__ == 'Switch' : algorithm_options = {} for algo in pipe_element . elements : algo_params = [] # hyper params corresponding to \"algo\" for name , value in algo . hyperparameters . items (): smac_param = self . _convert_photonai_to_smac_param ( value , ( pipe_element . name + \"__\" + name )) # or element.name__algo.name__name algo_params . append ( smac_param ) algorithm_options [( pipe_element . name + \"__\" + algo . name )] = algo_params algos = CategoricalHyperparameter ( pipe_element . name + \"__algos\" , choices = algorithm_options . keys ()) self . switch_optiones [ pipe_element . name + \"__algos\" ] = algorithm_options . keys () self . cspace . add_hyperparameter ( algos ) for algo , params in algorithm_options . items (): for param in params : cond = InCondition ( child = param , parent = algos , values = [ algo ]) self . cspace . add_hyperparameter ( param ) self . cspace . add_condition ( cond ) continue if hasattr ( pipe_element , 'hyperparameters' ): for name , value in pipe_element . hyperparameters . items (): self . hyperparameters . append ( name ) # if we only have one value we do not need to optimize if isinstance ( value , list ) and len ( value ) < 2 : self . constant_dictionary [ name ] = value [ 0 ] continue if isinstance ( value , PhotonCategorical ) and len ( value . values ) < 2 : self . constant_dictionary [ name ] = value . values [ 0 ] continue smac_param = self . _convert_photonai_to_smac_param ( value , name ) if smac_param is not None : self . cspace . add_hyperparameter ( smac_param ) @staticmethod def _convert_photonai_to_smac_param ( hyperparam : PhotonHyperparam , name : str ): \"\"\" Helper function: Convert PHOTONAI to SMAC hyperparameter. Parameters: hyperparam: One of photonai.optimization.hyperparameters. name: Name of hyperparameter. \"\"\" if isinstance ( hyperparam , PhotonCategorical ) or isinstance ( hyperparam , BooleanSwitch ): return CategoricalHyperparameter ( name , hyperparam . values ) elif isinstance ( hyperparam , list ): return CategoricalHyperparameter ( name , hyperparam ) elif isinstance ( hyperparam , FloatRange ): if hyperparam . range_type in [ 'linspace' , 'logspace' ]: return UniformFloatHyperparameter ( name , hyperparam . start , hyperparam . stop , log = ( hyperparam . range_type == 'logspace' )) msg = str ( hyperparam . range_type ) + \"in your FloatRange is not implemented in SMAC.\" logger . error ( msg ) raise NotImplementedError ( msg ) elif isinstance ( hyperparam , IntegerRange ): return UniformIntegerHyperparameter ( name , hyperparam . start , hyperparam . stop ) msg = \"Cannot convert hyperparameter \" + str ( hyperparam ) + \". Supported types: Categorical, IntegerRange,\" \\ \"FloatRange, list.\" logger . error ( msg ) raise ValueError ( msg ) __init__ ( self , facade = 'SMAC4HPO' , run_obj = 'quality' , deterministic = 'true' , wallclock_limit = 60.0 , intensifier_kwargs = None , rng = 42 , ** kwargs ) special Initialize the object. Parameters: Name Type Description Default facade Choice of the SMAC backend strategy, [SMAC4BO, SMAC4HPO, SMAC4AC, BOHB4HPO]. 'SMAC4HPO' run_obj str Defines the optimization metric. When optimizing runtime, cutoff_time is required as well. 'quality' wallclock_limit float Maximum amount of wallclock-time used for optimization. 60.0 deterministic str If true, SMAC assumes that the target function or algorithm is deterministic (the same static seed of 0 is always passed to the function/algorithm). If false, different random seeds are passed to the target function/algorithm. 'true' intensifier_kwargs dict Dict for intensifier settings. None rng int Random seed of SMAC.facade. 42 **kwargs All initial kwargs are passed to SMACs scenario. List of all a vailable parameters . {} Source code in photonai/optimization/smac/smac.py def __init__ ( self , facade = 'SMAC4HPO' , run_obj : str = \"quality\" , deterministic : str = \"true\" , wallclock_limit : float = 60.0 , intensifier_kwargs : dict = None , rng : int = 42 , ** kwargs ): \"\"\" Initialize the object. Parameters: facade: Choice of the SMAC backend strategy, [SMAC4BO, SMAC4HPO, SMAC4AC, BOHB4HPO]. run_obj: Defines the optimization metric. When optimizing runtime, cutoff_time is required as well. wallclock_limit: Maximum amount of wallclock-time used for optimization. deterministic: If true, SMAC assumes that the target function or algorithm is deterministic (the same static seed of 0 is always passed to the function/algorithm). If false, different random seeds are passed to the target function/algorithm. intensifier_kwargs: Dict for intensifier settings. rng: Random seed of SMAC.facade. **kwargs: All initial kwargs are passed to SMACs scenario. [List of all a vailable parameters]( https://automl.github.io/SMAC3/master/options.html#scenario). \"\"\" super ( SMACOptimizer , self ) . __init__ () if not __found__ : msg = \"Module smac not found or not installed as expected. \" \\ \"Please install the smac/requirements.txt PHOTONAI provides.\" logger . error ( msg ) raise ModuleNotFoundError ( msg ) self . run_obj = run_obj self . deterministic = deterministic self . wallclock_limit = wallclock_limit self . kwargs = kwargs if facade in [ \"SMAC4BO\" , SMAC4BO , \"SMAC4AC\" , SMAC4AC , \"SMAC4HPO\" , SMAC4HPO , \"BOHB4HPO\" , BOHB4HPO ]: if type ( facade ) == str : self . facade = eval ( facade ) else : self . facade = facade else : msg = \"SMAC.facade {} not known. Please use one of ['SMAC4BO', 'SMAC4AC', 'SMAC4HPO'].\" logger . error ( msg . format ( str ( facade ))) raise ValueError ( msg . format ( str ( facade ))) self . rng = rng if not intensifier_kwargs : self . intensifier_kwargs = {} else : self . intensifier_kwargs = intensifier_kwargs self . cspace = ConfigurationSpace () # hyperparameter space for SMAC self . switch_optiones = {} self . hyperparameters = [] self . maximize_metric = False self . constant_dictionary = {} optimize ( self ) Start optimization process. Source code in photonai/optimization/smac/smac.py def optimize ( self ): \"\"\"Start optimization process.\"\"\" self . smac . optimize () prepare ( self , pipeline_elements , maximize_metric , objective_function ) Initializes the SMAC Optimizer. Parameters: Name Type Description Default pipeline_elements list List of all PipelineElements to create hyperparameter space. required maximize_metric bool Boolean to distinguish between score and error. required objective_function Callable The cost or objective function. required Source code in photonai/optimization/smac/smac.py def prepare ( self , pipeline_elements : list , maximize_metric : bool , objective_function : Callable ): \"\"\" Initializes the SMAC Optimizer. Parameters: pipeline_elements: List of all PipelineElements to create hyperparameter space. maximize_metric: Boolean to distinguish between score and error. objective_function: The cost or objective function. \"\"\" self . cspace = ConfigurationSpace () # build space self . _build_smac_space ( pipeline_elements ) if self . constant_dictionary : msg = \"PHOTONAI has detected some one-valued params in your hyperparameters. Pleas use the kwargs for \" \\ \"constant values. This run ignores following settings: \" + str ( self . constant_dictionary . keys ()) logger . warning ( msg ) warnings . warn ( msg ) self . maximize_metric = maximize_metric scenario_dict = self . kwargs scenario_dict . update ({ \"run_obj\" : self . run_obj , \"deterministic\" : self . deterministic , \"wallclock_limit\" : self . wallclock_limit , \"cs\" : self . cspace , \"limit_resources\" : False }) scenario = Scenario ( scenario_dict ) def smac_objective_function ( current_config ): current_config = { k : current_config [ k ] for k in current_config if ( current_config [ k ] and 'algos' not in k )} return objective_function ( current_config ) self . smac = self . facade ( scenario = scenario , intensifier_kwargs = self . intensifier_kwargs , rng = self . rng , tae_runner = smac_objective_function )","title":"SMAC"},{"location":"api/optimization/smac/#documentation-for-smacoptimizer","text":"SMAC Wrapper for PHOTONAI. SMAC (sequential model-based algorithm configuration) is a versatile tool for optimizing algorithm parameters. The main core consists of Bayesian Optimization in combination with an aggressive racing mechanism to efficiently decide which of two configurations performs better. SMAC usage and implementation details here . References: 1 2 3 Hutter, F. and Hoos, H. H. and Leyton-Brown, K. Sequential Model-Based Optimization for General Algorithm Configuration In: Proceedings of the conference on Learning and Intelligent OptimizatioN (LION 5) Examples: 1 2 3 4 5 6 my_pipe = Hyperpipe ( 'smac_example' , optimizer = 'smac' , optimizer_params = { \"facade\" : \"SMAC4BO\" , \"wallclock_limit\" : 60.0 * 10 , # seconds \"ta_run_limit\" : 100 }, # limit of configurations ... ) Source code in photonai/optimization/smac/smac.py class SMACOptimizer ( PhotonMasterOptimizer ): \"\"\"SMAC Wrapper for PHOTONAI. SMAC (sequential model-based algorithm configuration) is a versatile tool for optimizing algorithm parameters. The main core consists of Bayesian Optimization in combination with an aggressive racing mechanism to efficiently decide which of two configurations performs better. SMAC usage and implementation details [here]( https://automl.github.io/SMAC3/master/quickstart.html). References: Hutter, F. and Hoos, H. H. and Leyton-Brown, K. Sequential Model-Based Optimization for General Algorithm Configuration In: Proceedings of the conference on Learning and Intelligent OptimizatioN (LION 5) Example: ``` python my_pipe = Hyperpipe('smac_example', optimizer='smac', optimizer_params={\"facade\": \"SMAC4BO\", \"wallclock_limit\": 60.0*10, # seconds \"ta_run_limit\": 100}, # limit of configurations ...) ``` \"\"\" def __init__ ( self , facade = 'SMAC4HPO' , run_obj : str = \"quality\" , deterministic : str = \"true\" , wallclock_limit : float = 60.0 , intensifier_kwargs : dict = None , rng : int = 42 , ** kwargs ): \"\"\" Initialize the object. Parameters: facade: Choice of the SMAC backend strategy, [SMAC4BO, SMAC4HPO, SMAC4AC, BOHB4HPO]. run_obj: Defines the optimization metric. When optimizing runtime, cutoff_time is required as well. wallclock_limit: Maximum amount of wallclock-time used for optimization. deterministic: If true, SMAC assumes that the target function or algorithm is deterministic (the same static seed of 0 is always passed to the function/algorithm). If false, different random seeds are passed to the target function/algorithm. intensifier_kwargs: Dict for intensifier settings. rng: Random seed of SMAC.facade. **kwargs: All initial kwargs are passed to SMACs scenario. [List of all a vailable parameters]( https://automl.github.io/SMAC3/master/options.html#scenario). \"\"\" super ( SMACOptimizer , self ) . __init__ () if not __found__ : msg = \"Module smac not found or not installed as expected. \" \\ \"Please install the smac/requirements.txt PHOTONAI provides.\" logger . error ( msg ) raise ModuleNotFoundError ( msg ) self . run_obj = run_obj self . deterministic = deterministic self . wallclock_limit = wallclock_limit self . kwargs = kwargs if facade in [ \"SMAC4BO\" , SMAC4BO , \"SMAC4AC\" , SMAC4AC , \"SMAC4HPO\" , SMAC4HPO , \"BOHB4HPO\" , BOHB4HPO ]: if type ( facade ) == str : self . facade = eval ( facade ) else : self . facade = facade else : msg = \"SMAC.facade {} not known. Please use one of ['SMAC4BO', 'SMAC4AC', 'SMAC4HPO'].\" logger . error ( msg . format ( str ( facade ))) raise ValueError ( msg . format ( str ( facade ))) self . rng = rng if not intensifier_kwargs : self . intensifier_kwargs = {} else : self . intensifier_kwargs = intensifier_kwargs self . cspace = ConfigurationSpace () # hyperparameter space for SMAC self . switch_optiones = {} self . hyperparameters = [] self . maximize_metric = False self . constant_dictionary = {} def prepare ( self , pipeline_elements : list , maximize_metric : bool , objective_function : Callable ): \"\"\" Initializes the SMAC Optimizer. Parameters: pipeline_elements: List of all PipelineElements to create hyperparameter space. maximize_metric: Boolean to distinguish between score and error. objective_function: The cost or objective function. \"\"\" self . cspace = ConfigurationSpace () # build space self . _build_smac_space ( pipeline_elements ) if self . constant_dictionary : msg = \"PHOTONAI has detected some one-valued params in your hyperparameters. Pleas use the kwargs for \" \\ \"constant values. This run ignores following settings: \" + str ( self . constant_dictionary . keys ()) logger . warning ( msg ) warnings . warn ( msg ) self . maximize_metric = maximize_metric scenario_dict = self . kwargs scenario_dict . update ({ \"run_obj\" : self . run_obj , \"deterministic\" : self . deterministic , \"wallclock_limit\" : self . wallclock_limit , \"cs\" : self . cspace , \"limit_resources\" : False }) scenario = Scenario ( scenario_dict ) def smac_objective_function ( current_config ): current_config = { k : current_config [ k ] for k in current_config if ( current_config [ k ] and 'algos' not in k )} return objective_function ( current_config ) self . smac = self . facade ( scenario = scenario , intensifier_kwargs = self . intensifier_kwargs , rng = self . rng , tae_runner = smac_objective_function ) def optimize ( self ): \"\"\"Start optimization process.\"\"\" self . smac . optimize () def _build_smac_space ( self , pipeline_elements : list ): \"\"\" Build the entire SMAC hyperparameter space. Parameters: pipeline_elements: List of all PipelineElements to create the hyperparameter space. \"\"\" for pipe_element in pipeline_elements : # build conditions for switch elements if pipe_element . __class__ . __name__ == 'Switch' : algorithm_options = {} for algo in pipe_element . elements : algo_params = [] # hyper params corresponding to \"algo\" for name , value in algo . hyperparameters . items (): smac_param = self . _convert_photonai_to_smac_param ( value , ( pipe_element . name + \"__\" + name )) # or element.name__algo.name__name algo_params . append ( smac_param ) algorithm_options [( pipe_element . name + \"__\" + algo . name )] = algo_params algos = CategoricalHyperparameter ( pipe_element . name + \"__algos\" , choices = algorithm_options . keys ()) self . switch_optiones [ pipe_element . name + \"__algos\" ] = algorithm_options . keys () self . cspace . add_hyperparameter ( algos ) for algo , params in algorithm_options . items (): for param in params : cond = InCondition ( child = param , parent = algos , values = [ algo ]) self . cspace . add_hyperparameter ( param ) self . cspace . add_condition ( cond ) continue if hasattr ( pipe_element , 'hyperparameters' ): for name , value in pipe_element . hyperparameters . items (): self . hyperparameters . append ( name ) # if we only have one value we do not need to optimize if isinstance ( value , list ) and len ( value ) < 2 : self . constant_dictionary [ name ] = value [ 0 ] continue if isinstance ( value , PhotonCategorical ) and len ( value . values ) < 2 : self . constant_dictionary [ name ] = value . values [ 0 ] continue smac_param = self . _convert_photonai_to_smac_param ( value , name ) if smac_param is not None : self . cspace . add_hyperparameter ( smac_param ) @staticmethod def _convert_photonai_to_smac_param ( hyperparam : PhotonHyperparam , name : str ): \"\"\" Helper function: Convert PHOTONAI to SMAC hyperparameter. Parameters: hyperparam: One of photonai.optimization.hyperparameters. name: Name of hyperparameter. \"\"\" if isinstance ( hyperparam , PhotonCategorical ) or isinstance ( hyperparam , BooleanSwitch ): return CategoricalHyperparameter ( name , hyperparam . values ) elif isinstance ( hyperparam , list ): return CategoricalHyperparameter ( name , hyperparam ) elif isinstance ( hyperparam , FloatRange ): if hyperparam . range_type in [ 'linspace' , 'logspace' ]: return UniformFloatHyperparameter ( name , hyperparam . start , hyperparam . stop , log = ( hyperparam . range_type == 'logspace' )) msg = str ( hyperparam . range_type ) + \"in your FloatRange is not implemented in SMAC.\" logger . error ( msg ) raise NotImplementedError ( msg ) elif isinstance ( hyperparam , IntegerRange ): return UniformIntegerHyperparameter ( name , hyperparam . start , hyperparam . stop ) msg = \"Cannot convert hyperparameter \" + str ( hyperparam ) + \". Supported types: Categorical, IntegerRange,\" \\ \"FloatRange, list.\" logger . error ( msg ) raise ValueError ( msg )","title":"Documentation for SMACOptimizer"},{"location":"api/optimization/smac/#photonai.optimization.smac.smac.SMACOptimizer.__init__","text":"Initialize the object. Parameters: Name Type Description Default facade Choice of the SMAC backend strategy, [SMAC4BO, SMAC4HPO, SMAC4AC, BOHB4HPO]. 'SMAC4HPO' run_obj str Defines the optimization metric. When optimizing runtime, cutoff_time is required as well. 'quality' wallclock_limit float Maximum amount of wallclock-time used for optimization. 60.0 deterministic str If true, SMAC assumes that the target function or algorithm is deterministic (the same static seed of 0 is always passed to the function/algorithm). If false, different random seeds are passed to the target function/algorithm. 'true' intensifier_kwargs dict Dict for intensifier settings. None rng int Random seed of SMAC.facade. 42 **kwargs All initial kwargs are passed to SMACs scenario. List of all a vailable parameters . {} Source code in photonai/optimization/smac/smac.py def __init__ ( self , facade = 'SMAC4HPO' , run_obj : str = \"quality\" , deterministic : str = \"true\" , wallclock_limit : float = 60.0 , intensifier_kwargs : dict = None , rng : int = 42 , ** kwargs ): \"\"\" Initialize the object. Parameters: facade: Choice of the SMAC backend strategy, [SMAC4BO, SMAC4HPO, SMAC4AC, BOHB4HPO]. run_obj: Defines the optimization metric. When optimizing runtime, cutoff_time is required as well. wallclock_limit: Maximum amount of wallclock-time used for optimization. deterministic: If true, SMAC assumes that the target function or algorithm is deterministic (the same static seed of 0 is always passed to the function/algorithm). If false, different random seeds are passed to the target function/algorithm. intensifier_kwargs: Dict for intensifier settings. rng: Random seed of SMAC.facade. **kwargs: All initial kwargs are passed to SMACs scenario. [List of all a vailable parameters]( https://automl.github.io/SMAC3/master/options.html#scenario). \"\"\" super ( SMACOptimizer , self ) . __init__ () if not __found__ : msg = \"Module smac not found or not installed as expected. \" \\ \"Please install the smac/requirements.txt PHOTONAI provides.\" logger . error ( msg ) raise ModuleNotFoundError ( msg ) self . run_obj = run_obj self . deterministic = deterministic self . wallclock_limit = wallclock_limit self . kwargs = kwargs if facade in [ \"SMAC4BO\" , SMAC4BO , \"SMAC4AC\" , SMAC4AC , \"SMAC4HPO\" , SMAC4HPO , \"BOHB4HPO\" , BOHB4HPO ]: if type ( facade ) == str : self . facade = eval ( facade ) else : self . facade = facade else : msg = \"SMAC.facade {} not known. Please use one of ['SMAC4BO', 'SMAC4AC', 'SMAC4HPO'].\" logger . error ( msg . format ( str ( facade ))) raise ValueError ( msg . format ( str ( facade ))) self . rng = rng if not intensifier_kwargs : self . intensifier_kwargs = {} else : self . intensifier_kwargs = intensifier_kwargs self . cspace = ConfigurationSpace () # hyperparameter space for SMAC self . switch_optiones = {} self . hyperparameters = [] self . maximize_metric = False self . constant_dictionary = {}","title":"__init__()"},{"location":"api/optimization/smac/#photonai.optimization.smac.smac.SMACOptimizer.optimize","text":"Start optimization process. Source code in photonai/optimization/smac/smac.py def optimize ( self ): \"\"\"Start optimization process.\"\"\" self . smac . optimize ()","title":"optimize()"},{"location":"api/optimization/smac/#photonai.optimization.smac.smac.SMACOptimizer.prepare","text":"Initializes the SMAC Optimizer. Parameters: Name Type Description Default pipeline_elements list List of all PipelineElements to create hyperparameter space. required maximize_metric bool Boolean to distinguish between score and error. required objective_function Callable The cost or objective function. required Source code in photonai/optimization/smac/smac.py def prepare ( self , pipeline_elements : list , maximize_metric : bool , objective_function : Callable ): \"\"\" Initializes the SMAC Optimizer. Parameters: pipeline_elements: List of all PipelineElements to create hyperparameter space. maximize_metric: Boolean to distinguish between score and error. objective_function: The cost or objective function. \"\"\" self . cspace = ConfigurationSpace () # build space self . _build_smac_space ( pipeline_elements ) if self . constant_dictionary : msg = \"PHOTONAI has detected some one-valued params in your hyperparameters. Pleas use the kwargs for \" \\ \"constant values. This run ignores following settings: \" + str ( self . constant_dictionary . keys ()) logger . warning ( msg ) warnings . warn ( msg ) self . maximize_metric = maximize_metric scenario_dict = self . kwargs scenario_dict . update ({ \"run_obj\" : self . run_obj , \"deterministic\" : self . deterministic , \"wallclock_limit\" : self . wallclock_limit , \"cs\" : self . cspace , \"limit_resources\" : False }) scenario = Scenario ( scenario_dict ) def smac_objective_function ( current_config ): current_config = { k : current_config [ k ] for k in current_config if ( current_config [ k ] and 'algos' not in k )} return objective_function ( current_config ) self . smac = self . facade ( scenario = scenario , intensifier_kwargs = self . intensifier_kwargs , rng = self . rng , tae_runner = smac_objective_function )","title":"prepare()"},{"location":"api/optimization/hyperparameter/boolean_switch/","text":"Documentation for BooleanSwitch Boolean switch. Class for defining a boolean hyperparameter. Equivalent to Categorical([True, False]). Source code in photonai/optimization/hyperparameters.py class BooleanSwitch ( PhotonHyperparam ): \"\"\"Boolean switch. Class for defining a boolean hyperparameter. Equivalent to Categorical([True, False]). \"\"\" def __init__ ( self ): \"\"\"Initialize the object.\"\"\" super ( BooleanSwitch , self ) . __init__ ([ True , False ]) __init__ ( self ) special Initialize the object. Source code in photonai/optimization/hyperparameters.py def __init__ ( self ): \"\"\"Initialize the object.\"\"\" super ( BooleanSwitch , self ) . __init__ ([ True , False ])","title":"BooleanSwitch"},{"location":"api/optimization/hyperparameter/boolean_switch/#documentation-for-booleanswitch","text":"Boolean switch. Class for defining a boolean hyperparameter. Equivalent to Categorical([True, False]). Source code in photonai/optimization/hyperparameters.py class BooleanSwitch ( PhotonHyperparam ): \"\"\"Boolean switch. Class for defining a boolean hyperparameter. Equivalent to Categorical([True, False]). \"\"\" def __init__ ( self ): \"\"\"Initialize the object.\"\"\" super ( BooleanSwitch , self ) . __init__ ([ True , False ])","title":"Documentation for BooleanSwitch"},{"location":"api/optimization/hyperparameter/boolean_switch/#photonai.optimization.hyperparameters.BooleanSwitch.__init__","text":"Initialize the object. Source code in photonai/optimization/hyperparameters.py def __init__ ( self ): \"\"\"Initialize the object.\"\"\" super ( BooleanSwitch , self ) . __init__ ([ True , False ])","title":"__init__()"},{"location":"api/optimization/hyperparameter/categorical/","text":"Documentation for Categorical Class for defining a definite list of values. Source code in photonai/optimization/hyperparameters.py class Categorical ( PhotonHyperparam ): \"\"\" Class for defining a definite list of values. \"\"\" def __init__ ( self , values : list ): \"\"\" Initialize the object. Parameters: values: Definite list of hyperparameter values. \"\"\" super ( Categorical , self ) . __init__ ( values ) def __getitem__ ( self , item ): return self . values . __getitem__ ( item ) __init__ ( self , values ) special Initialize the object. Parameters: Name Type Description Default values list Definite list of hyperparameter values. required Source code in photonai/optimization/hyperparameters.py def __init__ ( self , values : list ): \"\"\" Initialize the object. Parameters: values: Definite list of hyperparameter values. \"\"\" super ( Categorical , self ) . __init__ ( values )","title":"Categorical"},{"location":"api/optimization/hyperparameter/categorical/#documentation-for-categorical","text":"Class for defining a definite list of values. Source code in photonai/optimization/hyperparameters.py class Categorical ( PhotonHyperparam ): \"\"\" Class for defining a definite list of values. \"\"\" def __init__ ( self , values : list ): \"\"\" Initialize the object. Parameters: values: Definite list of hyperparameter values. \"\"\" super ( Categorical , self ) . __init__ ( values ) def __getitem__ ( self , item ): return self . values . __getitem__ ( item )","title":"Documentation for Categorical"},{"location":"api/optimization/hyperparameter/categorical/#photonai.optimization.hyperparameters.Categorical.__init__","text":"Initialize the object. Parameters: Name Type Description Default values list Definite list of hyperparameter values. required Source code in photonai/optimization/hyperparameters.py def __init__ ( self , values : list ): \"\"\" Initialize the object. Parameters: values: Definite list of hyperparameter values. \"\"\" super ( Categorical , self ) . __init__ ( values )","title":"__init__()"},{"location":"api/optimization/hyperparameter/float_range/","text":"Documentation for FloatRange Float range. Class for easily creating an interval of numbers to be tested in the optimization process. Source code in photonai/optimization/hyperparameters.py class FloatRange ( NumberRange ): \"\"\"Float range. Class for easily creating an interval of numbers to be tested in the optimization process. \"\"\" def __init__ ( self , start : float , stop : float , range_type : str = 'linspace' , step : float = None , num : int = None , ** kwargs ): \"\"\" Initialize the object. Parameters: start: The start value for generating the lower bound. The resulting interval includes the value. stop: The stop value for generating the upper bound. - if range_type == \"range\": The end value is not included in the interval (see documentation of numpy.arange). - if range_type == \"linspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.linspace). - if range_type == \"logspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.logspace). - if range_type == \"geomspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.logspace). range_type: Which method to use for generating the number interval. Possible options, - \"range\": numpy.arange is used to generate a list of values separated by the same step width. - \"linspace\": numpy.linspace is used to generate a certain number of values between start and stop. - \"logspace\": numpy.logspace is used to generate a logarithmically distributed range of a certain length. - \"geomspace\": numpy.geomspace is used to generate numbers spaced evenly on a log scale (geometric progression). step: If range_type == 'range', the spacing between values. num: If range_type == 'linspace', range_type == 'logspace', or range_type == 'geomspace', the number of samples to generate. kwargs: Further parameters that should be passed to the numpy function chosen with range_type. \"\"\" super ( FloatRange , self ) . __init__ ( start , stop , range_type , step , num , np . float64 , ** kwargs ) def get_random_value ( self , definite_list : bool = False ): \"\"\" Method for random search to get a random value based on the underlying domain. Parameters: definite_list: Choice between an element of a discrete list or a value within an interval. As example, the num parameter would vanishes when this parameter is set to False. \"\"\" if definite_list : if not self . values : msg = \"No values were set. Please use transform method.\" logger . error ( msg ) raise ValueError ( msg ) return random . choice ( self . values ) else : return random . uniform ( self . start , self . stop ) __init__ ( self , start , stop , range_type = 'linspace' , step = None , num = None , ** kwargs ) special Initialize the object. Parameters: Name Type Description Default start float The start value for generating the lower bound. The resulting interval includes the value. required stop float The stop value for generating the upper bound. if range_type == \"range\": The end value is not included in the interval (see documentation of numpy.arange). if range_type == \"linspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.linspace). if range_type == \"logspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.logspace). if range_type == \"geomspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.logspace). required range_type str Which method to use for generating the number interval. Possible options, \"range\": numpy.arange is used to generate a list of values separated by the same step width. \"linspace\": numpy.linspace is used to generate a certain number of values between start and stop. \"logspace\": numpy.logspace is used to generate a logarithmically distributed range of a certain length. \"geomspace\": numpy.geomspace is used to generate numbers spaced evenly on a log scale (geometric progression). 'linspace' step float If range_type == 'range', the spacing between values. None num int If range_type == 'linspace', range_type == 'logspace', or range_type == 'geomspace', the number of samples to generate. None kwargs Further parameters that should be passed to the numpy function chosen with range_type. {} Source code in photonai/optimization/hyperparameters.py def __init__ ( self , start : float , stop : float , range_type : str = 'linspace' , step : float = None , num : int = None , ** kwargs ): \"\"\" Initialize the object. Parameters: start: The start value for generating the lower bound. The resulting interval includes the value. stop: The stop value for generating the upper bound. - if range_type == \"range\": The end value is not included in the interval (see documentation of numpy.arange). - if range_type == \"linspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.linspace). - if range_type == \"logspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.logspace). - if range_type == \"geomspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.logspace). range_type: Which method to use for generating the number interval. Possible options, - \"range\": numpy.arange is used to generate a list of values separated by the same step width. - \"linspace\": numpy.linspace is used to generate a certain number of values between start and stop. - \"logspace\": numpy.logspace is used to generate a logarithmically distributed range of a certain length. - \"geomspace\": numpy.geomspace is used to generate numbers spaced evenly on a log scale (geometric progression). step: If range_type == 'range', the spacing between values. num: If range_type == 'linspace', range_type == 'logspace', or range_type == 'geomspace', the number of samples to generate. kwargs: Further parameters that should be passed to the numpy function chosen with range_type. \"\"\" super ( FloatRange , self ) . __init__ ( start , stop , range_type , step , num , np . float64 , ** kwargs ) get_random_value ( self , definite_list = False ) Method for random search to get a random value based on the underlying domain. Parameters: Name Type Description Default definite_list bool Choice between an element of a discrete list or a value within an interval. As example, the num parameter would vanishes when this parameter is set to False. False Source code in photonai/optimization/hyperparameters.py def get_random_value ( self , definite_list : bool = False ): \"\"\" Method for random search to get a random value based on the underlying domain. Parameters: definite_list: Choice between an element of a discrete list or a value within an interval. As example, the num parameter would vanishes when this parameter is set to False. \"\"\" if definite_list : if not self . values : msg = \"No values were set. Please use transform method.\" logger . error ( msg ) raise ValueError ( msg ) return random . choice ( self . values ) else : return random . uniform ( self . start , self . stop )","title":"FloatRange"},{"location":"api/optimization/hyperparameter/float_range/#documentation-for-floatrange","text":"Float range. Class for easily creating an interval of numbers to be tested in the optimization process. Source code in photonai/optimization/hyperparameters.py class FloatRange ( NumberRange ): \"\"\"Float range. Class for easily creating an interval of numbers to be tested in the optimization process. \"\"\" def __init__ ( self , start : float , stop : float , range_type : str = 'linspace' , step : float = None , num : int = None , ** kwargs ): \"\"\" Initialize the object. Parameters: start: The start value for generating the lower bound. The resulting interval includes the value. stop: The stop value for generating the upper bound. - if range_type == \"range\": The end value is not included in the interval (see documentation of numpy.arange). - if range_type == \"linspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.linspace). - if range_type == \"logspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.logspace). - if range_type == \"geomspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.logspace). range_type: Which method to use for generating the number interval. Possible options, - \"range\": numpy.arange is used to generate a list of values separated by the same step width. - \"linspace\": numpy.linspace is used to generate a certain number of values between start and stop. - \"logspace\": numpy.logspace is used to generate a logarithmically distributed range of a certain length. - \"geomspace\": numpy.geomspace is used to generate numbers spaced evenly on a log scale (geometric progression). step: If range_type == 'range', the spacing between values. num: If range_type == 'linspace', range_type == 'logspace', or range_type == 'geomspace', the number of samples to generate. kwargs: Further parameters that should be passed to the numpy function chosen with range_type. \"\"\" super ( FloatRange , self ) . __init__ ( start , stop , range_type , step , num , np . float64 , ** kwargs ) def get_random_value ( self , definite_list : bool = False ): \"\"\" Method for random search to get a random value based on the underlying domain. Parameters: definite_list: Choice between an element of a discrete list or a value within an interval. As example, the num parameter would vanishes when this parameter is set to False. \"\"\" if definite_list : if not self . values : msg = \"No values were set. Please use transform method.\" logger . error ( msg ) raise ValueError ( msg ) return random . choice ( self . values ) else : return random . uniform ( self . start , self . stop )","title":"Documentation for FloatRange"},{"location":"api/optimization/hyperparameter/float_range/#photonai.optimization.hyperparameters.FloatRange.__init__","text":"Initialize the object. Parameters: Name Type Description Default start float The start value for generating the lower bound. The resulting interval includes the value. required stop float The stop value for generating the upper bound. if range_type == \"range\": The end value is not included in the interval (see documentation of numpy.arange). if range_type == \"linspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.linspace). if range_type == \"logspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.logspace). if range_type == \"geomspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.logspace). required range_type str Which method to use for generating the number interval. Possible options, \"range\": numpy.arange is used to generate a list of values separated by the same step width. \"linspace\": numpy.linspace is used to generate a certain number of values between start and stop. \"logspace\": numpy.logspace is used to generate a logarithmically distributed range of a certain length. \"geomspace\": numpy.geomspace is used to generate numbers spaced evenly on a log scale (geometric progression). 'linspace' step float If range_type == 'range', the spacing between values. None num int If range_type == 'linspace', range_type == 'logspace', or range_type == 'geomspace', the number of samples to generate. None kwargs Further parameters that should be passed to the numpy function chosen with range_type. {} Source code in photonai/optimization/hyperparameters.py def __init__ ( self , start : float , stop : float , range_type : str = 'linspace' , step : float = None , num : int = None , ** kwargs ): \"\"\" Initialize the object. Parameters: start: The start value for generating the lower bound. The resulting interval includes the value. stop: The stop value for generating the upper bound. - if range_type == \"range\": The end value is not included in the interval (see documentation of numpy.arange). - if range_type == \"linspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.linspace). - if range_type == \"logspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.logspace). - if range_type == \"geomspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.logspace). range_type: Which method to use for generating the number interval. Possible options, - \"range\": numpy.arange is used to generate a list of values separated by the same step width. - \"linspace\": numpy.linspace is used to generate a certain number of values between start and stop. - \"logspace\": numpy.logspace is used to generate a logarithmically distributed range of a certain length. - \"geomspace\": numpy.geomspace is used to generate numbers spaced evenly on a log scale (geometric progression). step: If range_type == 'range', the spacing between values. num: If range_type == 'linspace', range_type == 'logspace', or range_type == 'geomspace', the number of samples to generate. kwargs: Further parameters that should be passed to the numpy function chosen with range_type. \"\"\" super ( FloatRange , self ) . __init__ ( start , stop , range_type , step , num , np . float64 , ** kwargs )","title":"__init__()"},{"location":"api/optimization/hyperparameter/float_range/#photonai.optimization.hyperparameters.FloatRange.get_random_value","text":"Method for random search to get a random value based on the underlying domain. Parameters: Name Type Description Default definite_list bool Choice between an element of a discrete list or a value within an interval. As example, the num parameter would vanishes when this parameter is set to False. False Source code in photonai/optimization/hyperparameters.py def get_random_value ( self , definite_list : bool = False ): \"\"\" Method for random search to get a random value based on the underlying domain. Parameters: definite_list: Choice between an element of a discrete list or a value within an interval. As example, the num parameter would vanishes when this parameter is set to False. \"\"\" if definite_list : if not self . values : msg = \"No values were set. Please use transform method.\" logger . error ( msg ) raise ValueError ( msg ) return random . choice ( self . values ) else : return random . uniform ( self . start , self . stop )","title":"get_random_value()"},{"location":"api/optimization/hyperparameter/integer_range/","text":"Documentation for IntegerRange Integer range. Class for easily creating a range of integers to be tested in optimization process. Source code in photonai/optimization/hyperparameters.py class IntegerRange ( NumberRange ): \"\"\"Integer range. Class for easily creating a range of integers to be tested in optimization process. \"\"\" def __init__ ( self , start : float , stop : float , range_type : str = 'range' , step : int = None , num : int = None , ** kwargs ): \"\"\" Initialize the object. Parameters: start: The start value for generating the lower bound. The resulting interval includes the value. stop: The stop value for generating the upper bound. - if range_type == \"range\": The end value is not included in the interval (see documentation of numpy.arange). - if range_type == \"linspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.linspace). - if range_type == \"logspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.logspace). - if range_type == \"geomspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.logspace). range_type: Which method to use for generating the number interval. Possible options, - \"range\": numpy.arange is used to generate a list of values separated by the same step width. - \"linspace\": numpy.linspace is used to generate a certain number of values between start and stop. - \"logspace\": numpy.logspace is used to generate a logarithmically distributed range of a certain length. - \"geomspace\": numpy.geomspace is used to generate numbers spaced evenly on a log scale (geometric progression). step: If range_type == 'range', the spacing between values. num: If range_type == 'linspace', range_type == 'logspace', or range_type == 'geomspace', the number of samples to generate. **kwargs: Further parameters that should be passed to the numpy function chosen with range_type. \"\"\" super () . __init__ ( start , stop , range_type , step , num , np . int32 , ** kwargs ) def get_random_value ( self , definite_list : bool = False ): \"\"\" Method for random search to get a random value based on the underlying domain. Parameters: definite_list: Choice between an element of a discrete list or a value within an interval. As example, the step parameter would vanishes when this parameter is set to False. \"\"\" if definite_list : if not self . values : msg = \"No values were set. Please use transform method.\" logger . error ( msg ) raise ValueError ( msg ) return random . choice ( self . values ) else : return random . randint ( self . start , self . stop - 1 ) __init__ ( self , start , stop , range_type = 'range' , step = None , num = None , ** kwargs ) special Initialize the object. Parameters: Name Type Description Default start float The start value for generating the lower bound. The resulting interval includes the value. required stop float The stop value for generating the upper bound. if range_type == \"range\": The end value is not included in the interval (see documentation of numpy.arange). if range_type == \"linspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.linspace). if range_type == \"logspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.logspace). if range_type == \"geomspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.logspace). required range_type str Which method to use for generating the number interval. Possible options, \"range\": numpy.arange is used to generate a list of values separated by the same step width. \"linspace\": numpy.linspace is used to generate a certain number of values between start and stop. \"logspace\": numpy.logspace is used to generate a logarithmically distributed range of a certain length. \"geomspace\": numpy.geomspace is used to generate numbers spaced evenly on a log scale (geometric progression). 'range' step int If range_type == 'range', the spacing between values. None num int If range_type == 'linspace', range_type == 'logspace', or range_type == 'geomspace', the number of samples to generate. None **kwargs Further parameters that should be passed to the numpy function chosen with range_type. {} Source code in photonai/optimization/hyperparameters.py def __init__ ( self , start : float , stop : float , range_type : str = 'range' , step : int = None , num : int = None , ** kwargs ): \"\"\" Initialize the object. Parameters: start: The start value for generating the lower bound. The resulting interval includes the value. stop: The stop value for generating the upper bound. - if range_type == \"range\": The end value is not included in the interval (see documentation of numpy.arange). - if range_type == \"linspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.linspace). - if range_type == \"logspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.logspace). - if range_type == \"geomspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.logspace). range_type: Which method to use for generating the number interval. Possible options, - \"range\": numpy.arange is used to generate a list of values separated by the same step width. - \"linspace\": numpy.linspace is used to generate a certain number of values between start and stop. - \"logspace\": numpy.logspace is used to generate a logarithmically distributed range of a certain length. - \"geomspace\": numpy.geomspace is used to generate numbers spaced evenly on a log scale (geometric progression). step: If range_type == 'range', the spacing between values. num: If range_type == 'linspace', range_type == 'logspace', or range_type == 'geomspace', the number of samples to generate. **kwargs: Further parameters that should be passed to the numpy function chosen with range_type. \"\"\" super () . __init__ ( start , stop , range_type , step , num , np . int32 , ** kwargs ) get_random_value ( self , definite_list = False ) Method for random search to get a random value based on the underlying domain. Parameters: Name Type Description Default definite_list bool Choice between an element of a discrete list or a value within an interval. As example, the step parameter would vanishes when this parameter is set to False. False Source code in photonai/optimization/hyperparameters.py def get_random_value ( self , definite_list : bool = False ): \"\"\" Method for random search to get a random value based on the underlying domain. Parameters: definite_list: Choice between an element of a discrete list or a value within an interval. As example, the step parameter would vanishes when this parameter is set to False. \"\"\" if definite_list : if not self . values : msg = \"No values were set. Please use transform method.\" logger . error ( msg ) raise ValueError ( msg ) return random . choice ( self . values ) else : return random . randint ( self . start , self . stop - 1 )","title":"IntegerRange"},{"location":"api/optimization/hyperparameter/integer_range/#documentation-for-integerrange","text":"Integer range. Class for easily creating a range of integers to be tested in optimization process. Source code in photonai/optimization/hyperparameters.py class IntegerRange ( NumberRange ): \"\"\"Integer range. Class for easily creating a range of integers to be tested in optimization process. \"\"\" def __init__ ( self , start : float , stop : float , range_type : str = 'range' , step : int = None , num : int = None , ** kwargs ): \"\"\" Initialize the object. Parameters: start: The start value for generating the lower bound. The resulting interval includes the value. stop: The stop value for generating the upper bound. - if range_type == \"range\": The end value is not included in the interval (see documentation of numpy.arange). - if range_type == \"linspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.linspace). - if range_type == \"logspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.logspace). - if range_type == \"geomspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.logspace). range_type: Which method to use for generating the number interval. Possible options, - \"range\": numpy.arange is used to generate a list of values separated by the same step width. - \"linspace\": numpy.linspace is used to generate a certain number of values between start and stop. - \"logspace\": numpy.logspace is used to generate a logarithmically distributed range of a certain length. - \"geomspace\": numpy.geomspace is used to generate numbers spaced evenly on a log scale (geometric progression). step: If range_type == 'range', the spacing between values. num: If range_type == 'linspace', range_type == 'logspace', or range_type == 'geomspace', the number of samples to generate. **kwargs: Further parameters that should be passed to the numpy function chosen with range_type. \"\"\" super () . __init__ ( start , stop , range_type , step , num , np . int32 , ** kwargs ) def get_random_value ( self , definite_list : bool = False ): \"\"\" Method for random search to get a random value based on the underlying domain. Parameters: definite_list: Choice between an element of a discrete list or a value within an interval. As example, the step parameter would vanishes when this parameter is set to False. \"\"\" if definite_list : if not self . values : msg = \"No values were set. Please use transform method.\" logger . error ( msg ) raise ValueError ( msg ) return random . choice ( self . values ) else : return random . randint ( self . start , self . stop - 1 )","title":"Documentation for IntegerRange"},{"location":"api/optimization/hyperparameter/integer_range/#photonai.optimization.hyperparameters.IntegerRange.__init__","text":"Initialize the object. Parameters: Name Type Description Default start float The start value for generating the lower bound. The resulting interval includes the value. required stop float The stop value for generating the upper bound. if range_type == \"range\": The end value is not included in the interval (see documentation of numpy.arange). if range_type == \"linspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.linspace). if range_type == \"logspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.logspace). if range_type == \"geomspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.logspace). required range_type str Which method to use for generating the number interval. Possible options, \"range\": numpy.arange is used to generate a list of values separated by the same step width. \"linspace\": numpy.linspace is used to generate a certain number of values between start and stop. \"logspace\": numpy.logspace is used to generate a logarithmically distributed range of a certain length. \"geomspace\": numpy.geomspace is used to generate numbers spaced evenly on a log scale (geometric progression). 'range' step int If range_type == 'range', the spacing between values. None num int If range_type == 'linspace', range_type == 'logspace', or range_type == 'geomspace', the number of samples to generate. None **kwargs Further parameters that should be passed to the numpy function chosen with range_type. {} Source code in photonai/optimization/hyperparameters.py def __init__ ( self , start : float , stop : float , range_type : str = 'range' , step : int = None , num : int = None , ** kwargs ): \"\"\" Initialize the object. Parameters: start: The start value for generating the lower bound. The resulting interval includes the value. stop: The stop value for generating the upper bound. - if range_type == \"range\": The end value is not included in the interval (see documentation of numpy.arange). - if range_type == \"linspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.linspace). - if range_type == \"logspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.logspace). - if range_type == \"geomspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.logspace). range_type: Which method to use for generating the number interval. Possible options, - \"range\": numpy.arange is used to generate a list of values separated by the same step width. - \"linspace\": numpy.linspace is used to generate a certain number of values between start and stop. - \"logspace\": numpy.logspace is used to generate a logarithmically distributed range of a certain length. - \"geomspace\": numpy.geomspace is used to generate numbers spaced evenly on a log scale (geometric progression). step: If range_type == 'range', the spacing between values. num: If range_type == 'linspace', range_type == 'logspace', or range_type == 'geomspace', the number of samples to generate. **kwargs: Further parameters that should be passed to the numpy function chosen with range_type. \"\"\" super () . __init__ ( start , stop , range_type , step , num , np . int32 , ** kwargs )","title":"__init__()"},{"location":"api/optimization/hyperparameter/integer_range/#photonai.optimization.hyperparameters.IntegerRange.get_random_value","text":"Method for random search to get a random value based on the underlying domain. Parameters: Name Type Description Default definite_list bool Choice between an element of a discrete list or a value within an interval. As example, the step parameter would vanishes when this parameter is set to False. False Source code in photonai/optimization/hyperparameters.py def get_random_value ( self , definite_list : bool = False ): \"\"\" Method for random search to get a random value based on the underlying domain. Parameters: definite_list: Choice between an element of a discrete list or a value within an interval. As example, the step parameter would vanishes when this parameter is set to False. \"\"\" if definite_list : if not self . values : msg = \"No values were set. Please use transform method.\" logger . error ( msg ) raise ValueError ( msg ) return random . choice ( self . values ) else : return random . randint ( self . start , self . stop - 1 )","title":"get_random_value()"},{"location":"api/processing/results_handler/","text":"Documentation for ResultsHandler Provides all functions that operate on calculated results. As IO for the results object the ResultsHandler is able to handle results on its own. Source code in photonai/processing/results_handler.py class ResultsHandler : \"\"\" Provides all functions that operate on calculated results. As IO for the results object the ResultsHandler is able to handle results on its own. \"\"\" def __init__ ( self , results_object : MDBHyperpipe = None , output_settings = None ): \"\"\" Initialize the object. Parameters: results_object: All results are stored here. An initial setting is not necessary, because a later loading via file or MongoDB is possible. output_settings (OutputSettings): Setting for creation and storage of the results_object. \"\"\" self . results = results_object self . output_settings = output_settings def load_from_file ( self , results_file : str ): \"\"\" Read results_file from json into MDBHyperpipe object self.results. Parameters: results_file: Full path to json file. \"\"\" self . results = MDBHyperpipe . from_document ( json . load ( open ( results_file , 'r' ))) def load_from_mongodb ( self , mongodb_connect_url : str , pipe_name : str ): \"\"\" Read results_file from MongoDB into MDBHyperpipe object self.results. Parameters: mongodb_connect_url: MongoDB connection string. pipe_name: Name of the stored hyperpipe. \"\"\" connect ( mongodb_connect_url , alias = \"photon_core\" ) results = list ( MDBHyperpipe . objects . raw ({ 'name' : pipe_name })) if len ( results ) == 1 : self . results = results [ 0 ] elif len ( results ) > 1 : self . results = MDBHyperpipe . objects . order_by ([( \"computation_start_time\" , DESCENDING )]) . raw ({ 'name' : pipe_name }) . first () warn_text = 'Found multiple hyperpipes with that name. Returning most recent one.' logger . warning ( warn_text ) warnings . warn ( warn_text ) else : raise FileNotFoundError ( 'Could not load hyperpipe from MongoDB.' ) @staticmethod def get_methods () -> list : \"\"\" This function returns a list of all methods available for ResultsHandler. Returns: List of all available methods. \"\"\" methods_list = [ s for s in dir ( ResultsHandler ) if '__' not in s ] return methods_list def get_performance_table ( self ): \"\"\"This function returns a summary table of the overall results. ToDo: add best_config information! \"\"\" res_tab = pd . DataFrame () for i , folds in enumerate ( self . results . outer_folds ): # add best config infos res_tab . loc [ i , 'best_config' ] = str ( folds . best_config . human_readable_config ) # add fold index res_tab . loc [ i , 'fold' ] = folds . fold_nr # add sample size infos res_tab . loc [ i , 'n_train' ] = folds . best_config . best_config_score . number_samples_training res_tab . loc [ i , 'n_validation' ] = folds . best_config . best_config_score . number_samples_validation # add performance metrics d = folds . best_config . best_config_score . validation . metrics for key , value in d . items (): res_tab . loc [ i , key ] = value # add row with overall info res_tab . loc [ i + 1 , 'n_validation' ] = np . sum ( res_tab [ 'n_validation' ]) for key , value in d . items (): m = res_tab . loc [:, key ] res_tab . loc [ i + 1 , key ] = np . mean ( m ) res_tab . loc [ i + 1 , key + '_sem' ] = sem ( m ) # standard error of the mean res_tab . loc [ i + 1 , 'best_config' ] = 'Overall' return res_tab def get_performance_outer_folds ( self ): performances = dict () for metric in self . results . outer_folds [ 0 ] . best_config . best_config_score . validation . metrics . keys (): performances [ metric ] = list () for i , fold in enumerate ( self . results . outer_folds ): for metric , value in fold . best_config . best_config_score . validation . metrics . items (): performances [ metric ] . append ( value ) return performances def get_config_evaluations ( self ) -> dict : \"\"\" Return the test performance of every tested configuration in every outer fold. Returns: Test performance of every configuration. \"\"\" config_performances = list () maximum_fold = None for outer_fold in self . results . outer_folds : if maximum_fold is None or len ( outer_fold . tested_config_list ) > maximum_fold : maximum_fold = len ( outer_fold . tested_config_list ) for outer_fold in self . results . outer_folds : performance = dict () for metric in self . results . hyperpipe_info . metrics : performance [ metric ] = list () for i in range ( maximum_fold ): # for config in outer_fold.tested_config_list: for metric in self . results . hyperpipe_info . metrics : if i >= len ( outer_fold . tested_config_list ): performance [ metric ] . append ( np . nan ) continue config = outer_fold . tested_config_list [ i ] if config . config_failed : performance [ metric ] . append ( np . nan ) else : for item in config . metrics_test : if ( item . operation == 'mean' ) and ( item . metric_name == metric ): performance [ metric ] . append ( item . value ) config_performances . append ( performance ) config_performances_dict = dict () for metric in self . results . hyperpipe_info . metrics : config_performances_dict [ metric ] = list () for fold in config_performances : config_performances_dict [ metric ] . append ( fold [ metric ]) return config_performances_dict def get_minimum_config_evaluations ( self ): config_evaluations = self . get_config_evaluations () minimum_config_evaluations = dict () for metric , evaluations in config_evaluations . items (): minimum_config_evaluations [ metric ] = list () greater_is_better = Scorer . greater_is_better_distinction ( metric ) for fold in evaluations : fold_evaluations = list () if greater_is_better : for i , config in enumerate ( fold ): if i == 0 : last_config = config else : if config > last_config : last_config = config fold_evaluations . append ( last_config ) else : last_config = np . inf for i , config in enumerate ( fold ): if i == 0 : last_config = config else : if config < last_config : last_config = config fold_evaluations . append ( last_config ) minimum_config_evaluations [ metric ] . append ( fold_evaluations ) return minimum_config_evaluations def get_learning_curves ( self , config_nr , outer_fold_nr , save ): \"\"\"This function gets the learning curves out of the result tree. It returns the learning curves as a pandas dataframe. If save = True it saves the learning curves as a csv file. \"\"\" cuts = self . results . hyperpipe_info . learning_curves_cut . values [ 1 :] + [ 1. ] fold_num = len ( self . results . outer_folds [ 0 ] . tested_config_list [ config_nr - 1 ] . inner_folds ) idx = pd . MultiIndex . from_product ([ cuts , [ i + 1 for i in range ( fold_num )]], names = [ 'Cut' , 'Inner Fold Nr.' ]) col = pd . MultiIndex . from_product ([ self . results . hyperpipe_info . metrics , [ 'test' , 'train' ]]) data = {} for metric in self . results . hyperpipe_info . metrics : config = self . results . outer_folds [ outer_fold_nr - 1 ] . tested_config_list [ config_nr - 1 ] for t in [ 1 , 2 ]: curves = [] for cut_nr , cut in enumerate ( cuts ): curves += [ config . inner_folds [ fold ] . learning_curves [ cut_nr ][ t ][ metric ] for fold in range ( fold_num )] data . update ({( metric , [ 'test' , 'train' ][ t - 1 ]): curves }) curves = pd . DataFrame ( data , index = idx , columns = col ) if save : curves . to_csv ( self . _save_prep_learning_curves ( 'lc_outer_fold_ %d _config_ %d .csv' % ( outer_fold_nr , config_nr ))) return curves def plot_curves ( self , curves : pd . DataFrame , title : str = 'Learning Curves' ): \"\"\"This function plots the learning curves. Parameters: curves: Dataframe with multi-index: (run - fraction of data) columns: at least (metric, train/test) floats title: Subtitle of plot. \"\"\" metrics = self . results . hyperpipe_info . metrics fig , axes = plt . subplots ( 1 , len ( metrics ), figsize = ( len ( metrics ) * 4. , 4. )) if len ( metrics ) == 1 : axes = [ axes ] cuts = curves . index . get_level_values ( 0 ) col_template = tuple ( curves . columns [ 0 ]) # iterate only over first 2 entries [(metric, train/test)], example 'mean' as third for metric , ax in zip ( metrics , axes ): for subset in [ 'test' , 'train' ]: sns . lineplot ( x = cuts , y = curves [( metric , subset ) + col_template [ 2 :]], label = metric + '_' + subset , ax = ax ) ax . set ( xlabel = 'Fraction of Train Data used' , ylabel = 'Metric Value' ) ax . legend ( fontsize = 'small' ) plt . suptitle ( title ) plt . tight_layout ( rect = [ 0 , 0.03 , 1 , 0.95 ]) return fig def plot_learning_curves_config ( self , config_nr , outer_fold_nr , save , show = False ): \"\"\"This function gets the learning curves for a specific config nr. and outer fold nr. and plots them If config_nr = -1 it gets the best config of the outer fold If save = True the plot is saved If show = True the plot is shown \"\"\" if config_nr == - 1 : config_nr = self . results . best_config . config_nr curves = self . get_learning_curves ( config_nr , outer_fold_nr , save ) curves . columns = curves . columns . to_flat_index () fig = self . plot_curves ( curves , 'Learning Curves (Outer Fold Nr. %d Config Nr. %d )' % ( outer_fold_nr , config_nr )) if save : plt . savefig ( self . _save_prep_learning_curves ( 'lc_outer_fold_ %d _config_ %d .png' % ( outer_fold_nr , config_nr ))) if show : plt . show () plt . close () def plot_learning_curves_outer_fold ( self , outer_fold_nr , config_nr_list = None , save = True , show = False ): \"\"\"This function gets the learning curves for a list of configs in a specific outer fold and plots them For each config the mean of the learning curves of all inner folds is used If config_nr = -1 it gets the best config of the outer fold If save = True the plot is saved If show = True the plot is shown \"\"\" if config_nr_list is None : config_nr_list = np . arange ( 1 , len ( self . results . outer_folds [ outer_fold_nr - 1 ] . tested_config_list ) + 1 ) elif - 1 in config_nr_list : config_nr_list = [ nr for nr in config_nr_list if nr is not self . results . best_config . config_nr ] config_nr_list [ config_nr_list == - 1 ] = self . results . best_config . config_nr curves_list = [] for config_nr in config_nr_list : curves = self . get_learning_curves ( config_nr , outer_fold_nr , save ) curves_list . append ( curves . groupby ( level = 0 ) . agg ([ 'mean' ])) curves_configs = pd . concat ( curves_list , axis = 0 , names = [ \"Config Nr.\" ], keys = config_nr_list ) curves_configs . columns = curves_configs . columns . to_flat_index () curves_configs = curves_configs . swaplevel () fig = self . plot_curves ( curves_configs , 'Learning Curves (Outer Fold Nr. %d )' % outer_fold_nr ) if save : curves_configs . to_csv ( self . _save_prep_learning_curves ( 'lc_outer_fold_ {} .csv' . format ( outer_fold_nr ))) plt . savefig ( self . _save_prep_learning_curves ( 'lc_outer_fold_ {} .png' . format ( outer_fold_nr ))) if show : plt . show () plt . close () def _save_prep_learning_curves ( self , file_name ): path = self . results . output_folder + '/learning_curves/' if not os . path . exists ( path ): os . makedirs ( path ) return os . path . join ( path , file_name ) def save_all_learning_curves ( self ): for outer_fold_nr in range ( 1 , len ( self . results . outer_folds ) + 1 ): for config_nr in range ( 1 , len ( self . results . outer_folds [ 0 ] . tested_config_list ) + 1 ): self . plot_learning_curves_config ( config_nr , outer_fold_nr , save = True ) def plot_optimizer_history ( self , metric , title : str = 'Optimizer History' , type : str = 'plot' , reduce_scatter_by : Union [ int , str ] = 'auto' , file : str = None ): \"\"\" :param metric: specify metric that has been stored within the PHOTONAI results tree :param type: 'plot' or 'scatter' :param reduce_scatter_by: integer or string ('auto'), reduce the number of points plotted by scatter :param file: specify a filename if you want to save the plot :return: \"\"\" if metric not in self . results . hyperpipe_info . metrics : raise ValueError ( 'Metric \" {} \" not stored in results tree' . format ( metric )) config_evaluations = self . get_config_evaluations () minimum_config_evaluations = self . get_minimum_config_evaluations () # handle different lengths min_corresponding = len ( min ( config_evaluations [ metric ], key = len )) config_evaluations_corres = [ configs [: min_corresponding ] for configs in config_evaluations [ metric ]] minimum_config_evaluations_corres = [ configs [: min_corresponding ] for configs in minimum_config_evaluations [ metric ]] mean = np . nanmean ( np . asarray ( config_evaluations_corres ), axis = 0 ) mean_min = np . nanmean ( np . asarray ( minimum_config_evaluations_corres ), axis = 0 ) greater_is_better = Scorer . greater_is_better_distinction ( metric ) if greater_is_better : caption = 'Maximum' else : caption = 'Minimum' plt . figure () if type == 'plot' : plt . plot ( np . arange ( 0 , len ( mean )), mean , '-' , color = 'gray' , label = 'Mean Performance' ) elif type == 'scatter' : # now do smoothing if isinstance ( reduce_scatter_by , str ): if reduce_scatter_by != 'auto' : msg = ' {} is not a valid smoothing_kernel specifier. ' \\ 'Falling back to \"auto\".' . format ( reduce_scatter_by ) logger . warning ( msg ) warnings . warn ( msg ) # if auto, then calculate size of reduce_scatter_by so that 75 points on x remain # smallest reduce_scatter_by should be 1 reduce_scatter_by = max ([ np . floor ( min_corresponding / 75 ) . astype ( int ), 1 ]) if reduce_scatter_by > 1 : plt . plot ([], [], ' ' , label = \"scatter reduced by factor {} \" . format ( reduce_scatter_by )) for i , fold in enumerate ( config_evaluations [ metric ]): # add a few None so that list can be divided by smoothing_kernel remaining = len ( fold ) % reduce_scatter_by if remaining : fold . extend ([ np . nan ] * ( reduce_scatter_by - remaining )) # calculate mean over every n named_steps so that plot is less cluttered reduced_fold = np . nanmean ( np . asarray ( fold ) . reshape ( - 1 , reduce_scatter_by ), axis = 1 ) reduced_xfit = np . arange ( reduce_scatter_by / 2 , len ( fold ), step = reduce_scatter_by ) if i == len ( config_evaluations [ metric ]) - 1 : plt . scatter ( reduced_xfit , np . asarray ( reduced_fold ), color = 'gray' , alpha = 0.5 , label = 'Performance' , marker = '.' ) else : plt . scatter ( reduced_xfit , np . asarray ( reduced_fold ), color = 'gray' , alpha = 0.5 , marker = '.' ) else : raise ValueError ( 'Please specify either \"plot\" or \"scatter\".' ) plt . plot ( np . arange ( 0 , len ( mean_min )), mean_min , '-' , color = 'black' , label = 'Mean {} Performance' . format ( caption )) for i , fold in enumerate ( minimum_config_evaluations [ metric ]): xfit = np . arange ( 0 , len ( fold )) plt . plot ( xfit , fold , '-' , color = 'black' , alpha = 0.5 ) plt . ylabel ( metric . replace ( '_' , ' ' )) plt . xlabel ( 'No of Evaluations' ) plt . legend () plt . title ( title ) if file : plt . savefig ( file ) else : file = os . path . join ( self . results . output_folder , \"optimizer_history.png\" ) plt . savefig ( file ) plt . close () def get_importance_scores ( self ): \"\"\" This function returns the importance scores for the best configuration of each outer fold. \"\"\" imps = [] for i , fold in enumerate ( self . results . outer_folds ): imps . append ( fold . best_config . best_config_score . feature_importances ) return imps @staticmethod def collect_fold_lists ( score_info_list , fold_nr , predictions_filename = '' ): if len ( score_info_list ) > 0 : fold_nr_array = [] collectables = { 'y_pred' : [], 'y_true' : [], 'indices' : [], 'probabilities' : []} for i , score_info in enumerate ( score_info_list ): for collectable_key , collectable_list in collectables . items (): if getattr ( score_info , collectable_key ) is not None and len ( getattr ( score_info , collectable_key )) > 0 : collectables [ collectable_key ] . extend ( list ( getattr ( score_info , collectable_key ))) else : collectables [ collectable_key ] . extend ( list ( np . full (( len ( score_info . y_true )), np . nan ))) fold_nr_array . extend ( list ( np . ones (( len ( score_info . y_true ),)) * fold_nr [ i ])) # enable nd y_pred support if len ( collectables [ \"y_pred\" ]) > len ( collectables [ \"y_true\" ]): tmp_collectables_y_pred = collectables [ \"y_pred\" ] headers = collectables [ \"y_pred\" ][ 0 ] for i , header in enumerate ( list ( headers )): collectables [ header ] = [ x [ i ] for x in tmp_collectables_y_pred if x != tmp_collectables_y_pred [ 0 ]] collectables [ \"fold\" ] = fold_nr_array # convert to pandas dataframe to use their sorting algorithm save_df = pd . DataFrame ( collectables ) sorted_df = save_df . sort_values ( by = 'indices' ) if predictions_filename != '' : sorted_df . to_csv ( predictions_filename , index = None ) return sorted_df . to_dict ( 'list' ) def get_mean_train_predictions ( self , filename = '' ): \"\"\" This function returns the MEAN predictions, true targets, and fold index for the TRAINING Set of the best configuration of each outer fold. \"\"\" if self . results is None : raise ValueError ( \"Result tree information is needed but results attribute of object is None.\" ) score_info_list = list () fold_nr_list = list () for outer_fold in self . results . outer_folds : score_info_list . append ( outer_fold . best_config . best_config_score . training ) fold_nr_list . append ( outer_fold . fold_nr ) infos = self . collect_fold_lists ( score_info_list , fold_nr_list , filename ) infos = { key : np . array ( value ) for key , value in infos . items ()} num_items = np . unique ( infos [ \"indices\" ]) mean_pred = np . zeros ( num_items . shape ) y_true = np . zeros ( num_items . shape ) for i in num_items : idx = ( infos [ \"indices\" ] == i ) mean_pred [ i ] = np . mean ( infos [ \"y_pred\" ][ idx ]) y_true [ i ] = infos [ \"y_true\" ][ idx ][ 0 ] return { 'y_true' : y_true , 'y_pred' : mean_pred , 'indices' : num_items } def get_test_predictions ( self , filename = '' ): \"\"\" This function returns the predictions, true targets, and fold index for the best configuration of each outer fold. \"\"\" if self . results is None : raise ValueError ( \"Result tree information is needed but results attribute of object is None.\" ) score_info_list = list () fold_nr_list = list () for outer_fold in self . results . outer_folds : score_info_list . append ( outer_fold . best_config . best_config_score . validation ) fold_nr_list . append ( outer_fold . fold_nr ) return self . collect_fold_lists ( score_info_list , fold_nr_list , filename ) def get_validation_predictions ( self , outer_fold_nr = 0 , config_no = 0 , filename = '' ): \"\"\" This function returns the predictions, probabilities, true targets, fold and index for the config_nr of the given outer_fold \"\"\" score_info_list = list () fold_nr_list = list () if self . results is None : raise ValueError ( \"Result tree information is needed but results attribute of object is None.\" ) # Todo: find config by config_id for inner_fold in self . results . outer_folds [ outer_fold_nr ] . tested_config_list [ config_no ] . inner_folds : score_info_list . append ( inner_fold . validation ) fold_nr_list . append ( inner_fold . fold_nr ) return self . collect_fold_lists ( score_info_list , fold_nr_list , filename ) def eval_mean_time_components ( self , write_results = True , plotly_return = False ): \"\"\" This function create charts and tables out of the time-monitoring. \"\"\" result_dict = {} caching = False default_dict = { 'total_seconds' : 0 , 'total_items_processed' : 0 , 'mean_seconds_per_config' : 0 , 'mean_seconds_per_item' : 0 } # sum up times per element, 1. per config, and 2. in total for outer_fold in self . results . outer_folds : for config_nr , config in enumerate ( outer_fold . tested_config_list ): tmp_config_dict = {} # resort time entries for each element so that is has the following structure # element_name -> fit/transform/predict -> (seconds, nr_items) for inner_fold in config . inner_folds : for time_key , time_values in inner_fold . time_monitor . items (): for value_item in time_values : name , time , nr_items = value_item [ 0 ], value_item [ 1 ], value_item [ 2 ] if name not in tmp_config_dict : tmp_config_dict [ name ] = {} if time_key not in tmp_config_dict [ name ]: tmp_config_dict [ name ][ time_key ] = [] tmp_config_dict [ name ][ time_key ] . append (( time , nr_items )) # calculate mean time per config and absolute time for element_name , element_time_dict in tmp_config_dict . items (): for element_time_key , element_time_list in element_time_dict . items (): if element_time_key == \"transform_cached\" : caching = True mean_time = np . mean ([ i [ 0 ] for i in element_time_list ]) total_time = np . sum ([ i [ 0 ] for i in element_time_list ]) total_items_processed = np . sum ([ i [ 1 ] for i in element_time_list ]) if element_name not in result_dict : result_dict [ element_name ] = {} if element_time_key not in result_dict [ element_name ]: result_dict [ element_name ][ element_time_key ] = dict ( default_dict ) result_dict [ element_name ][ element_time_key ][ 'total_seconds' ] += total_time result_dict [ element_name ][ element_time_key ][ 'total_items_processed' ] += total_items_processed mean_time_per_config = result_dict [ element_name ][ element_time_key ][ 'mean_seconds_per_config' ] tmp_total_mean = (( mean_time_per_config * config_nr ) + mean_time ) / ( config_nr + 1 ) result_dict [ element_name ][ element_time_key ][ 'mean_seconds_per_config' ] = tmp_total_mean tmp_mean_per_item = result_dict [ element_name ][ element_time_key ][ 'total_seconds' ] / \\ result_dict [ element_name ][ element_time_key ][ 'total_items_processed' ] result_dict [ element_name ][ element_time_key ][ 'mean_seconds_per_item' ] = tmp_mean_per_item format_str = ' {:06.6f} ' if caching : # in case we used caching add transform_cached and transform_computed values to transform_total for name , sub_result_dict in result_dict . items (): if \"transform_cached\" in sub_result_dict : result_dict [ name ][ \"transform\" ] = dict ( default_dict ) for value_dict in sub_result_dict . values (): for info in value_dict . keys (): result_dict [ name ][ \"transform\" ][ info ] = result_dict [ name ][ \"transform_cached\" ][ info ] # in case everything's been in the cache we have no computation if \"transform_computed\" in sub_result_dict : result_dict [ name ][ \"transform\" ][ info ] += result_dict [ name ][ \"transform_computed\" ][ info ] if \"transform_computed\" in sub_result_dict : # calculate a ratio, if caching was helpful and how much of the time it saved result_dict [ name ][ \"cache_ratio\" ] = result_dict [ name ][ \"transform_cached\" ][ \"total_seconds\" ] / \\ result_dict [ name ][ \"transform_computed\" ][ \"total_seconds\" ] # in case of caching we have different plot plus a different csv file csv_keys = [ \"fit\" , \"transform\" , \"transform_computed\" , \"transform_cached\" , \"predict\" ] csv_titles = csv_keys plot_list = [ \"fit\" , \"transform\" , \"transform_cached\" ] method_list = [ \"fit\" , \"transform_computed\" , \"transform_cached\" , \"predict\" ] else : csv_keys = [ \"fit\" , \"transform_computed\" , \"predict\" ] csv_titles = [ \"fit\" , \"transform\" , \"predict\" ] plot_list = [ \"fit\" , \"transform_computed\" ] method_list = [ \"fit\" , \"transform_computed\" , \"predict\" ] # write csv file with time analysis if write_results : sub_keys = [ \"total_seconds\" , \"mean_seconds_per_config\" , \"mean_seconds_per_item\" ] csv_filename = os . path . join ( self . results . output_folder , 'time_monitor.csv' ) with open ( csv_filename , 'w' ) as csvfile : writer = csv . writer ( csvfile ) header1 = [ \"\" ] for k_name in csv_titles : header1 . extend ([ k_name , \"\" , \"\" ]) header2 = [ \"Element\" ] + ( sub_keys * len ( csv_titles )) if caching : header1 . append ( \"\" ) header2 . append ( \"cache_ratio\" ) writer . writerow ( header1 ) writer . writerow ( header2 ) for item , item_dict in result_dict . items (): row = [ item ] for time_key in csv_keys : for sub_key in sub_keys : if time_key in item_dict : row . append ( format_str . format ( item_dict [ time_key ][ sub_key ])) else : row . append ( '' ) if caching : if \"cache_ratio\" in item_dict : row . append ( item_dict [ \"cache_ratio\" ]) writer . writerow ( row ) # plot figure # TODO! Use PiePlotlyPlot class without cricle imports plotly_dict = { 'layout' : { 'title' : 'Time Monitor Pie Chart' , 'showlegend' : True , 'height' : 600 , 'annotations' : []}, 'data' : [] } def append_plotly ( labels , values , name , colors , domain ): \"\"\" helper function (temporary -> to.do above) \"\"\" plotly_dict [ \"data\" ] . append ({ 'labels' : labels , 'values' : values , 'type' : 'pie' , 'name' : name , 'marker' : { 'colors' : colors }, 'domain' : domain , 'hoverinfo' : 'label+percent' , 'textposition' : 'inside' }) plotly_dict [ 'layout' ][ 'annotations' ] . append ({ \"x\" : np . mean ( domain [ \"x\" ]), \"y\" : ( domain [ \"y\" ][ 1 ]), \"font\" : { \"size\" : 16 }, \"text\" : name , \"xref\" : \"paper\" , \"yref\" : \"paper\" , \"xanchor\" : \"center\" , \"yanchor\" : \"bottom\" , \"showarrow\" : False }) def eval_mean_time_autopct ( values ): def my_autopct ( pct ): total = sum ( values ) if pct / total >= 1 : return str ( round ( pct , 1 )) + \"%\" else : return None return my_autopct # Create nxm sub plots cpl = len ( plot_list ) gs = matplotlib . gridspec . GridSpec ( int (( cpl - 1 ) / 3 ) + 2 , min ( cpl , 3 )) legend_theme = plt . get_cmap ( 'Set3' ) legend_theme2 = plt . get_cmap ( 'tab10' ) element_names = [ name for name , element in result_dict . items ()] fig = plt . figure ( figsize = ( 10 , 7 ), dpi = 160 ) colors = [ legend_theme ( 1. * i / len ( element_names )) for i in range ( len ( element_names ))] for i , k in enumerate ( plot_list ): ax = plt . subplot ( gs [ int ( i / 3 ), i % 3 ]) ax . set_prop_cycle ( \"color\" , colors ) data = [ element [ k ][ \"total_seconds\" ] if k in element else 0 for name , element in result_dict . items ()] data_sum = sum ( data ) if data_sum == 0 : data_sum = 1 values = [ val / data_sum for val in data ] patches , _ , _ = plt . pie ( values , shadow = True , startangle = 90 , autopct = eval_mean_time_autopct ( data ), pctdistance = 0.7 ) plt . axis ( 'equal' ) plt . title ( k ) append_plotly ( labels = [ str ( d ) for d in element_names ], values = values , name = k , colors = colors , domain = { 'x' : [ i / len ( plot_list ), ( i + 1 ) / len ( plot_list )], 'y' : [ 0.55 , 1 ]}) plt . legend ( loc = 'upper left' , labels = [ ' %s ' % l for l in element_names ], prop = { 'size' : 10 }, bbox_to_anchor = ( 0.0 , 1 ), bbox_transform = fig . transFigure ) # add another plot for the comparison of the fit/transform/predict methods ax2 = plt . subplot ( gs [ int ( i / 3 ) + 1 , :]) colors = [ legend_theme2 ( 1. * i / len ( data )) for i in range ( len ( method_list ))] ax2 . set_prop_cycle ( \"color\" , colors ) data = [] for k in method_list : data . append ( np . sum ([ element [ k ][ \"total_seconds\" ] for name , element in result_dict . items () if k in element ])) patches_an , _ , _ = plt . pie ([ val / sum ( data ) for val in data ], shadow = True , startangle = 90 , pctdistance = 0.7 , autopct = eval_mean_time_autopct ( data )) append_plotly ( labels = method_list , values = [ val / sum ( data ) for val in data ], name = \"methods\" , colors = colors , domain = { 'x' : [ 0 , 1 ], 'y' : [ 0 , 0.45 ]}) plt . axis ( 'equal' ) plt . title ( \"methods\" ) plt . legend ( loc = 'lower left' , labels = [ ' %s ' % l for l in method_list ], prop = { 'size' : 10 }, bbox_transform = fig . transFigure ) # for only one legend #fig.legend(patches+patches_an, element_names+method_list, prop={'size': 10}, loc='lower left') if write_results : plt . savefig ( os . path . join ( self . results . output_folder , 'time_monitor_pie.png' )) plt . close () if plotly_return : str_fig = \"var layout =\" + str ( plotly_dict [ \"layout\" ]) + \";\" str_fig += \"var data = \" + str ( plotly_dict [ \"data\" ]) + \";\" str_fig += \"Plotly.newPlot('\" + \"time_monitor_pie_id\" + \"',data, layout);\" return str_fig . replace ( \"False\" , \"false\" ) . replace ( \"True\" , \"true\" ) def save ( self ): if self . output_settings . mongodb_connect_url : connect ( self . output_settings . mongodb_connect_url , alias = 'photon_core' ) logger . info ( 'Write results to mongodb...' ) try : self . results . save () except DocumentTooLarge : logger . error ( 'Could not save document into MongoDB: Document too large' ) if self . output_settings . save_output : logger . info ( \"Writing results to project folder...\" ) self . write_result_tree_to_file () def save_backmapping ( self , filename : str , backmapping ): try : if isinstance ( backmapping , list ): backmapping = np . asarray ( backmapping ) try : from nibabel.nifti1 import Nifti1Image if isinstance ( backmapping , Nifti1Image ): backmapping . to_filename ( os . path . join ( self . results . output_folder , filename + '.nii.gz' )) except ImportError : pass finally : if isinstance ( backmapping , np . ndarray ): if backmapping . size > 1000 : np . savez ( os . path . join ( self . results . output_folder , filename + '.npz' ), backmapping ) else : np . savetxt ( os . path . join ( self . results . output_folder , filename + '.csv' ), backmapping , delimiter = ',' ) else : with open ( os . path . join ( self . results . output_folder , filename + '.p' ), 'wb' ) as f : pickle . dump ( backmapping , f ) except Exception as e : logger . error ( \"Could not save backmapped feature importances.\" ) logger . error ( e ) def write_convenience_files ( self ): if self . output_settings . save_output : logger . info ( \"Writing summary file, plots and prediction csv to result folder ...\" ) self . write_summary () self . write_predictions_file () def convert_to_json_serializable ( self , value ): if isinstance ( value , ( int , np . int32 , np . int64 )): return int ( value ) if isinstance ( value , ( float , np . float32 , np . float64 )): if self . output_settings . reduce_space : return round ( float ( value ), 3 ) return float ( value ) else : return json_util . default ( value ) def write_result_tree_to_file ( self ): try : local_file = os . path . join ( self . results . output_folder , 'photon_result_file.json' ) result = self . round_floats ( self . results . to_son () . to_dict ()) with open ( local_file , 'w' ) as outfile : json . dump ( result , outfile , default = self . convert_to_json_serializable ) except OSError as e : logger . error ( \"Could not write results to local file\" ) logger . error ( str ( e )) @classmethod def round_floats ( cls , d ): # recursive method for rounding all floats in result.json result = {} if isinstance ( d , dict ): for key , value in d . items (): value = cls . round_floats ( value ) result . update ({ key : value }) return result elif isinstance ( d , list ): return [ cls . round_floats ( val ) for val in d ] elif isinstance ( d , float ): return round ( d , 6 ) else : return d def get_best_config_inner_fold_predictions ( self , filename = '' ): score_info_list = [] fold_nr = [] for inner_fold in self . results . best_config . inner_folds : score_info_list . append ( inner_fold . validation ) fold_nr . append ( inner_fold . fold_nr ) return self . collect_fold_lists ( score_info_list , fold_nr , filename ) def write_predictions_file ( self ): if self . output_settings . save_output : filename = os . path . join ( self . output_settings . results_folder , 'best_config_predictions.csv' ) # usually we write the predictions for the outer fold if not self . output_settings . save_predictions_from_best_config_inner_folds : return self . get_test_predictions ( filename ) # in case no outer folds exist, we write the inner_fold predictions else : return self . get_best_config_inner_fold_predictions ( filename ) def _get_best_outer_fold_configs_per_estimator ( self ) -> dict : # 1. find out which estimators there are last_element_name_identifier , last_element_dict = list ( self . results . hyperpipe_info . elements . items ())[ - 1 ] no_switch_found = False if not \":\" in last_element_name_identifier : no_switch_found = True last_element_base_element , last_element_name = last_element_name_identifier . split ( \":\" ) if not last_element_base_element == \"SWITCH\" : no_switch_found = True if no_switch_found : logger . info ( \"Could not identify switch at the end of the pipeline. Estimator Comparison aborted.\" ) return # generate config key by switch name search_key = last_element_name + \"__\" + \"estimator_name\" estimator_list = last_element_dict . keys () best_configs_from_estimators = dict () for estimator in estimator_list : best_configs_from_estimators [ estimator ] = list () # 2. iterate list and filter configs for outer_fold in self . results . outer_folds : for estimator_name in estimator_list : try : best_estimator_config = outer_fold . get_optimum_config ( metric = self . results . hyperpipe_info . best_config_metric , maximize_metric = self . results . hyperpipe_info . maximize_best_config_metric , dict_filter = ( search_key , estimator_name )) best_configs_from_estimators [ estimator_name ] . append ( best_estimator_config ) except Warning as w : logger . info ( \"Could not find best config for estimator {} \" \"in outer fold {} \" . format ( estimator_name , outer_fold . fold_nr )) return best_configs_from_estimators def get_n_best_validation_configs_per_estimator ( self , n = 10 , estimator_names = None ) -> dict : best_configs_from_estimator = self . _get_best_outer_fold_configs_per_estimator () if estimator_names : all_estimators = list ( best_configs_from_estimator . keys ()) for name in all_estimators : if name not in estimator_names : del best_configs_from_estimator [ name ] best_n_config_dict = dict () for estimator_name , estimator_list in best_configs_from_estimator . items (): if n > len ( estimator_list ): n_configs_per_estimator = [ c . config_dict for c in estimator_list ] else : sort_order = np . argsort ([[ c . get_test_metric ( self . results . hyperpipe_info . best_config_metric , 'mean' ) for c in estimator_list ]]) n_configs_per_estimator = [ estimator_list [ idx ] . config_dict for idx in sort_order [: n ]] best_n_config_dict [ estimator_name ] = n_configs_per_estimator print_config_list_table ( estimator_name , n_configs_per_estimator ) return best_n_config_dict def get_mean_of_best_validation_configs_per_estimator ( self , write_to_file = False ): best_configs_from_estimators = self . _get_best_outer_fold_configs_per_estimator () # get mean values for each metric for each estimator config list estimator_performance_values = dict () for estimator_name , estimator_config_list in best_configs_from_estimators . items (): estimator_performance_values [ estimator_name ] = dict () for metric in self . results . hyperpipe_info . metrics : performance_values = [ c . get_test_metric ( metric , 'mean' ) for c in estimator_config_list ] estimator_performance_values [ estimator_name ][ metric ] = np . mean ( performance_values ) output = print_estimator_metrics ( estimator_performance_values , self . results . hyperpipe_info . metrics , True ) if write_to_file : text_file = open ( os . path . join ( self . output_settings . results_folder , \"mean_best_estimator_performance.txt\" ), \"w\" ) text_file . write ( output ) text_file . close () return output def text_summary ( self ): def divider ( header ): return header . ljust ( 101 , '=' ) output_string = divider ( \"ANALYSIS INFORMATION \" ) elapsed_time = self . results . computation_end_time - self . results . computation_start_time output_string += \"\"\" Project Folder: {} , Computation Time: {} - {} Duration: {} Optimized for: {} Hyperparameter Optimizer: {} \"\"\" . format ( self . output_settings . results_folder , self . results . computation_start_time , self . results . computation_end_time , elapsed_time , self . results . hyperpipe_info . best_config_metric , self . results . hyperpipe_info . optimization [ \"Optimizer\" ]) output_string += divider ( \"DUMMY RESULTS \" ) output_string += \"\"\" {} \"\"\" . format ( print_metrics ( \"DUMMY\" , self . results . dummy_estimator . get_test_metric ( operation = 'mean' ), summary = True )) output_string += divider ( \"AVERAGE PERFORMANCE ACROSS OUTER FOLDS \" ) test_metrics = self . results . get_test_metric_dict () train_metrics = self . results . get_train_metric_dict () output_string += \"\"\" {} \"\"\" . format ( self . print_table_for_performance_overview ( train_metrics , test_metrics )) output_string += divider ( \"BEST HYPERPARAMETER CONFIGURATION \" ) output_string += \"\"\" {} \"\"\" . format ( json . dumps ( self . results . best_config . human_readable_config , indent = 4 , sort_keys = True )) output_string += \"\"\" {} \"\"\" . format ( print_outer_folds ( self . results . hyperpipe_info . metrics , self . results . outer_folds , summary = True )) output_string += divider ( \"PHOTONAI {} \" . format ( __version__ )) if self . output_settings . results_folder is not None : output_string += \" \\n Your results are stored in \" + self . output_settings . results_folder + \" \\n \" output_string += \"Go to https://explorer.photon-ai.com and upload your photon_result_file.json \" \\ \"for convenient result visualization! \\n \" output_string += \"For more info and documentation visit https://www.photon-ai.com\" if self . output_settings . save_output : try : summary_filename = os . path . join ( self . output_settings . results_folder , 'photon_summary.txt' ) text_file = open ( summary_filename , \"w\" ) text_file . write ( output_string ) text_file . close () except OSError as e : logger . error ( \"Could not write summary file\" ) logger . error ( str ( e )) return output_string @staticmethod def print_table_for_performance_overview ( metric_dict_train , metric_dict_test ): x = PrettyTable () x . field_names = [ \"Metric Name\" , \"Training Mean\" , \"Training Std\" , \"Test Mean\" , \"Test Std\" ] for element_key , element_dict in metric_dict_train . items (): x . add_row ([ element_key , element_dict [ \"mean\" ], element_dict [ \"std\" ], metric_dict_test [ element_key ][ \"mean\" ], metric_dict_test [ element_key ][ \"std\" ]]) return x __init__ ( self , results_object = None , output_settings = None ) special Initialize the object. Parameters: Name Type Description Default results_object MDBHyperpipe All results are stored here. An initial setting is not necessary, because a later loading via file or MongoDB is possible. None output_settings OutputSettings Setting for creation and storage of the results_object. None Source code in photonai/processing/results_handler.py def __init__ ( self , results_object : MDBHyperpipe = None , output_settings = None ): \"\"\" Initialize the object. Parameters: results_object: All results are stored here. An initial setting is not necessary, because a later loading via file or MongoDB is possible. output_settings (OutputSettings): Setting for creation and storage of the results_object. \"\"\" self . results = results_object self . output_settings = output_settings get_config_evaluations ( self ) Return the test performance of every tested configuration in every outer fold. Returns: Type Description dict Test performance of every configuration. Source code in photonai/processing/results_handler.py def get_config_evaluations ( self ) -> dict : \"\"\" Return the test performance of every tested configuration in every outer fold. Returns: Test performance of every configuration. \"\"\" config_performances = list () maximum_fold = None for outer_fold in self . results . outer_folds : if maximum_fold is None or len ( outer_fold . tested_config_list ) > maximum_fold : maximum_fold = len ( outer_fold . tested_config_list ) for outer_fold in self . results . outer_folds : performance = dict () for metric in self . results . hyperpipe_info . metrics : performance [ metric ] = list () for i in range ( maximum_fold ): # for config in outer_fold.tested_config_list: for metric in self . results . hyperpipe_info . metrics : if i >= len ( outer_fold . tested_config_list ): performance [ metric ] . append ( np . nan ) continue config = outer_fold . tested_config_list [ i ] if config . config_failed : performance [ metric ] . append ( np . nan ) else : for item in config . metrics_test : if ( item . operation == 'mean' ) and ( item . metric_name == metric ): performance [ metric ] . append ( item . value ) config_performances . append ( performance ) config_performances_dict = dict () for metric in self . results . hyperpipe_info . metrics : config_performances_dict [ metric ] = list () for fold in config_performances : config_performances_dict [ metric ] . append ( fold [ metric ]) return config_performances_dict get_methods () staticmethod This function returns a list of all methods available for ResultsHandler. Returns: Type Description list List of all available methods. Source code in photonai/processing/results_handler.py @staticmethod def get_methods () -> list : \"\"\" This function returns a list of all methods available for ResultsHandler. Returns: List of all available methods. \"\"\" methods_list = [ s for s in dir ( ResultsHandler ) if '__' not in s ] return methods_list get_performance_table ( self ) This function returns a summary table of the overall results. ToDo: add best_config information! Source code in photonai/processing/results_handler.py def get_performance_table ( self ): \"\"\"This function returns a summary table of the overall results. ToDo: add best_config information! \"\"\" res_tab = pd . DataFrame () for i , folds in enumerate ( self . results . outer_folds ): # add best config infos res_tab . loc [ i , 'best_config' ] = str ( folds . best_config . human_readable_config ) # add fold index res_tab . loc [ i , 'fold' ] = folds . fold_nr # add sample size infos res_tab . loc [ i , 'n_train' ] = folds . best_config . best_config_score . number_samples_training res_tab . loc [ i , 'n_validation' ] = folds . best_config . best_config_score . number_samples_validation # add performance metrics d = folds . best_config . best_config_score . validation . metrics for key , value in d . items (): res_tab . loc [ i , key ] = value # add row with overall info res_tab . loc [ i + 1 , 'n_validation' ] = np . sum ( res_tab [ 'n_validation' ]) for key , value in d . items (): m = res_tab . loc [:, key ] res_tab . loc [ i + 1 , key ] = np . mean ( m ) res_tab . loc [ i + 1 , key + '_sem' ] = sem ( m ) # standard error of the mean res_tab . loc [ i + 1 , 'best_config' ] = 'Overall' return res_tab load_from_file ( self , results_file ) Read results_file from json into MDBHyperpipe object self.results. Parameters: Name Type Description Default results_file str Full path to json file. required Source code in photonai/processing/results_handler.py def load_from_file ( self , results_file : str ): \"\"\" Read results_file from json into MDBHyperpipe object self.results. Parameters: results_file: Full path to json file. \"\"\" self . results = MDBHyperpipe . from_document ( json . load ( open ( results_file , 'r' ))) load_from_mongodb ( self , mongodb_connect_url , pipe_name ) Read results_file from MongoDB into MDBHyperpipe object self.results. Parameters: Name Type Description Default mongodb_connect_url str MongoDB connection string. required pipe_name str Name of the stored hyperpipe. required Source code in photonai/processing/results_handler.py def load_from_mongodb ( self , mongodb_connect_url : str , pipe_name : str ): \"\"\" Read results_file from MongoDB into MDBHyperpipe object self.results. Parameters: mongodb_connect_url: MongoDB connection string. pipe_name: Name of the stored hyperpipe. \"\"\" connect ( mongodb_connect_url , alias = \"photon_core\" ) results = list ( MDBHyperpipe . objects . raw ({ 'name' : pipe_name })) if len ( results ) == 1 : self . results = results [ 0 ] elif len ( results ) > 1 : self . results = MDBHyperpipe . objects . order_by ([( \"computation_start_time\" , DESCENDING )]) . raw ({ 'name' : pipe_name }) . first () warn_text = 'Found multiple hyperpipes with that name. Returning most recent one.' logger . warning ( warn_text ) warnings . warn ( warn_text ) else : raise FileNotFoundError ( 'Could not load hyperpipe from MongoDB.' )","title":"Results Handler"},{"location":"api/processing/results_handler/#documentation-for-resultshandler","text":"Provides all functions that operate on calculated results. As IO for the results object the ResultsHandler is able to handle results on its own. Source code in photonai/processing/results_handler.py class ResultsHandler : \"\"\" Provides all functions that operate on calculated results. As IO for the results object the ResultsHandler is able to handle results on its own. \"\"\" def __init__ ( self , results_object : MDBHyperpipe = None , output_settings = None ): \"\"\" Initialize the object. Parameters: results_object: All results are stored here. An initial setting is not necessary, because a later loading via file or MongoDB is possible. output_settings (OutputSettings): Setting for creation and storage of the results_object. \"\"\" self . results = results_object self . output_settings = output_settings def load_from_file ( self , results_file : str ): \"\"\" Read results_file from json into MDBHyperpipe object self.results. Parameters: results_file: Full path to json file. \"\"\" self . results = MDBHyperpipe . from_document ( json . load ( open ( results_file , 'r' ))) def load_from_mongodb ( self , mongodb_connect_url : str , pipe_name : str ): \"\"\" Read results_file from MongoDB into MDBHyperpipe object self.results. Parameters: mongodb_connect_url: MongoDB connection string. pipe_name: Name of the stored hyperpipe. \"\"\" connect ( mongodb_connect_url , alias = \"photon_core\" ) results = list ( MDBHyperpipe . objects . raw ({ 'name' : pipe_name })) if len ( results ) == 1 : self . results = results [ 0 ] elif len ( results ) > 1 : self . results = MDBHyperpipe . objects . order_by ([( \"computation_start_time\" , DESCENDING )]) . raw ({ 'name' : pipe_name }) . first () warn_text = 'Found multiple hyperpipes with that name. Returning most recent one.' logger . warning ( warn_text ) warnings . warn ( warn_text ) else : raise FileNotFoundError ( 'Could not load hyperpipe from MongoDB.' ) @staticmethod def get_methods () -> list : \"\"\" This function returns a list of all methods available for ResultsHandler. Returns: List of all available methods. \"\"\" methods_list = [ s for s in dir ( ResultsHandler ) if '__' not in s ] return methods_list def get_performance_table ( self ): \"\"\"This function returns a summary table of the overall results. ToDo: add best_config information! \"\"\" res_tab = pd . DataFrame () for i , folds in enumerate ( self . results . outer_folds ): # add best config infos res_tab . loc [ i , 'best_config' ] = str ( folds . best_config . human_readable_config ) # add fold index res_tab . loc [ i , 'fold' ] = folds . fold_nr # add sample size infos res_tab . loc [ i , 'n_train' ] = folds . best_config . best_config_score . number_samples_training res_tab . loc [ i , 'n_validation' ] = folds . best_config . best_config_score . number_samples_validation # add performance metrics d = folds . best_config . best_config_score . validation . metrics for key , value in d . items (): res_tab . loc [ i , key ] = value # add row with overall info res_tab . loc [ i + 1 , 'n_validation' ] = np . sum ( res_tab [ 'n_validation' ]) for key , value in d . items (): m = res_tab . loc [:, key ] res_tab . loc [ i + 1 , key ] = np . mean ( m ) res_tab . loc [ i + 1 , key + '_sem' ] = sem ( m ) # standard error of the mean res_tab . loc [ i + 1 , 'best_config' ] = 'Overall' return res_tab def get_performance_outer_folds ( self ): performances = dict () for metric in self . results . outer_folds [ 0 ] . best_config . best_config_score . validation . metrics . keys (): performances [ metric ] = list () for i , fold in enumerate ( self . results . outer_folds ): for metric , value in fold . best_config . best_config_score . validation . metrics . items (): performances [ metric ] . append ( value ) return performances def get_config_evaluations ( self ) -> dict : \"\"\" Return the test performance of every tested configuration in every outer fold. Returns: Test performance of every configuration. \"\"\" config_performances = list () maximum_fold = None for outer_fold in self . results . outer_folds : if maximum_fold is None or len ( outer_fold . tested_config_list ) > maximum_fold : maximum_fold = len ( outer_fold . tested_config_list ) for outer_fold in self . results . outer_folds : performance = dict () for metric in self . results . hyperpipe_info . metrics : performance [ metric ] = list () for i in range ( maximum_fold ): # for config in outer_fold.tested_config_list: for metric in self . results . hyperpipe_info . metrics : if i >= len ( outer_fold . tested_config_list ): performance [ metric ] . append ( np . nan ) continue config = outer_fold . tested_config_list [ i ] if config . config_failed : performance [ metric ] . append ( np . nan ) else : for item in config . metrics_test : if ( item . operation == 'mean' ) and ( item . metric_name == metric ): performance [ metric ] . append ( item . value ) config_performances . append ( performance ) config_performances_dict = dict () for metric in self . results . hyperpipe_info . metrics : config_performances_dict [ metric ] = list () for fold in config_performances : config_performances_dict [ metric ] . append ( fold [ metric ]) return config_performances_dict def get_minimum_config_evaluations ( self ): config_evaluations = self . get_config_evaluations () minimum_config_evaluations = dict () for metric , evaluations in config_evaluations . items (): minimum_config_evaluations [ metric ] = list () greater_is_better = Scorer . greater_is_better_distinction ( metric ) for fold in evaluations : fold_evaluations = list () if greater_is_better : for i , config in enumerate ( fold ): if i == 0 : last_config = config else : if config > last_config : last_config = config fold_evaluations . append ( last_config ) else : last_config = np . inf for i , config in enumerate ( fold ): if i == 0 : last_config = config else : if config < last_config : last_config = config fold_evaluations . append ( last_config ) minimum_config_evaluations [ metric ] . append ( fold_evaluations ) return minimum_config_evaluations def get_learning_curves ( self , config_nr , outer_fold_nr , save ): \"\"\"This function gets the learning curves out of the result tree. It returns the learning curves as a pandas dataframe. If save = True it saves the learning curves as a csv file. \"\"\" cuts = self . results . hyperpipe_info . learning_curves_cut . values [ 1 :] + [ 1. ] fold_num = len ( self . results . outer_folds [ 0 ] . tested_config_list [ config_nr - 1 ] . inner_folds ) idx = pd . MultiIndex . from_product ([ cuts , [ i + 1 for i in range ( fold_num )]], names = [ 'Cut' , 'Inner Fold Nr.' ]) col = pd . MultiIndex . from_product ([ self . results . hyperpipe_info . metrics , [ 'test' , 'train' ]]) data = {} for metric in self . results . hyperpipe_info . metrics : config = self . results . outer_folds [ outer_fold_nr - 1 ] . tested_config_list [ config_nr - 1 ] for t in [ 1 , 2 ]: curves = [] for cut_nr , cut in enumerate ( cuts ): curves += [ config . inner_folds [ fold ] . learning_curves [ cut_nr ][ t ][ metric ] for fold in range ( fold_num )] data . update ({( metric , [ 'test' , 'train' ][ t - 1 ]): curves }) curves = pd . DataFrame ( data , index = idx , columns = col ) if save : curves . to_csv ( self . _save_prep_learning_curves ( 'lc_outer_fold_ %d _config_ %d .csv' % ( outer_fold_nr , config_nr ))) return curves def plot_curves ( self , curves : pd . DataFrame , title : str = 'Learning Curves' ): \"\"\"This function plots the learning curves. Parameters: curves: Dataframe with multi-index: (run - fraction of data) columns: at least (metric, train/test) floats title: Subtitle of plot. \"\"\" metrics = self . results . hyperpipe_info . metrics fig , axes = plt . subplots ( 1 , len ( metrics ), figsize = ( len ( metrics ) * 4. , 4. )) if len ( metrics ) == 1 : axes = [ axes ] cuts = curves . index . get_level_values ( 0 ) col_template = tuple ( curves . columns [ 0 ]) # iterate only over first 2 entries [(metric, train/test)], example 'mean' as third for metric , ax in zip ( metrics , axes ): for subset in [ 'test' , 'train' ]: sns . lineplot ( x = cuts , y = curves [( metric , subset ) + col_template [ 2 :]], label = metric + '_' + subset , ax = ax ) ax . set ( xlabel = 'Fraction of Train Data used' , ylabel = 'Metric Value' ) ax . legend ( fontsize = 'small' ) plt . suptitle ( title ) plt . tight_layout ( rect = [ 0 , 0.03 , 1 , 0.95 ]) return fig def plot_learning_curves_config ( self , config_nr , outer_fold_nr , save , show = False ): \"\"\"This function gets the learning curves for a specific config nr. and outer fold nr. and plots them If config_nr = -1 it gets the best config of the outer fold If save = True the plot is saved If show = True the plot is shown \"\"\" if config_nr == - 1 : config_nr = self . results . best_config . config_nr curves = self . get_learning_curves ( config_nr , outer_fold_nr , save ) curves . columns = curves . columns . to_flat_index () fig = self . plot_curves ( curves , 'Learning Curves (Outer Fold Nr. %d Config Nr. %d )' % ( outer_fold_nr , config_nr )) if save : plt . savefig ( self . _save_prep_learning_curves ( 'lc_outer_fold_ %d _config_ %d .png' % ( outer_fold_nr , config_nr ))) if show : plt . show () plt . close () def plot_learning_curves_outer_fold ( self , outer_fold_nr , config_nr_list = None , save = True , show = False ): \"\"\"This function gets the learning curves for a list of configs in a specific outer fold and plots them For each config the mean of the learning curves of all inner folds is used If config_nr = -1 it gets the best config of the outer fold If save = True the plot is saved If show = True the plot is shown \"\"\" if config_nr_list is None : config_nr_list = np . arange ( 1 , len ( self . results . outer_folds [ outer_fold_nr - 1 ] . tested_config_list ) + 1 ) elif - 1 in config_nr_list : config_nr_list = [ nr for nr in config_nr_list if nr is not self . results . best_config . config_nr ] config_nr_list [ config_nr_list == - 1 ] = self . results . best_config . config_nr curves_list = [] for config_nr in config_nr_list : curves = self . get_learning_curves ( config_nr , outer_fold_nr , save ) curves_list . append ( curves . groupby ( level = 0 ) . agg ([ 'mean' ])) curves_configs = pd . concat ( curves_list , axis = 0 , names = [ \"Config Nr.\" ], keys = config_nr_list ) curves_configs . columns = curves_configs . columns . to_flat_index () curves_configs = curves_configs . swaplevel () fig = self . plot_curves ( curves_configs , 'Learning Curves (Outer Fold Nr. %d )' % outer_fold_nr ) if save : curves_configs . to_csv ( self . _save_prep_learning_curves ( 'lc_outer_fold_ {} .csv' . format ( outer_fold_nr ))) plt . savefig ( self . _save_prep_learning_curves ( 'lc_outer_fold_ {} .png' . format ( outer_fold_nr ))) if show : plt . show () plt . close () def _save_prep_learning_curves ( self , file_name ): path = self . results . output_folder + '/learning_curves/' if not os . path . exists ( path ): os . makedirs ( path ) return os . path . join ( path , file_name ) def save_all_learning_curves ( self ): for outer_fold_nr in range ( 1 , len ( self . results . outer_folds ) + 1 ): for config_nr in range ( 1 , len ( self . results . outer_folds [ 0 ] . tested_config_list ) + 1 ): self . plot_learning_curves_config ( config_nr , outer_fold_nr , save = True ) def plot_optimizer_history ( self , metric , title : str = 'Optimizer History' , type : str = 'plot' , reduce_scatter_by : Union [ int , str ] = 'auto' , file : str = None ): \"\"\" :param metric: specify metric that has been stored within the PHOTONAI results tree :param type: 'plot' or 'scatter' :param reduce_scatter_by: integer or string ('auto'), reduce the number of points plotted by scatter :param file: specify a filename if you want to save the plot :return: \"\"\" if metric not in self . results . hyperpipe_info . metrics : raise ValueError ( 'Metric \" {} \" not stored in results tree' . format ( metric )) config_evaluations = self . get_config_evaluations () minimum_config_evaluations = self . get_minimum_config_evaluations () # handle different lengths min_corresponding = len ( min ( config_evaluations [ metric ], key = len )) config_evaluations_corres = [ configs [: min_corresponding ] for configs in config_evaluations [ metric ]] minimum_config_evaluations_corres = [ configs [: min_corresponding ] for configs in minimum_config_evaluations [ metric ]] mean = np . nanmean ( np . asarray ( config_evaluations_corres ), axis = 0 ) mean_min = np . nanmean ( np . asarray ( minimum_config_evaluations_corres ), axis = 0 ) greater_is_better = Scorer . greater_is_better_distinction ( metric ) if greater_is_better : caption = 'Maximum' else : caption = 'Minimum' plt . figure () if type == 'plot' : plt . plot ( np . arange ( 0 , len ( mean )), mean , '-' , color = 'gray' , label = 'Mean Performance' ) elif type == 'scatter' : # now do smoothing if isinstance ( reduce_scatter_by , str ): if reduce_scatter_by != 'auto' : msg = ' {} is not a valid smoothing_kernel specifier. ' \\ 'Falling back to \"auto\".' . format ( reduce_scatter_by ) logger . warning ( msg ) warnings . warn ( msg ) # if auto, then calculate size of reduce_scatter_by so that 75 points on x remain # smallest reduce_scatter_by should be 1 reduce_scatter_by = max ([ np . floor ( min_corresponding / 75 ) . astype ( int ), 1 ]) if reduce_scatter_by > 1 : plt . plot ([], [], ' ' , label = \"scatter reduced by factor {} \" . format ( reduce_scatter_by )) for i , fold in enumerate ( config_evaluations [ metric ]): # add a few None so that list can be divided by smoothing_kernel remaining = len ( fold ) % reduce_scatter_by if remaining : fold . extend ([ np . nan ] * ( reduce_scatter_by - remaining )) # calculate mean over every n named_steps so that plot is less cluttered reduced_fold = np . nanmean ( np . asarray ( fold ) . reshape ( - 1 , reduce_scatter_by ), axis = 1 ) reduced_xfit = np . arange ( reduce_scatter_by / 2 , len ( fold ), step = reduce_scatter_by ) if i == len ( config_evaluations [ metric ]) - 1 : plt . scatter ( reduced_xfit , np . asarray ( reduced_fold ), color = 'gray' , alpha = 0.5 , label = 'Performance' , marker = '.' ) else : plt . scatter ( reduced_xfit , np . asarray ( reduced_fold ), color = 'gray' , alpha = 0.5 , marker = '.' ) else : raise ValueError ( 'Please specify either \"plot\" or \"scatter\".' ) plt . plot ( np . arange ( 0 , len ( mean_min )), mean_min , '-' , color = 'black' , label = 'Mean {} Performance' . format ( caption )) for i , fold in enumerate ( minimum_config_evaluations [ metric ]): xfit = np . arange ( 0 , len ( fold )) plt . plot ( xfit , fold , '-' , color = 'black' , alpha = 0.5 ) plt . ylabel ( metric . replace ( '_' , ' ' )) plt . xlabel ( 'No of Evaluations' ) plt . legend () plt . title ( title ) if file : plt . savefig ( file ) else : file = os . path . join ( self . results . output_folder , \"optimizer_history.png\" ) plt . savefig ( file ) plt . close () def get_importance_scores ( self ): \"\"\" This function returns the importance scores for the best configuration of each outer fold. \"\"\" imps = [] for i , fold in enumerate ( self . results . outer_folds ): imps . append ( fold . best_config . best_config_score . feature_importances ) return imps @staticmethod def collect_fold_lists ( score_info_list , fold_nr , predictions_filename = '' ): if len ( score_info_list ) > 0 : fold_nr_array = [] collectables = { 'y_pred' : [], 'y_true' : [], 'indices' : [], 'probabilities' : []} for i , score_info in enumerate ( score_info_list ): for collectable_key , collectable_list in collectables . items (): if getattr ( score_info , collectable_key ) is not None and len ( getattr ( score_info , collectable_key )) > 0 : collectables [ collectable_key ] . extend ( list ( getattr ( score_info , collectable_key ))) else : collectables [ collectable_key ] . extend ( list ( np . full (( len ( score_info . y_true )), np . nan ))) fold_nr_array . extend ( list ( np . ones (( len ( score_info . y_true ),)) * fold_nr [ i ])) # enable nd y_pred support if len ( collectables [ \"y_pred\" ]) > len ( collectables [ \"y_true\" ]): tmp_collectables_y_pred = collectables [ \"y_pred\" ] headers = collectables [ \"y_pred\" ][ 0 ] for i , header in enumerate ( list ( headers )): collectables [ header ] = [ x [ i ] for x in tmp_collectables_y_pred if x != tmp_collectables_y_pred [ 0 ]] collectables [ \"fold\" ] = fold_nr_array # convert to pandas dataframe to use their sorting algorithm save_df = pd . DataFrame ( collectables ) sorted_df = save_df . sort_values ( by = 'indices' ) if predictions_filename != '' : sorted_df . to_csv ( predictions_filename , index = None ) return sorted_df . to_dict ( 'list' ) def get_mean_train_predictions ( self , filename = '' ): \"\"\" This function returns the MEAN predictions, true targets, and fold index for the TRAINING Set of the best configuration of each outer fold. \"\"\" if self . results is None : raise ValueError ( \"Result tree information is needed but results attribute of object is None.\" ) score_info_list = list () fold_nr_list = list () for outer_fold in self . results . outer_folds : score_info_list . append ( outer_fold . best_config . best_config_score . training ) fold_nr_list . append ( outer_fold . fold_nr ) infos = self . collect_fold_lists ( score_info_list , fold_nr_list , filename ) infos = { key : np . array ( value ) for key , value in infos . items ()} num_items = np . unique ( infos [ \"indices\" ]) mean_pred = np . zeros ( num_items . shape ) y_true = np . zeros ( num_items . shape ) for i in num_items : idx = ( infos [ \"indices\" ] == i ) mean_pred [ i ] = np . mean ( infos [ \"y_pred\" ][ idx ]) y_true [ i ] = infos [ \"y_true\" ][ idx ][ 0 ] return { 'y_true' : y_true , 'y_pred' : mean_pred , 'indices' : num_items } def get_test_predictions ( self , filename = '' ): \"\"\" This function returns the predictions, true targets, and fold index for the best configuration of each outer fold. \"\"\" if self . results is None : raise ValueError ( \"Result tree information is needed but results attribute of object is None.\" ) score_info_list = list () fold_nr_list = list () for outer_fold in self . results . outer_folds : score_info_list . append ( outer_fold . best_config . best_config_score . validation ) fold_nr_list . append ( outer_fold . fold_nr ) return self . collect_fold_lists ( score_info_list , fold_nr_list , filename ) def get_validation_predictions ( self , outer_fold_nr = 0 , config_no = 0 , filename = '' ): \"\"\" This function returns the predictions, probabilities, true targets, fold and index for the config_nr of the given outer_fold \"\"\" score_info_list = list () fold_nr_list = list () if self . results is None : raise ValueError ( \"Result tree information is needed but results attribute of object is None.\" ) # Todo: find config by config_id for inner_fold in self . results . outer_folds [ outer_fold_nr ] . tested_config_list [ config_no ] . inner_folds : score_info_list . append ( inner_fold . validation ) fold_nr_list . append ( inner_fold . fold_nr ) return self . collect_fold_lists ( score_info_list , fold_nr_list , filename ) def eval_mean_time_components ( self , write_results = True , plotly_return = False ): \"\"\" This function create charts and tables out of the time-monitoring. \"\"\" result_dict = {} caching = False default_dict = { 'total_seconds' : 0 , 'total_items_processed' : 0 , 'mean_seconds_per_config' : 0 , 'mean_seconds_per_item' : 0 } # sum up times per element, 1. per config, and 2. in total for outer_fold in self . results . outer_folds : for config_nr , config in enumerate ( outer_fold . tested_config_list ): tmp_config_dict = {} # resort time entries for each element so that is has the following structure # element_name -> fit/transform/predict -> (seconds, nr_items) for inner_fold in config . inner_folds : for time_key , time_values in inner_fold . time_monitor . items (): for value_item in time_values : name , time , nr_items = value_item [ 0 ], value_item [ 1 ], value_item [ 2 ] if name not in tmp_config_dict : tmp_config_dict [ name ] = {} if time_key not in tmp_config_dict [ name ]: tmp_config_dict [ name ][ time_key ] = [] tmp_config_dict [ name ][ time_key ] . append (( time , nr_items )) # calculate mean time per config and absolute time for element_name , element_time_dict in tmp_config_dict . items (): for element_time_key , element_time_list in element_time_dict . items (): if element_time_key == \"transform_cached\" : caching = True mean_time = np . mean ([ i [ 0 ] for i in element_time_list ]) total_time = np . sum ([ i [ 0 ] for i in element_time_list ]) total_items_processed = np . sum ([ i [ 1 ] for i in element_time_list ]) if element_name not in result_dict : result_dict [ element_name ] = {} if element_time_key not in result_dict [ element_name ]: result_dict [ element_name ][ element_time_key ] = dict ( default_dict ) result_dict [ element_name ][ element_time_key ][ 'total_seconds' ] += total_time result_dict [ element_name ][ element_time_key ][ 'total_items_processed' ] += total_items_processed mean_time_per_config = result_dict [ element_name ][ element_time_key ][ 'mean_seconds_per_config' ] tmp_total_mean = (( mean_time_per_config * config_nr ) + mean_time ) / ( config_nr + 1 ) result_dict [ element_name ][ element_time_key ][ 'mean_seconds_per_config' ] = tmp_total_mean tmp_mean_per_item = result_dict [ element_name ][ element_time_key ][ 'total_seconds' ] / \\ result_dict [ element_name ][ element_time_key ][ 'total_items_processed' ] result_dict [ element_name ][ element_time_key ][ 'mean_seconds_per_item' ] = tmp_mean_per_item format_str = ' {:06.6f} ' if caching : # in case we used caching add transform_cached and transform_computed values to transform_total for name , sub_result_dict in result_dict . items (): if \"transform_cached\" in sub_result_dict : result_dict [ name ][ \"transform\" ] = dict ( default_dict ) for value_dict in sub_result_dict . values (): for info in value_dict . keys (): result_dict [ name ][ \"transform\" ][ info ] = result_dict [ name ][ \"transform_cached\" ][ info ] # in case everything's been in the cache we have no computation if \"transform_computed\" in sub_result_dict : result_dict [ name ][ \"transform\" ][ info ] += result_dict [ name ][ \"transform_computed\" ][ info ] if \"transform_computed\" in sub_result_dict : # calculate a ratio, if caching was helpful and how much of the time it saved result_dict [ name ][ \"cache_ratio\" ] = result_dict [ name ][ \"transform_cached\" ][ \"total_seconds\" ] / \\ result_dict [ name ][ \"transform_computed\" ][ \"total_seconds\" ] # in case of caching we have different plot plus a different csv file csv_keys = [ \"fit\" , \"transform\" , \"transform_computed\" , \"transform_cached\" , \"predict\" ] csv_titles = csv_keys plot_list = [ \"fit\" , \"transform\" , \"transform_cached\" ] method_list = [ \"fit\" , \"transform_computed\" , \"transform_cached\" , \"predict\" ] else : csv_keys = [ \"fit\" , \"transform_computed\" , \"predict\" ] csv_titles = [ \"fit\" , \"transform\" , \"predict\" ] plot_list = [ \"fit\" , \"transform_computed\" ] method_list = [ \"fit\" , \"transform_computed\" , \"predict\" ] # write csv file with time analysis if write_results : sub_keys = [ \"total_seconds\" , \"mean_seconds_per_config\" , \"mean_seconds_per_item\" ] csv_filename = os . path . join ( self . results . output_folder , 'time_monitor.csv' ) with open ( csv_filename , 'w' ) as csvfile : writer = csv . writer ( csvfile ) header1 = [ \"\" ] for k_name in csv_titles : header1 . extend ([ k_name , \"\" , \"\" ]) header2 = [ \"Element\" ] + ( sub_keys * len ( csv_titles )) if caching : header1 . append ( \"\" ) header2 . append ( \"cache_ratio\" ) writer . writerow ( header1 ) writer . writerow ( header2 ) for item , item_dict in result_dict . items (): row = [ item ] for time_key in csv_keys : for sub_key in sub_keys : if time_key in item_dict : row . append ( format_str . format ( item_dict [ time_key ][ sub_key ])) else : row . append ( '' ) if caching : if \"cache_ratio\" in item_dict : row . append ( item_dict [ \"cache_ratio\" ]) writer . writerow ( row ) # plot figure # TODO! Use PiePlotlyPlot class without cricle imports plotly_dict = { 'layout' : { 'title' : 'Time Monitor Pie Chart' , 'showlegend' : True , 'height' : 600 , 'annotations' : []}, 'data' : [] } def append_plotly ( labels , values , name , colors , domain ): \"\"\" helper function (temporary -> to.do above) \"\"\" plotly_dict [ \"data\" ] . append ({ 'labels' : labels , 'values' : values , 'type' : 'pie' , 'name' : name , 'marker' : { 'colors' : colors }, 'domain' : domain , 'hoverinfo' : 'label+percent' , 'textposition' : 'inside' }) plotly_dict [ 'layout' ][ 'annotations' ] . append ({ \"x\" : np . mean ( domain [ \"x\" ]), \"y\" : ( domain [ \"y\" ][ 1 ]), \"font\" : { \"size\" : 16 }, \"text\" : name , \"xref\" : \"paper\" , \"yref\" : \"paper\" , \"xanchor\" : \"center\" , \"yanchor\" : \"bottom\" , \"showarrow\" : False }) def eval_mean_time_autopct ( values ): def my_autopct ( pct ): total = sum ( values ) if pct / total >= 1 : return str ( round ( pct , 1 )) + \"%\" else : return None return my_autopct # Create nxm sub plots cpl = len ( plot_list ) gs = matplotlib . gridspec . GridSpec ( int (( cpl - 1 ) / 3 ) + 2 , min ( cpl , 3 )) legend_theme = plt . get_cmap ( 'Set3' ) legend_theme2 = plt . get_cmap ( 'tab10' ) element_names = [ name for name , element in result_dict . items ()] fig = plt . figure ( figsize = ( 10 , 7 ), dpi = 160 ) colors = [ legend_theme ( 1. * i / len ( element_names )) for i in range ( len ( element_names ))] for i , k in enumerate ( plot_list ): ax = plt . subplot ( gs [ int ( i / 3 ), i % 3 ]) ax . set_prop_cycle ( \"color\" , colors ) data = [ element [ k ][ \"total_seconds\" ] if k in element else 0 for name , element in result_dict . items ()] data_sum = sum ( data ) if data_sum == 0 : data_sum = 1 values = [ val / data_sum for val in data ] patches , _ , _ = plt . pie ( values , shadow = True , startangle = 90 , autopct = eval_mean_time_autopct ( data ), pctdistance = 0.7 ) plt . axis ( 'equal' ) plt . title ( k ) append_plotly ( labels = [ str ( d ) for d in element_names ], values = values , name = k , colors = colors , domain = { 'x' : [ i / len ( plot_list ), ( i + 1 ) / len ( plot_list )], 'y' : [ 0.55 , 1 ]}) plt . legend ( loc = 'upper left' , labels = [ ' %s ' % l for l in element_names ], prop = { 'size' : 10 }, bbox_to_anchor = ( 0.0 , 1 ), bbox_transform = fig . transFigure ) # add another plot for the comparison of the fit/transform/predict methods ax2 = plt . subplot ( gs [ int ( i / 3 ) + 1 , :]) colors = [ legend_theme2 ( 1. * i / len ( data )) for i in range ( len ( method_list ))] ax2 . set_prop_cycle ( \"color\" , colors ) data = [] for k in method_list : data . append ( np . sum ([ element [ k ][ \"total_seconds\" ] for name , element in result_dict . items () if k in element ])) patches_an , _ , _ = plt . pie ([ val / sum ( data ) for val in data ], shadow = True , startangle = 90 , pctdistance = 0.7 , autopct = eval_mean_time_autopct ( data )) append_plotly ( labels = method_list , values = [ val / sum ( data ) for val in data ], name = \"methods\" , colors = colors , domain = { 'x' : [ 0 , 1 ], 'y' : [ 0 , 0.45 ]}) plt . axis ( 'equal' ) plt . title ( \"methods\" ) plt . legend ( loc = 'lower left' , labels = [ ' %s ' % l for l in method_list ], prop = { 'size' : 10 }, bbox_transform = fig . transFigure ) # for only one legend #fig.legend(patches+patches_an, element_names+method_list, prop={'size': 10}, loc='lower left') if write_results : plt . savefig ( os . path . join ( self . results . output_folder , 'time_monitor_pie.png' )) plt . close () if plotly_return : str_fig = \"var layout =\" + str ( plotly_dict [ \"layout\" ]) + \";\" str_fig += \"var data = \" + str ( plotly_dict [ \"data\" ]) + \";\" str_fig += \"Plotly.newPlot('\" + \"time_monitor_pie_id\" + \"',data, layout);\" return str_fig . replace ( \"False\" , \"false\" ) . replace ( \"True\" , \"true\" ) def save ( self ): if self . output_settings . mongodb_connect_url : connect ( self . output_settings . mongodb_connect_url , alias = 'photon_core' ) logger . info ( 'Write results to mongodb...' ) try : self . results . save () except DocumentTooLarge : logger . error ( 'Could not save document into MongoDB: Document too large' ) if self . output_settings . save_output : logger . info ( \"Writing results to project folder...\" ) self . write_result_tree_to_file () def save_backmapping ( self , filename : str , backmapping ): try : if isinstance ( backmapping , list ): backmapping = np . asarray ( backmapping ) try : from nibabel.nifti1 import Nifti1Image if isinstance ( backmapping , Nifti1Image ): backmapping . to_filename ( os . path . join ( self . results . output_folder , filename + '.nii.gz' )) except ImportError : pass finally : if isinstance ( backmapping , np . ndarray ): if backmapping . size > 1000 : np . savez ( os . path . join ( self . results . output_folder , filename + '.npz' ), backmapping ) else : np . savetxt ( os . path . join ( self . results . output_folder , filename + '.csv' ), backmapping , delimiter = ',' ) else : with open ( os . path . join ( self . results . output_folder , filename + '.p' ), 'wb' ) as f : pickle . dump ( backmapping , f ) except Exception as e : logger . error ( \"Could not save backmapped feature importances.\" ) logger . error ( e ) def write_convenience_files ( self ): if self . output_settings . save_output : logger . info ( \"Writing summary file, plots and prediction csv to result folder ...\" ) self . write_summary () self . write_predictions_file () def convert_to_json_serializable ( self , value ): if isinstance ( value , ( int , np . int32 , np . int64 )): return int ( value ) if isinstance ( value , ( float , np . float32 , np . float64 )): if self . output_settings . reduce_space : return round ( float ( value ), 3 ) return float ( value ) else : return json_util . default ( value ) def write_result_tree_to_file ( self ): try : local_file = os . path . join ( self . results . output_folder , 'photon_result_file.json' ) result = self . round_floats ( self . results . to_son () . to_dict ()) with open ( local_file , 'w' ) as outfile : json . dump ( result , outfile , default = self . convert_to_json_serializable ) except OSError as e : logger . error ( \"Could not write results to local file\" ) logger . error ( str ( e )) @classmethod def round_floats ( cls , d ): # recursive method for rounding all floats in result.json result = {} if isinstance ( d , dict ): for key , value in d . items (): value = cls . round_floats ( value ) result . update ({ key : value }) return result elif isinstance ( d , list ): return [ cls . round_floats ( val ) for val in d ] elif isinstance ( d , float ): return round ( d , 6 ) else : return d def get_best_config_inner_fold_predictions ( self , filename = '' ): score_info_list = [] fold_nr = [] for inner_fold in self . results . best_config . inner_folds : score_info_list . append ( inner_fold . validation ) fold_nr . append ( inner_fold . fold_nr ) return self . collect_fold_lists ( score_info_list , fold_nr , filename ) def write_predictions_file ( self ): if self . output_settings . save_output : filename = os . path . join ( self . output_settings . results_folder , 'best_config_predictions.csv' ) # usually we write the predictions for the outer fold if not self . output_settings . save_predictions_from_best_config_inner_folds : return self . get_test_predictions ( filename ) # in case no outer folds exist, we write the inner_fold predictions else : return self . get_best_config_inner_fold_predictions ( filename ) def _get_best_outer_fold_configs_per_estimator ( self ) -> dict : # 1. find out which estimators there are last_element_name_identifier , last_element_dict = list ( self . results . hyperpipe_info . elements . items ())[ - 1 ] no_switch_found = False if not \":\" in last_element_name_identifier : no_switch_found = True last_element_base_element , last_element_name = last_element_name_identifier . split ( \":\" ) if not last_element_base_element == \"SWITCH\" : no_switch_found = True if no_switch_found : logger . info ( \"Could not identify switch at the end of the pipeline. Estimator Comparison aborted.\" ) return # generate config key by switch name search_key = last_element_name + \"__\" + \"estimator_name\" estimator_list = last_element_dict . keys () best_configs_from_estimators = dict () for estimator in estimator_list : best_configs_from_estimators [ estimator ] = list () # 2. iterate list and filter configs for outer_fold in self . results . outer_folds : for estimator_name in estimator_list : try : best_estimator_config = outer_fold . get_optimum_config ( metric = self . results . hyperpipe_info . best_config_metric , maximize_metric = self . results . hyperpipe_info . maximize_best_config_metric , dict_filter = ( search_key , estimator_name )) best_configs_from_estimators [ estimator_name ] . append ( best_estimator_config ) except Warning as w : logger . info ( \"Could not find best config for estimator {} \" \"in outer fold {} \" . format ( estimator_name , outer_fold . fold_nr )) return best_configs_from_estimators def get_n_best_validation_configs_per_estimator ( self , n = 10 , estimator_names = None ) -> dict : best_configs_from_estimator = self . _get_best_outer_fold_configs_per_estimator () if estimator_names : all_estimators = list ( best_configs_from_estimator . keys ()) for name in all_estimators : if name not in estimator_names : del best_configs_from_estimator [ name ] best_n_config_dict = dict () for estimator_name , estimator_list in best_configs_from_estimator . items (): if n > len ( estimator_list ): n_configs_per_estimator = [ c . config_dict for c in estimator_list ] else : sort_order = np . argsort ([[ c . get_test_metric ( self . results . hyperpipe_info . best_config_metric , 'mean' ) for c in estimator_list ]]) n_configs_per_estimator = [ estimator_list [ idx ] . config_dict for idx in sort_order [: n ]] best_n_config_dict [ estimator_name ] = n_configs_per_estimator print_config_list_table ( estimator_name , n_configs_per_estimator ) return best_n_config_dict def get_mean_of_best_validation_configs_per_estimator ( self , write_to_file = False ): best_configs_from_estimators = self . _get_best_outer_fold_configs_per_estimator () # get mean values for each metric for each estimator config list estimator_performance_values = dict () for estimator_name , estimator_config_list in best_configs_from_estimators . items (): estimator_performance_values [ estimator_name ] = dict () for metric in self . results . hyperpipe_info . metrics : performance_values = [ c . get_test_metric ( metric , 'mean' ) for c in estimator_config_list ] estimator_performance_values [ estimator_name ][ metric ] = np . mean ( performance_values ) output = print_estimator_metrics ( estimator_performance_values , self . results . hyperpipe_info . metrics , True ) if write_to_file : text_file = open ( os . path . join ( self . output_settings . results_folder , \"mean_best_estimator_performance.txt\" ), \"w\" ) text_file . write ( output ) text_file . close () return output def text_summary ( self ): def divider ( header ): return header . ljust ( 101 , '=' ) output_string = divider ( \"ANALYSIS INFORMATION \" ) elapsed_time = self . results . computation_end_time - self . results . computation_start_time output_string += \"\"\" Project Folder: {} , Computation Time: {} - {} Duration: {} Optimized for: {} Hyperparameter Optimizer: {} \"\"\" . format ( self . output_settings . results_folder , self . results . computation_start_time , self . results . computation_end_time , elapsed_time , self . results . hyperpipe_info . best_config_metric , self . results . hyperpipe_info . optimization [ \"Optimizer\" ]) output_string += divider ( \"DUMMY RESULTS \" ) output_string += \"\"\" {} \"\"\" . format ( print_metrics ( \"DUMMY\" , self . results . dummy_estimator . get_test_metric ( operation = 'mean' ), summary = True )) output_string += divider ( \"AVERAGE PERFORMANCE ACROSS OUTER FOLDS \" ) test_metrics = self . results . get_test_metric_dict () train_metrics = self . results . get_train_metric_dict () output_string += \"\"\" {} \"\"\" . format ( self . print_table_for_performance_overview ( train_metrics , test_metrics )) output_string += divider ( \"BEST HYPERPARAMETER CONFIGURATION \" ) output_string += \"\"\" {} \"\"\" . format ( json . dumps ( self . results . best_config . human_readable_config , indent = 4 , sort_keys = True )) output_string += \"\"\" {} \"\"\" . format ( print_outer_folds ( self . results . hyperpipe_info . metrics , self . results . outer_folds , summary = True )) output_string += divider ( \"PHOTONAI {} \" . format ( __version__ )) if self . output_settings . results_folder is not None : output_string += \" \\n Your results are stored in \" + self . output_settings . results_folder + \" \\n \" output_string += \"Go to https://explorer.photon-ai.com and upload your photon_result_file.json \" \\ \"for convenient result visualization! \\n \" output_string += \"For more info and documentation visit https://www.photon-ai.com\" if self . output_settings . save_output : try : summary_filename = os . path . join ( self . output_settings . results_folder , 'photon_summary.txt' ) text_file = open ( summary_filename , \"w\" ) text_file . write ( output_string ) text_file . close () except OSError as e : logger . error ( \"Could not write summary file\" ) logger . error ( str ( e )) return output_string @staticmethod def print_table_for_performance_overview ( metric_dict_train , metric_dict_test ): x = PrettyTable () x . field_names = [ \"Metric Name\" , \"Training Mean\" , \"Training Std\" , \"Test Mean\" , \"Test Std\" ] for element_key , element_dict in metric_dict_train . items (): x . add_row ([ element_key , element_dict [ \"mean\" ], element_dict [ \"std\" ], metric_dict_test [ element_key ][ \"mean\" ], metric_dict_test [ element_key ][ \"std\" ]]) return x","title":"Documentation for ResultsHandler"},{"location":"api/processing/results_handler/#photonai.processing.results_handler.ResultsHandler.__init__","text":"Initialize the object. Parameters: Name Type Description Default results_object MDBHyperpipe All results are stored here. An initial setting is not necessary, because a later loading via file or MongoDB is possible. None output_settings OutputSettings Setting for creation and storage of the results_object. None Source code in photonai/processing/results_handler.py def __init__ ( self , results_object : MDBHyperpipe = None , output_settings = None ): \"\"\" Initialize the object. Parameters: results_object: All results are stored here. An initial setting is not necessary, because a later loading via file or MongoDB is possible. output_settings (OutputSettings): Setting for creation and storage of the results_object. \"\"\" self . results = results_object self . output_settings = output_settings","title":"__init__()"},{"location":"api/processing/results_handler/#photonai.processing.results_handler.ResultsHandler.get_config_evaluations","text":"Return the test performance of every tested configuration in every outer fold. Returns: Type Description dict Test performance of every configuration. Source code in photonai/processing/results_handler.py def get_config_evaluations ( self ) -> dict : \"\"\" Return the test performance of every tested configuration in every outer fold. Returns: Test performance of every configuration. \"\"\" config_performances = list () maximum_fold = None for outer_fold in self . results . outer_folds : if maximum_fold is None or len ( outer_fold . tested_config_list ) > maximum_fold : maximum_fold = len ( outer_fold . tested_config_list ) for outer_fold in self . results . outer_folds : performance = dict () for metric in self . results . hyperpipe_info . metrics : performance [ metric ] = list () for i in range ( maximum_fold ): # for config in outer_fold.tested_config_list: for metric in self . results . hyperpipe_info . metrics : if i >= len ( outer_fold . tested_config_list ): performance [ metric ] . append ( np . nan ) continue config = outer_fold . tested_config_list [ i ] if config . config_failed : performance [ metric ] . append ( np . nan ) else : for item in config . metrics_test : if ( item . operation == 'mean' ) and ( item . metric_name == metric ): performance [ metric ] . append ( item . value ) config_performances . append ( performance ) config_performances_dict = dict () for metric in self . results . hyperpipe_info . metrics : config_performances_dict [ metric ] = list () for fold in config_performances : config_performances_dict [ metric ] . append ( fold [ metric ]) return config_performances_dict","title":"get_config_evaluations()"},{"location":"api/processing/results_handler/#photonai.processing.results_handler.ResultsHandler.get_methods","text":"This function returns a list of all methods available for ResultsHandler. Returns: Type Description list List of all available methods. Source code in photonai/processing/results_handler.py @staticmethod def get_methods () -> list : \"\"\" This function returns a list of all methods available for ResultsHandler. Returns: List of all available methods. \"\"\" methods_list = [ s for s in dir ( ResultsHandler ) if '__' not in s ] return methods_list","title":"get_methods()"},{"location":"api/processing/results_handler/#photonai.processing.results_handler.ResultsHandler.get_performance_table","text":"This function returns a summary table of the overall results. ToDo: add best_config information! Source code in photonai/processing/results_handler.py def get_performance_table ( self ): \"\"\"This function returns a summary table of the overall results. ToDo: add best_config information! \"\"\" res_tab = pd . DataFrame () for i , folds in enumerate ( self . results . outer_folds ): # add best config infos res_tab . loc [ i , 'best_config' ] = str ( folds . best_config . human_readable_config ) # add fold index res_tab . loc [ i , 'fold' ] = folds . fold_nr # add sample size infos res_tab . loc [ i , 'n_train' ] = folds . best_config . best_config_score . number_samples_training res_tab . loc [ i , 'n_validation' ] = folds . best_config . best_config_score . number_samples_validation # add performance metrics d = folds . best_config . best_config_score . validation . metrics for key , value in d . items (): res_tab . loc [ i , key ] = value # add row with overall info res_tab . loc [ i + 1 , 'n_validation' ] = np . sum ( res_tab [ 'n_validation' ]) for key , value in d . items (): m = res_tab . loc [:, key ] res_tab . loc [ i + 1 , key ] = np . mean ( m ) res_tab . loc [ i + 1 , key + '_sem' ] = sem ( m ) # standard error of the mean res_tab . loc [ i + 1 , 'best_config' ] = 'Overall' return res_tab","title":"get_performance_table()"},{"location":"api/processing/results_handler/#photonai.processing.results_handler.ResultsHandler.load_from_file","text":"Read results_file from json into MDBHyperpipe object self.results. Parameters: Name Type Description Default results_file str Full path to json file. required Source code in photonai/processing/results_handler.py def load_from_file ( self , results_file : str ): \"\"\" Read results_file from json into MDBHyperpipe object self.results. Parameters: results_file: Full path to json file. \"\"\" self . results = MDBHyperpipe . from_document ( json . load ( open ( results_file , 'r' )))","title":"load_from_file()"},{"location":"api/processing/results_handler/#photonai.processing.results_handler.ResultsHandler.load_from_mongodb","text":"Read results_file from MongoDB into MDBHyperpipe object self.results. Parameters: Name Type Description Default mongodb_connect_url str MongoDB connection string. required pipe_name str Name of the stored hyperpipe. required Source code in photonai/processing/results_handler.py def load_from_mongodb ( self , mongodb_connect_url : str , pipe_name : str ): \"\"\" Read results_file from MongoDB into MDBHyperpipe object self.results. Parameters: mongodb_connect_url: MongoDB connection string. pipe_name: Name of the stored hyperpipe. \"\"\" connect ( mongodb_connect_url , alias = \"photon_core\" ) results = list ( MDBHyperpipe . objects . raw ({ 'name' : pipe_name })) if len ( results ) == 1 : self . results = results [ 0 ] elif len ( results ) > 1 : self . results = MDBHyperpipe . objects . order_by ([( \"computation_start_time\" , DESCENDING )]) . raw ({ 'name' : pipe_name }) . first () warn_text = 'Found multiple hyperpipes with that name. Returning most recent one.' logger . warning ( warn_text ) warnings . warn ( warn_text ) else : raise FileNotFoundError ( 'Could not load hyperpipe from MongoDB.' )","title":"load_from_mongodb()"},{"location":"examples/classification/","text":"Classification Classification is one of the central machine learning tasks. With PHOTONAI, classification pipelines can be created and designed easily. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 from sklearn.datasets import load_breast_cancer from sklearn.model_selection import KFold from photonai.base import Hyperpipe , PipelineElement from photonai.optimization import FloatRange , Categorical , IntegerRange my_pipe = Hyperpipe ( 'basic_svm_pipe' , inner_cv = KFold ( n_splits = 5 ), outer_cv = KFold ( n_splits = 3 ), optimizer = 'sk_opt' , optimizer_params = { 'n_configurations' : 15 }, metrics = [ 'accuracy' , 'precision' , 'recall' , 'balanced_accuracy' ], best_config_metric = 'accuracy' , project_folder = './tmp' ) my_pipe . add ( PipelineElement ( 'StandardScaler' )) my_pipe += PipelineElement ( 'PCA' , hyperparameters = { 'n_components' : IntegerRange ( 10 , 30 )}, test_disabled = True ) my_pipe += PipelineElement ( 'SVC' , hyperparameters = { 'kernel' : Categorical ([ 'rbf' , 'linear' ]), 'C' : FloatRange ( 1 , 6 )}, gamma = 'scale' ) X , y = load_breast_cancer ( return_X_y = True ) my_pipe . fit ( X , y )","title":"Simple Classification"},{"location":"examples/compare_estimators/","text":"Comparing Estimators With the specialized switch optimizer the user can allocate the same computational resource to hyperparameter optimize the pipeline for each learning algorithm in a final switch element , respectively. The user chooses a hyperparameter optimization strategy, to be applied to optimize the pipeline for each learning algorithm in a distinct hyperparameter space. Thereby each algorithm is optimized with the pipeline with the same settings, so that comparability between the learning algorithms is given. Another strategy would be to optimize estimator selection within a unified hyperparameter space, e.g. by applying the smac3 optimizer . Within a unified hyperparameter space there is an exploration phase, after which only the most promising algorithms receive further computational time and thus, some learning algorithms receive more computational resources than others. This strategy is capable to auto- matically select the best algorithm, however it is due to the given reasons less suitable for algorithm comparisons. With the last line of code in this example, the user requests a comparative performance metrics table, that shows the mean validation performances for the best configurations found in each outer fold for each estimator, respectively. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 from sklearn.datasets import load_breast_cancer from sklearn.model_selection import KFold from photonai.base import Hyperpipe , PipelineElement , Switch from photonai.optimization import FloatRange , IntegerRange my_pipe = Hyperpipe ( 'hp_switch_optimizer' , inner_cv = KFold ( n_splits = 5 ), outer_cv = KFold ( n_splits = 3 ), optimizer = 'switch' , optimizer_params = { 'name' : 'sk_opt' , 'n_configurations' : 50 }, metrics = [ 'accuracy' , 'precision' , 'recall' , 'balanced_accuracy' ], best_config_metric = 'accuracy' , project_folder = './tmp' , verbosity = 1 ) my_pipe . add ( PipelineElement ( 'StandardScaler' )) my_pipe += PipelineElement ( 'PCA' , hyperparameters = { 'n_components' : IntegerRange ( 10 , 30 )}, test_disabled = True ) # set up two learning algorithms in an ensemble estimator_selection = Switch ( 'estimators' ) estimator_selection += PipelineElement ( 'RandomForestClassifier' , criterion = 'gini' , hyperparameters = { 'min_samples_split' : IntegerRange ( 2 , 4 ), 'max_features' : [ 'auto' , 'sqrt' , 'log2' ], 'bootstrap' : [ True , False ]}) estimator_selection += PipelineElement ( 'SVC' , hyperparameters = { 'C' : FloatRange ( 0.5 , 25 ), 'kernel' : [ 'linear' , 'rbf' ]}) my_pipe += estimator_selection X , y = load_breast_cancer ( return_X_y = True ) my_pipe . fit ( X , y ) my_pipe . results_handler . get_mean_of_best_validation_configs_per_estimator ()","title":"Compare estimators"},{"location":"examples/compare_estimators/#comparing-estimators","text":"With the specialized switch optimizer the user can allocate the same computational resource to hyperparameter optimize the pipeline for each learning algorithm in a final switch element , respectively. The user chooses a hyperparameter optimization strategy, to be applied to optimize the pipeline for each learning algorithm in a distinct hyperparameter space. Thereby each algorithm is optimized with the pipeline with the same settings, so that comparability between the learning algorithms is given. Another strategy would be to optimize estimator selection within a unified hyperparameter space, e.g. by applying the smac3 optimizer . Within a unified hyperparameter space there is an exploration phase, after which only the most promising algorithms receive further computational time and thus, some learning algorithms receive more computational resources than others. This strategy is capable to auto- matically select the best algorithm, however it is due to the given reasons less suitable for algorithm comparisons. With the last line of code in this example, the user requests a comparative performance metrics table, that shows the mean validation performances for the best configurations found in each outer fold for each estimator, respectively. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 from sklearn.datasets import load_breast_cancer from sklearn.model_selection import KFold from photonai.base import Hyperpipe , PipelineElement , Switch from photonai.optimization import FloatRange , IntegerRange my_pipe = Hyperpipe ( 'hp_switch_optimizer' , inner_cv = KFold ( n_splits = 5 ), outer_cv = KFold ( n_splits = 3 ), optimizer = 'switch' , optimizer_params = { 'name' : 'sk_opt' , 'n_configurations' : 50 }, metrics = [ 'accuracy' , 'precision' , 'recall' , 'balanced_accuracy' ], best_config_metric = 'accuracy' , project_folder = './tmp' , verbosity = 1 ) my_pipe . add ( PipelineElement ( 'StandardScaler' )) my_pipe += PipelineElement ( 'PCA' , hyperparameters = { 'n_components' : IntegerRange ( 10 , 30 )}, test_disabled = True ) # set up two learning algorithms in an ensemble estimator_selection = Switch ( 'estimators' ) estimator_selection += PipelineElement ( 'RandomForestClassifier' , criterion = 'gini' , hyperparameters = { 'min_samples_split' : IntegerRange ( 2 , 4 ), 'max_features' : [ 'auto' , 'sqrt' , 'log2' ], 'bootstrap' : [ True , False ]}) estimator_selection += PipelineElement ( 'SVC' , hyperparameters = { 'C' : FloatRange ( 0.5 , 25 ), 'kernel' : [ 'linear' , 'rbf' ]}) my_pipe += estimator_selection X , y = load_breast_cancer ( return_X_y = True ) my_pipe . fit ( X , y ) my_pipe . results_handler . get_mean_of_best_validation_configs_per_estimator ()","title":"Comparing Estimators"},{"location":"examples/confounder_removal/","text":"In some situations, and especially in the life sciences, we are interested in the predictive value of certain features but would therefore like to exclude the contribution of confounding variables. In order to do that, simple linear models can be used to regress out the effect of a confounder from all of the features. However, to ensure the independence of training and test set, this has to be done within the cross-validation framework. Adding a ConfoundRemoval PipelineElement to a PHOTONAI pipeline will ensure exactly that when regressing out confounding effects. The confounder variables can be passed to the Hyperpipe in the .fit() method (see example below). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 from sklearn.datasets import load_breast_cancer from sklearn.model_selection import KFold from photonai.base import Hyperpipe , PipelineElement # WE USE THE BREAST CANCER SET FROM SKLEARN data = load_breast_cancer () y = data . target # now let's assume we want to regress out the effect of mean_radius and mean_texture X = data . data [:, 2 :] mean_radius = data . data [:, 0 ] mean_texture = data . data [:, 1 ] # BUILD HYPERPIPE pipe = Hyperpipe ( 'confounder_removal_example' , optimizer = 'grid_search' , metrics = [ 'accuracy' , 'precision' , 'recall' ], best_config_metric = 'accuracy' , outer_cv = KFold ( n_splits = 5 ), inner_cv = KFold ( n_splits = 3 ), verbosity = 1 , project_folder = './tmp/' ) # # there are two ways of specifying multiple confounders # # first, you can simply pass a dictionary with \"confounder\" as key and a data matrix or list as value # pipe += PipelineElement('ConfounderRemoval', {}, standardize_covariates=True, test_disabled=False) # pipe.fit(X, y, confounder=[mean_radius, mean_texture]) # pipe += PipelineElement('SVC') # second, you can also specify the names of the variables that should be used in the confounder removal step pipe += PipelineElement ( 'ConfounderRemoval' , {}, standardize_covariates = True , test_disabled = True , confounder_names = [ 'mean_radius' , 'mean_texture' ]) pipe += PipelineElement ( 'SVC' ) # those names must be keys in the kwargs dictionary pipe . fit ( X , y , mean_radius = mean_radius , mean_texture = mean_texture )","title":"Remove confounders"},{"location":"examples/dnn_multiclass_prediction/","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 from sklearn.datasets import load_digits from sklearn.model_selection import KFold from photonai.base import Hyperpipe , PipelineElement from photonai.optimization import Categorical # WE USE THE BREAST CANCER SET FROM SKLEARN X , y = load_digits ( n_class = 5 , return_X_y = True ) # DESIGN YOUR PIPELINE my_pipe = Hyperpipe ( 'basic_keras_multiclass_pipe' , optimizer = 'grid_search' , optimizer_params = {}, metrics = [ 'accuracy' ], best_config_metric = 'accuracy' , outer_cv = KFold ( n_splits = 2 ), inner_cv = KFold ( n_splits = 2 ), verbosity = 1 , project_folder = './tmp/' ) # ADD ELEMENTS TO YOUR PIPELINE my_pipe . add ( PipelineElement ( 'StandardScaler' )) # attention: shape of hidden_layer_sizes == shape of activations. If you want to choose a function in every layer, # grid_search eliminates combinations with len(hidden_layer_size) != len(activations). # Check out: hidden_layer_sizes=[25, 10], activations=['tanh', 'relu'] my_pipe += PipelineElement ( 'KerasDnnClassifier' , hyperparameters = { 'hidden_layer_sizes' : Categorical ([[ 20 , 10 , 5 ], [ 10 , 8 , 4 ]]), 'dropout_rate' : Categorical ([ 0.5 , [ 0.5 , 0.5 , 0.5 ]])}, activations = 'relu' , nn_batch_size = 32 , epochs = 50 , multi_class = True , verbosity = 0 ) # NOW TRAIN YOUR PIPELINE my_pipe . fit ( X , y )","title":"Use a DNN with multiclass prediction"},{"location":"examples/group_driven_cv_split/","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 import numpy as np from sklearn.datasets import load_breast_cancer from sklearn.model_selection import GroupKFold , GroupShuffleSplit from photonai.base import Hyperpipe , PipelineElement from photonai.optimization import FloatRange , Categorical # WE USE THE BREAST CANCER SET FROM SKLEARN X , y = load_breast_cancer ( return_X_y = True ) groups = np . random . random_integers ( 0 , 3 , ( len ( y ), )) # DESIGN YOUR PIPELINE my_pipe = Hyperpipe ( 'group_split_pipe' , optimizer = 'grid_search' , metrics = [ 'accuracy' , 'precision' , 'recall' ], best_config_metric = 'accuracy' , outer_cv = GroupKFold ( n_splits = 4 ), inner_cv = GroupShuffleSplit ( n_splits = 10 ), verbosity = 1 , project_folder = './tmp/' ) # ADD ELEMENTS TO YOUR PIPELINE # first normalize all features my_pipe += PipelineElement ( 'StandardScaler' ) # then do feature selection using a PCA, specify which values to try in the hyperparameter search my_pipe += PipelineElement ( 'PCA' , hyperparameters = { 'n_components' : [ 5 , 10 , None ]}, test_disabled = True ) # engage and optimize the good old SVM for Classification my_pipe += PipelineElement ( 'SVC' , hyperparameters = { 'kernel' : Categorical ([ 'rbf' , 'linear' ]), 'C' : FloatRange ( 0.5 , 2 , \"linspace\" , num = 5 )}) # NOW TRAIN YOUR PIPELINE my_pipe . fit ( X , y , groups = groups )","title":"Use site-specific validation"},{"location":"examples/imbalanced_data/","text":"Imbalanced Data Transform We have a simple solution for imbalanced classes in a classification problem. Based on the imbalanced-learn package , you can choose between over-, under- and combinesampling. Have a look at the Developer Website for details about the balancing data algorithms. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 import warnings from sklearn.model_selection import StratifiedKFold , StratifiedShuffleSplit from sklearn.exceptions import UndefinedMetricWarning from imblearn.datasets import fetch_datasets from photonai.base import Hyperpipe , PipelineElement from photonai.optimization import Categorical # Since we test very imbalanced data, we want to ignore some metric based zero-divisions. warnings . filterwarnings ( \"ignore\" , category = UndefinedMetricWarning ) # example of imbalanced dataset dataset = fetch_datasets ()[ 'coil_2000' ] X , y = dataset . data , dataset . target # ratio class 0: 6%, class 1: 94% my_pipe = Hyperpipe ( 'balancing_pipe' , optimizer = 'grid_search' , metrics = [ 'accuracy' , 'precision' , 'recall' , 'f1_score' ], best_config_metric = 'f1_score' , outer_cv = StratifiedKFold ( n_splits = 3 ), inner_cv = StratifiedShuffleSplit ( n_splits = 5 , test_size = 0.2 ), verbosity = 1 , project_folder = './tmp/' ) # ADD ELEMENTS TO YOUR PIPELINE my_pipe += PipelineElement ( 'StandardScaler' ) tested_methods = Categorical ([ 'RandomOverSampler' , 'SMOTEENN' , 'SVMSMOTE' , 'BorderlineSMOTE' , 'SMOTE' ]) # Only SMOTE got a different input parameter. # All other strategies stay with the default setting. # Please do not try to optimize over this parameter (not use config inside the 'hyperparameters'). my_pipe += PipelineElement ( 'ImbalancedDataTransformer' , hyperparameters = { 'method_name' : tested_methods }, config = { \"SMOTE\" : { \"k_neighbors\" : 3 }}, test_disabled = True ) my_pipe += PipelineElement ( \"RandomForestClassifier\" , n_estimators = 200 ) # NOW TRAIN YOUR PIPELINE my_pipe . fit ( X , y ) # Possible values for method_name: # imbalance_type = OVERSAMPLING: # - ADASYN # - BorderlineSMOTE # - KMeansSMOTE # - RandomOverSampler # - SMOTE # - SMOTENC # - SVMSMOTE # # imbalance_type = UNDERSAMPLING: # - ClusterCentroids, # - RandomUnderSampler, # - NearMiss, # - InstanceHardnessThreshold, # - CondensedNearestNeighbour, # - EditedNearestNeighbours, # - RepeatedEditedNearestNeighbours, # - AllKNN, # - NeighbourhoodCleaningRule, # - OneSidedSelection # # imbalance_type = COMBINE: # - SMOTEENN, # - SMOTETomek","title":"Over- /Undersampling"},{"location":"examples/no_outer_cv/","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 import numpy as np from sklearn.datasets import load_boston from sklearn.model_selection import KFold from photonai.base import Hyperpipe , PipelineElement X , y = load_boston ( return_X_y = True ) my_pipe = Hyperpipe ( name = 'single_outer_pipe' , metrics = [ 'mean_absolute_error' , 'mean_squared_error' , 'pearson_correlation' ], best_config_metric = 'mean_absolute_error' , use_test_set = False , inner_cv = KFold ( n_splits = 10 , shuffle = True , random_state = 42 ), verbosity = 0 , project_folder = './tmp/' ) # ADD ELEMENTS TO YOUR PIPELINE my_pipe += PipelineElement ( 'SimpleImputer' , missing_values = np . nan , strategy = 'median' ) my_pipe += PipelineElement ( 'StandardScaler' ) my_pipe += PipelineElement ( 'GaussianProcessRegressor' ) # NOW TRAIN YOUR PIPELINE my_pipe . fit ( X , y ) # find mean and std of all metrics here test_metrics = my_pipe . results . best_config . metrics_test train_metrics = my_pipe . results . best_config . metrics_train","title":"No Hyperparameter Optimization"},{"location":"examples/permutation_importances/","text":"Permutation Importance PHOTONAI conveniently integrates scikit-learns permutation importance function to get the permutation feature importances for the optimum pipe after optimization is finished. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 from sklearn.datasets import load_diabetes from sklearn.model_selection import KFold , train_test_split from photonai.base import Hyperpipe , PipelineElement diabetes = load_diabetes () X_train , X_val , y_train , y_val = train_test_split ( diabetes . data , diabetes . target , random_state = 0 ) my_pipe = Hyperpipe ( 'basic_ridge_pipe' , inner_cv = KFold ( n_splits = 5 ), outer_cv = KFold ( n_splits = 3 ), optimizer = 'grid_search' , metrics = [ 'mean_absolute_error' ], best_config_metric = 'mean_absolute_error' , project_folder = './tmp' ) my_pipe += PipelineElement ( \"StandardScaler\" ) my_pipe += PipelineElement ( 'Ridge' , alpha = 1e-2 ) my_pipe . fit ( X_train , y_train ) r = my_pipe . get_permutation_feature_importances ( n_repeats = 50 , random_state = 0 ) for i in r [ \"mean\" ] . argsort ()[:: - 1 ]: if r [ \"mean\" ][ i ] - 2 * r [ \"std\" ][ i ] > 0 : print ( f \" { diabetes . feature_names [ i ] : <8 } \" f \" { r [ 'mean' ][ i ] : .3f } \" f \" +/- { r [ 'std' ][ i ] : .3f } \" ) # get permutation importances posthoc # reloaded_hyperpipe = Hyperpipe.reload_hyperpipe(\"full_path/to/results_folder/\", X_train, y_train) # post_hoc_perm_importances = Hyperpipe.get_permutation_feature_importances(n_repeats=5, random_state=0)","title":"Permutation Importances"},{"location":"examples/permutation_importances/#permutation-importance","text":"PHOTONAI conveniently integrates scikit-learns permutation importance function to get the permutation feature importances for the optimum pipe after optimization is finished. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 from sklearn.datasets import load_diabetes from sklearn.model_selection import KFold , train_test_split from photonai.base import Hyperpipe , PipelineElement diabetes = load_diabetes () X_train , X_val , y_train , y_val = train_test_split ( diabetes . data , diabetes . target , random_state = 0 ) my_pipe = Hyperpipe ( 'basic_ridge_pipe' , inner_cv = KFold ( n_splits = 5 ), outer_cv = KFold ( n_splits = 3 ), optimizer = 'grid_search' , metrics = [ 'mean_absolute_error' ], best_config_metric = 'mean_absolute_error' , project_folder = './tmp' ) my_pipe += PipelineElement ( \"StandardScaler\" ) my_pipe += PipelineElement ( 'Ridge' , alpha = 1e-2 ) my_pipe . fit ( X_train , y_train ) r = my_pipe . get_permutation_feature_importances ( n_repeats = 50 , random_state = 0 ) for i in r [ \"mean\" ] . argsort ()[:: - 1 ]: if r [ \"mean\" ][ i ] - 2 * r [ \"std\" ][ i ] > 0 : print ( f \" { diabetes . feature_names [ i ] : <8 } \" f \" { r [ 'mean' ][ i ] : .3f } \" f \" +/- { r [ 'std' ][ i ] : .3f } \" ) # get permutation importances posthoc # reloaded_hyperpipe = Hyperpipe.reload_hyperpipe(\"full_path/to/results_folder/\", X_train, y_train) # post_hoc_perm_importances = Hyperpipe.get_permutation_feature_importances(n_repeats=5, random_state=0)","title":"Permutation Importance"},{"location":"examples/permutation_test/","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 import uuid import numpy as np from sklearn.datasets import load_breast_cancer from photonai.processing.permutation_test import PermutationTest def create_hyperpipe (): # this is needed here for the parallelisation from photonai.base import Hyperpipe , PipelineElement , OutputSettings from sklearn.model_selection import GroupKFold from sklearn.model_selection import KFold settings = OutputSettings ( mongodb_connect_url = 'mongodb://localhost:27017/photon_results' ) my_pipe = Hyperpipe ( 'permutation_test_1' , optimizer = 'grid_search' , metrics = [ 'accuracy' , 'precision' , 'recall' ], best_config_metric = 'accuracy' , outer_cv = GroupKFold ( n_splits = 2 ), inner_cv = KFold ( n_splits = 2 ), calculate_metrics_across_folds = True , use_test_set = True , verbosity = 1 , project_folder = './tmp/' , output_settings = settings ) # Add transformer elements my_pipe += PipelineElement ( \"StandardScaler\" , hyperparameters = {}, test_disabled = True , with_mean = True , with_std = True ) my_pipe += PipelineElement ( \"PCA\" , test_disabled = False ) # Add estimator my_pipe += PipelineElement ( \"SVC\" , hyperparameters = { 'kernel' : [ 'linear' , 'rbf' ]}, gamma = 'scale' , max_iter = 1000000 ) return my_pipe X , y = load_breast_cancer ( return_X_y = True ) my_perm_id = str ( uuid . uuid4 ()) groups = np . random . random_integers ( 0 , 3 , ( len ( y ), )) # in case the permutation test for this specific hyperpipe has already been calculated, PHOTON will skip the permutation # runs and load existing results perm_tester = PermutationTest ( create_hyperpipe , n_perms = 2 , n_processes = 1 , random_state = 11 , permutation_id = my_perm_id ) perm_tester . fit ( X , y , groups = groups ) results = PermutationTest . _calculate_results ( my_perm_id , mongodb_path = 'mongodb://localhost:27017/photon_results' ) print ( results . p_values )","title":"Permutation test"},{"location":"examples/regression/","text":"Regression In contrast to a classification task, a regression model is based on continuous target values. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 from sklearn.datasets import load_boston from sklearn.model_selection import KFold from photonai.base import Hyperpipe , PipelineElement from photonai.optimization import IntegerRange , FloatRange my_pipe = Hyperpipe ( 'basic_regression_pipe' , optimizer = 'random_search' , optimizer_params = { 'n_configurations' : 25 }, metrics = [ 'mean_squared_error' , 'mean_absolute_error' , 'explained_variance' ], best_config_metric = 'mean_squared_error' , outer_cv = KFold ( n_splits = 3 , shuffle = True ), inner_cv = KFold ( n_splits = 3 , shuffle = True ), verbosity = 1 , project_folder = './tmp/' ) my_pipe += PipelineElement ( 'SimpleImputer' ) my_pipe += PipelineElement ( 'StandardScaler' ) my_pipe += PipelineElement ( 'LassoFeatureSelection' , hyperparameters = { 'percentile' : [ 0.1 , 0.2 , 0.3 ], 'alpha' : FloatRange ( 0.5 , 5 )}) my_pipe += PipelineElement ( 'RandomForestRegressor' , hyperparameters = { 'n_estimators' : IntegerRange ( 10 , 50 )}) # load data and train X , y = load_boston ( return_X_y = True ) my_pipe . fit ( X , y )","title":"Simple Regression"},{"location":"examples/sample_pairing/","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 from sklearn.datasets import load_breast_cancer from sklearn.model_selection import KFold from photonai.base import Hyperpipe , PipelineElement from photonai.optimization import Categorical # WE USE THE BREAST CANCER SET FROM SKLEARN X , y = load_breast_cancer ( return_X_y = True ) # DESIGN YOUR PIPELINE my_pipe = Hyperpipe ( 'sample_pairing_example_classification' , optimizer = 'grid_search' , metrics = [ 'accuracy' ], best_config_metric = 'accuracy' , outer_cv = KFold ( n_splits = 3 ), inner_cv = KFold ( n_splits = 3 ), verbosity = 1 , project_folder = './tmp' , random_seed = 42123 ) # ADD ELEMENTS TO YOUR PIPELINE my_pipe += PipelineElement ( 'StandardScaler' ) my_pipe += PipelineElement ( 'SamplePairingClassification' , hyperparameters = { 'draw_limit' : [ 500 , 1000 , 10000 ], 'generator' : Categorical ([ 'nearest_pair' ])}, distance_metric = 'euclidean' , test_disabled = True ) my_pipe += PipelineElement ( 'RandomForestClassifier' , hyperparameters = { 'n_estimators' : [ 10 , 100 ]}) # NOW TRAIN YOUR PIPELINE my_pipe . fit ( X , y )","title":"Sample Pairing"},{"location":"examples/scikit_learn_mlp/","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 from sklearn.datasets import load_breast_cancer from sklearn.model_selection import KFold from photonai.base import Hyperpipe , PipelineElement from photonai.optimization import IntegerRange # WE USE THE BREAST CANCER SET FROM SKLEARN X , y = load_breast_cancer ( return_X_y = True ) # DESIGN YOUR PIPELINE my_pipe = Hyperpipe ( 'multi_perceptron_pipe' , optimizer = 'sk_opt' , optimizer_params = { 'n_configurations' : 25 }, metrics = [ 'accuracy' , 'precision' , 'recall' , 'balanced_accuracy' ], best_config_metric = 'accuracy' , outer_cv = KFold ( n_splits = 3 ), inner_cv = KFold ( n_splits = 3 ), verbosity = 1 , project_folder = './tmp/' ) # ADD ELEMENTS TO YOUR PIPELINE my_pipe += PipelineElement ( 'StandardScaler' ) my_pipe += PipelineElement ( 'PhotonMLPClassifier' , hyperparameters = { 'layer_1' : IntegerRange ( 1 , 5 ), 'layer_2' : IntegerRange ( 0 , 5 ), 'layer_3' : IntegerRange ( 0 , 5 )}) # NOW TRAIN YOUR PIPELINE my_pipe . fit ( X , y )","title":"Optimize a MLP"},{"location":"features/additional_data/","text":"Stream and Access Additional Data Numerous use-cases rely on data not contained in the feature matrix at runtime, e.g. when aiming to control for the effect of covariates. In PHOTONAI, additional data can be streamed through the pipeline and is accessible for all pipeline steps while - importantly - being matched to the (nested) cross-validation splits. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 import numpy as np from sklearn.datasets import load_breast_cancer from sklearn.model_selection import KFold from sklearn.base import BaseEstimator , ClassifierMixin from photonai.base import Hyperpipe , PipelineElement class AdditionalDataWrapper ( BaseEstimator , ClassifierMixin ): def __init__ ( self ): self . needs_covariates = True def fit ( self , X , y , ** kwargs ): if \"true_predictions\" in kwargs : print ( \"Found additional data\" ) return self def predict ( self , X , ** kwargs ): y_true = kwargs [ \"true_predictions\" ] assert X . shape [ 0 ] == len ( y_true ) return y_true def save ( self ): return None my_pipe = Hyperpipe ( 'additional_data_pipe' , metrics = [ 'accuracy' , 'precision' , 'recall' , 'balanced_accuracy' ], best_config_metric = 'accuracy' , outer_cv = KFold ( n_splits = 3 ), inner_cv = KFold ( n_splits = 3 ), verbosity = 1 , project_folder = './tmp/' ) my_pipe . add ( PipelineElement ( 'StandardScaler' )) my_pipe += PipelineElement . create ( \"CustomWrapper\" , AdditionalDataWrapper (), hyperparameters = {}) X , y = load_breast_cancer ( return_X_y = True ) my_pipe . fit ( X , y , true_predictions = np . array ( y ))","title":"Stream additional data"},{"location":"features/additional_data/#stream-and-access-additional-data","text":"Numerous use-cases rely on data not contained in the feature matrix at runtime, e.g. when aiming to control for the effect of covariates. In PHOTONAI, additional data can be streamed through the pipeline and is accessible for all pipeline steps while - importantly - being matched to the (nested) cross-validation splits. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 import numpy as np from sklearn.datasets import load_breast_cancer from sklearn.model_selection import KFold from sklearn.base import BaseEstimator , ClassifierMixin from photonai.base import Hyperpipe , PipelineElement class AdditionalDataWrapper ( BaseEstimator , ClassifierMixin ): def __init__ ( self ): self . needs_covariates = True def fit ( self , X , y , ** kwargs ): if \"true_predictions\" in kwargs : print ( \"Found additional data\" ) return self def predict ( self , X , ** kwargs ): y_true = kwargs [ \"true_predictions\" ] assert X . shape [ 0 ] == len ( y_true ) return y_true def save ( self ): return None my_pipe = Hyperpipe ( 'additional_data_pipe' , metrics = [ 'accuracy' , 'precision' , 'recall' , 'balanced_accuracy' ], best_config_metric = 'accuracy' , outer_cv = KFold ( n_splits = 3 ), inner_cv = KFold ( n_splits = 3 ), verbosity = 1 , project_folder = './tmp/' ) my_pipe . add ( PipelineElement ( 'StandardScaler' )) my_pipe += PipelineElement . create ( \"CustomWrapper\" , AdditionalDataWrapper (), hyperparameters = {}) X , y = load_breast_cancer ( return_X_y = True ) my_pipe . fit ( X , y , true_predictions = np . array ( y ))","title":"Stream and Access Additional Data"},{"location":"features/batching/","text":"Batch Processing PHOTONAI offers batch processing of elements. This comes in handy for working memory sensitive tasks. An example is handling large medical data modalities, such as resampling gray matter 3D brain scan niftis. However, be aware that this only makes sense for algorithms that independently transform each item. Batching is easily accessed by adding the batch_size parameter to the PipelineElement . 1 PipelineElement ( \"LabelEncoder\" , batch_size = 10 )","title":"Transform in Batches"},{"location":"features/caching/","text":"Caching PHOTONAI offers a specialized caching that handles partially overlapping hyperparameter configurations for nested cross-validation splits. This is particularly useful for reusing results from expensive computations. More generally, caching is useful whenever re-computation needs more time than loading data. It is easily enabled by adding the cache_folder parameter to the Hyperpipe . 1 2 pipe = Hyperpipe ( \"...\" , cache_folder = \"./cache\" )","title":"Caching"},{"location":"features/callbacks/","text":"Callback Elements PHOTONAI implements pipeline callbacks which allow for live inspection of the data flowing through the pipeline at runtime. Callbacks act as pipeline elements and can be inserted at any point within the pipeline. They must define a function delegate which is called with the exact same data that the next pipeline step will receive. Thereby, a developer may inspect e.g. the shape and values of the feature matrix after a sequence of transformations have been applied. Return values from the delegate functions are ignored, so that after returning from the delegate call, the original data is directly passed to the next processing step. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 from sklearn.datasets import load_boston from sklearn.model_selection import KFold from photonai.base import Hyperpipe , PipelineElement , CallbackElement # DEFINE CALLBACK ELEMENT def my_monitor ( X , y = None , ** kwargs ): print ( X . shape ) # here is a useless statement where you can easily set a breakpoint # and do fancy developer stuff debug = True my_pipe = Hyperpipe ( 'monitoring_pipe' , optimizer = 'grid_search' , metrics = [ 'mean_squared_error' , 'pearson_correlation' ], best_config_metric = 'mean_squared_error' , outer_cv = KFold ( n_splits = 3 ), inner_cv = KFold ( n_splits = 3 ), verbosity = 1 , project_folder = './tmp/' ) my_pipe += PipelineElement ( 'StandardScaler' ) my_pipe += PipelineElement ( 'SamplePairingClassification' , hyperparameters = { 'draw_limit' : [ 500 , 1000 , 10000 ]}, distance_metric = 'euclidean' , generator = 'nearest_pair' , test_disabled = True ) # here we inspect the data after augmentation my_pipe += CallbackElement ( \"monitor\" , my_monitor ) my_pipe += PipelineElement ( 'RandomForestRegressor' , hyperparameters = { 'n_estimators' : [ 10 , 100 ]}) X , y = load_boston ( return_X_y = True ) my_pipe . fit ( X , y )","title":"Inspect the dataflow at runtime"},{"location":"features/custom_metrics/","text":"How to use custom metrics 1) You can give PHOTONAI a tuple consisting of a metric name and a function delegate that takes true and predicted values and returns a custom metric 2) You can also use a (custom or existing) class that inherits from keras.metrics.Metric 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 import numpy as np from sklearn.metrics import f1_score from sklearn.datasets import fetch_openml from sklearn.model_selection import KFold from keras.metrics import Accuracy from photonai.base import Hyperpipe , PipelineElement # you can have a simple delegate def custom_metric ( y_true , y_pred ): def hot_encoding ( targets , nclasses ): \"\"\"Convert indices to one-hot encoded labels.\"\"\" targets = np . array ( targets ) . reshape ( - 1 ) return np . eye ( nclasses )[ targets ] return f1_score ( hot_encoding ( y_true , 3 ), hot_encoding ( y_pred , 3 ), average = 'macro' ) my_pipe = Hyperpipe ( 'custom_metric_project' , optimizer = 'sk_opt' , optimizer_params = { 'n_configurations' : 25 }, # and here is how to register it in photonai metrics = [( 'custom_metric' , custom_metric ), Accuracy , 'accuracy' ], best_config_metric = 'custom_metric' , outer_cv = KFold ( n_splits = 3 ), inner_cv = KFold ( n_splits = 3 ), allow_multidim_targets = True , project_folder = './tmp/' ) my_pipe += PipelineElement ( 'StandardScaler' ) my_pipe += PipelineElement ( 'SVC' , kernel = 'rbf' ) X , y = fetch_openml ( \"cars1\" , return_X_y = True ) my_pipe . fit ( X . values , y . values . astype ( int ))","title":"Custom Metrics"},{"location":"features/custom_metrics/#how-to-use-custom-metrics","text":"1) You can give PHOTONAI a tuple consisting of a metric name and a function delegate that takes true and predicted values and returns a custom metric 2) You can also use a (custom or existing) class that inherits from keras.metrics.Metric 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 import numpy as np from sklearn.metrics import f1_score from sklearn.datasets import fetch_openml from sklearn.model_selection import KFold from keras.metrics import Accuracy from photonai.base import Hyperpipe , PipelineElement # you can have a simple delegate def custom_metric ( y_true , y_pred ): def hot_encoding ( targets , nclasses ): \"\"\"Convert indices to one-hot encoded labels.\"\"\" targets = np . array ( targets ) . reshape ( - 1 ) return np . eye ( nclasses )[ targets ] return f1_score ( hot_encoding ( y_true , 3 ), hot_encoding ( y_pred , 3 ), average = 'macro' ) my_pipe = Hyperpipe ( 'custom_metric_project' , optimizer = 'sk_opt' , optimizer_params = { 'n_configurations' : 25 }, # and here is how to register it in photonai metrics = [( 'custom_metric' , custom_metric ), Accuracy , 'accuracy' ], best_config_metric = 'custom_metric' , outer_cv = KFold ( n_splits = 3 ), inner_cv = KFold ( n_splits = 3 ), allow_multidim_targets = True , project_folder = './tmp/' ) my_pipe += PipelineElement ( 'StandardScaler' ) my_pipe += PipelineElement ( 'SVC' , kernel = 'rbf' ) X , y = fetch_openml ( \"cars1\" , return_X_y = True ) my_pipe . fit ( X . values , y . values . astype ( int ))","title":"How to use custom metrics"},{"location":"features/learning_curves/","text":"Learning Curves 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 from sklearn.datasets import load_breast_cancer from sklearn.model_selection import KFold from photonai.base import Hyperpipe , PipelineElement from photonai.optimization import FloatRange my_pipe = Hyperpipe ( 'basic_forest_pipe' , inner_cv = KFold ( n_splits = 2 ), outer_cv = KFold ( n_splits = 2 ), optimizer = 'sk_opt' , optimizer_params = { 'n_configurations' : 15 }, metrics = [ 'accuracy' , 'precision' , 'recall' , 'balanced_accuracy' ], best_config_metric = 'accuracy' , project_folder = './tmp' , # this is how to make photonai calculate learning curves # output and figures for this can be found in the project folder learning_curves = True , learning_curves_cut = FloatRange ( 0.1 , 1 , step = 0.1 )) my_pipe . add ( PipelineElement ( 'StandardScaler' )) my_pipe += PipelineElement ( 'RandomForestClassifier' ) X , y = load_breast_cancer ( return_X_y = True ) my_pipe . fit ( X , y )","title":"Learning Curves"},{"location":"features/performance_constraints/","text":"Performance Constraints Integrating performance baselines and performance expectations in the hyperparameter optimization process is furthermore helpful to increase the overall speed and efficiency. Further testing of a specific hyperparameter configuration in further inner-cross-validation folds can be skipped if the given configuration performs worse than a given static or dynamic threshold. There are three types of contraints implemented in PHOTONAI: MinimumPerformanceConstraint : the lower bound is the given threshold (e.g. accuracy of at least 0.8) BestPerformanceConstraint : the lower bound (+- margin) is the so far best metric value DummyPerformanceConstraint : the lower bound (+-margin) is the dummy performance of the specific metric The threshold is applied in three strategies: any : Computation is skipped if any of the folds is worse than the threshold first : Computation is skipped if the first fold performs worse than the threshold mean : Computation is skipped if the mean of all folds computed so far is worse than the threshold 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 from sklearn.datasets import load_boston from sklearn.model_selection import KFold from photonai.base import Hyperpipe , PipelineElement , OutputSettings from photonai.optimization import MinimumPerformanceConstraint , DummyPerformanceConstraint , BestPerformanceConstraint , IntegerRange import matplotlib.pyplot as plt my_pipe = Hyperpipe ( name = 'constrained_forest_pipe' , optimizer = 'grid_search' , metrics = [ 'mean_squared_error' , 'mean_absolute_error' , 'pearson_correlation' ], best_config_metric = 'mean_squared_error' , outer_cv = KFold ( n_splits = 3 , shuffle = True ), inner_cv = KFold ( n_splits = 10 ), use_test_set = True , verbosity = 1 , project_folder = './tmp' , # output_settings=OutputSettings(mongodb_connect_url=\"mongodb://localhost:27017/photon_results\", # save_output=True), performance_constraints = [ DummyPerformanceConstraint ( 'mean_absolute_error' ), MinimumPerformanceConstraint ( 'pearson_correlation' , 0.65 , 'any' ), BestPerformanceConstraint ( 'mean_squared_error' , 3 , 'mean' )]) my_pipe += PipelineElement ( 'StandardScaler' ) my_pipe += PipelineElement ( 'RandomForestRegressor' , hyperparameters = { 'n_estimators' : IntegerRange ( 5 , 50 )}) X , y = load_boston ( return_X_y = True ) my_pipe . fit ( X , y ) ## plot Scatter plot train_df = my_pipe . results_handler . get_mean_train_predictions () pred_df = my_pipe . results_handler . get_test_predictions () max_value = int ( max ( max ( pred_df [ 'y_true' ]), max ( pred_df [ 'y_pred' ]), max ( train_df [ 'y_pred' ]))) fig , main_axes = plt . subplots () main_axes . plot ( range ( max_value ), range ( max_value ), color = 'black' ) test_set = main_axes . scatter ( pred_df [ \"y_true\" ], pred_df [ \"y_pred\" ], label = \"Test\" ) train_set = main_axes . scatter ( train_df [ \"y_true\" ], train_df [ \"y_pred\" ], label = \"Training\" ) main_axes . legend ( handles = [ test_set , train_set ], loc = 'lower right' ) main_axes . set_xlabel ( \"y true\" ) main_axes . set_ylabel ( \"y predicted\" ) plt . show ()","title":"Hyperparameter Optimization Shortcuts"},{"location":"features/performance_constraints/#performance-constraints","text":"Integrating performance baselines and performance expectations in the hyperparameter optimization process is furthermore helpful to increase the overall speed and efficiency. Further testing of a specific hyperparameter configuration in further inner-cross-validation folds can be skipped if the given configuration performs worse than a given static or dynamic threshold. There are three types of contraints implemented in PHOTONAI: MinimumPerformanceConstraint : the lower bound is the given threshold (e.g. accuracy of at least 0.8) BestPerformanceConstraint : the lower bound (+- margin) is the so far best metric value DummyPerformanceConstraint : the lower bound (+-margin) is the dummy performance of the specific metric The threshold is applied in three strategies: any : Computation is skipped if any of the folds is worse than the threshold first : Computation is skipped if the first fold performs worse than the threshold mean : Computation is skipped if the mean of all folds computed so far is worse than the threshold 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 from sklearn.datasets import load_boston from sklearn.model_selection import KFold from photonai.base import Hyperpipe , PipelineElement , OutputSettings from photonai.optimization import MinimumPerformanceConstraint , DummyPerformanceConstraint , BestPerformanceConstraint , IntegerRange import matplotlib.pyplot as plt my_pipe = Hyperpipe ( name = 'constrained_forest_pipe' , optimizer = 'grid_search' , metrics = [ 'mean_squared_error' , 'mean_absolute_error' , 'pearson_correlation' ], best_config_metric = 'mean_squared_error' , outer_cv = KFold ( n_splits = 3 , shuffle = True ), inner_cv = KFold ( n_splits = 10 ), use_test_set = True , verbosity = 1 , project_folder = './tmp' , # output_settings=OutputSettings(mongodb_connect_url=\"mongodb://localhost:27017/photon_results\", # save_output=True), performance_constraints = [ DummyPerformanceConstraint ( 'mean_absolute_error' ), MinimumPerformanceConstraint ( 'pearson_correlation' , 0.65 , 'any' ), BestPerformanceConstraint ( 'mean_squared_error' , 3 , 'mean' )]) my_pipe += PipelineElement ( 'StandardScaler' ) my_pipe += PipelineElement ( 'RandomForestRegressor' , hyperparameters = { 'n_estimators' : IntegerRange ( 5 , 50 )}) X , y = load_boston ( return_X_y = True ) my_pipe . fit ( X , y ) ## plot Scatter plot train_df = my_pipe . results_handler . get_mean_train_predictions () pred_df = my_pipe . results_handler . get_test_predictions () max_value = int ( max ( max ( pred_df [ 'y_true' ]), max ( pred_df [ 'y_pred' ]), max ( train_df [ 'y_pred' ]))) fig , main_axes = plt . subplots () main_axes . plot ( range ( max_value ), range ( max_value ), color = 'black' ) test_set = main_axes . scatter ( pred_df [ \"y_true\" ], pred_df [ \"y_pred\" ], label = \"Test\" ) train_set = main_axes . scatter ( train_df [ \"y_true\" ], train_df [ \"y_pred\" ], label = \"Training\" ) main_axes . legend ( handles = [ test_set , train_set ], loc = 'lower right' ) main_axes . set_xlabel ( \"y true\" ) main_axes . set_ylabel ( \"y predicted\" ) plt . show ()","title":"Performance Constraints"},{"location":"features/permutation_test/","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 import uuid import numpy as np from sklearn.datasets import load_breast_cancer from photonai.processing.permutation_test import PermutationTest def create_hyperpipe (): # this is needed here for the parallelisation from photonai.base import Hyperpipe , PipelineElement , OutputSettings from sklearn.model_selection import GroupKFold from sklearn.model_selection import KFold settings = OutputSettings ( mongodb_connect_url = 'mongodb://localhost:27017/photon_results' ) my_pipe = Hyperpipe ( 'permutation_test_1' , optimizer = 'grid_search' , metrics = [ 'accuracy' , 'precision' , 'recall' ], best_config_metric = 'accuracy' , outer_cv = GroupKFold ( n_splits = 2 ), inner_cv = KFold ( n_splits = 2 ), calculate_metrics_across_folds = True , use_test_set = True , verbosity = 1 , project_folder = './tmp/' , output_settings = settings ) # Add transformer elements my_pipe += PipelineElement ( \"StandardScaler\" , hyperparameters = {}, test_disabled = True , with_mean = True , with_std = True ) my_pipe += PipelineElement ( \"PCA\" , test_disabled = False ) # Add estimator my_pipe += PipelineElement ( \"SVC\" , hyperparameters = { 'kernel' : [ 'linear' , 'rbf' ]}, gamma = 'scale' , max_iter = 1000000 ) return my_pipe X , y = load_breast_cancer ( return_X_y = True ) my_perm_id = str ( uuid . uuid4 ()) groups = np . random . random_integers ( 0 , 3 , ( len ( y ), )) # in case the permutation test for this specific hyperpipe has already been calculated, PHOTON will skip the permutation # runs and load existing results perm_tester = PermutationTest ( create_hyperpipe , n_perms = 2 , n_processes = 1 , random_state = 11 , permutation_id = my_perm_id ) perm_tester . fit ( X , y , groups = groups ) results = PermutationTest . _calculate_results ( my_perm_id , mongodb_path = 'mongodb://localhost:27017/photon_results' ) print ( results . p_values )","title":"Permutation test"},{"location":"features/preprocessing/","text":"How and why to apply preprocessing There are transformations that can be applied to the data as a whole BEFORE it is split into different training, validation, and testing subsets. Thereby, computational resources are saved as these operations are not repeated for each of the cross-validation folds. Importantly, this does only make sense for transformations that do not fit a model to the dataset as a group (such as e.g. a PCA) but rather apply transformations on a single- subject level (such as resampling a nifti image). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 from sklearn.model_selection import ShuffleSplit from sklearn.datasets import fetch_openml from photonai.base import Hyperpipe , PipelineElement , Preprocessing from photonai.optimization import FloatRange audiology = fetch_openml ( name = 'audiology' ) X = audiology . data . values y = audiology . target . values my_pipe = Hyperpipe ( 'hot_encoder_pipeline' , inner_cv = ShuffleSplit ( n_splits = 5 , test_size = 0.2 ), outer_cv = ShuffleSplit ( n_splits = 3 , test_size = 0.2 ), optimizer = 'sk_opt' , optimizer_params = { 'n_configurations' : 20 }, metrics = [ 'accuracy' ], best_config_metric = 'accuracy' , allow_multidim_targets = True , project_folder = './tmp' ) pre_proc = Preprocessing () pre_proc += PipelineElement ( 'OneHotEncoder' , sparse = False ) pre_proc += PipelineElement ( 'LabelEncoder' ) my_pipe += pre_proc my_pipe += PipelineElement ( 'PCA' , hyperparameters = { 'n_components' : FloatRange ( 0.2 , 0.7 )}) my_pipe += PipelineElement ( 'SVC' , hyperparameters = { 'C' : FloatRange ( 1 , 150 )}, kernel = 'rbf' ) my_pipe . fit ( X , y )","title":"Preprocessing"},{"location":"features/preprocessing/#how-and-why-to-apply-preprocessing","text":"There are transformations that can be applied to the data as a whole BEFORE it is split into different training, validation, and testing subsets. Thereby, computational resources are saved as these operations are not repeated for each of the cross-validation folds. Importantly, this does only make sense for transformations that do not fit a model to the dataset as a group (such as e.g. a PCA) but rather apply transformations on a single- subject level (such as resampling a nifti image). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 from sklearn.model_selection import ShuffleSplit from sklearn.datasets import fetch_openml from photonai.base import Hyperpipe , PipelineElement , Preprocessing from photonai.optimization import FloatRange audiology = fetch_openml ( name = 'audiology' ) X = audiology . data . values y = audiology . target . values my_pipe = Hyperpipe ( 'hot_encoder_pipeline' , inner_cv = ShuffleSplit ( n_splits = 5 , test_size = 0.2 ), outer_cv = ShuffleSplit ( n_splits = 3 , test_size = 0.2 ), optimizer = 'sk_opt' , optimizer_params = { 'n_configurations' : 20 }, metrics = [ 'accuracy' ], best_config_metric = 'accuracy' , allow_multidim_targets = True , project_folder = './tmp' ) pre_proc = Preprocessing () pre_proc += PipelineElement ( 'OneHotEncoder' , sparse = False ) pre_proc += PipelineElement ( 'LabelEncoder' ) my_pipe += pre_proc my_pipe += PipelineElement ( 'PCA' , hyperparameters = { 'n_components' : FloatRange ( 0.2 , 0.7 )}) my_pipe += PipelineElement ( 'SVC' , hyperparameters = { 'C' : FloatRange ( 1 , 150 )}, kernel = 'rbf' ) my_pipe . fit ( X , y )","title":"How and why to apply preprocessing"},{"location":"features/result_handler/","text":"How to query PHOTONAI's result logging There is a whole bunch of information stored in PHOTONAI's result logging tree. Here we showcase some convenience function to easily retrieve useful result details. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 import os import pandas as pd import numpy as np from sklearn.model_selection import StratifiedShuffleSplit from photonai.base import Hyperpipe , PipelineElement from photonai.optimization import FloatRange from sklearn.datasets import fetch_openml # blood-transfusion-service-center blood_transfusion = fetch_openml ( name = 'blood-transfusion-service-center' ) X = blood_transfusion . data . values y = blood_transfusion . target . values y = ( y == '2' ) . astype ( int ) my_pipe = Hyperpipe ( 'results_example' , optimizer = 'sk_opt' , optimizer_params = { 'n_configurations' : 15 , 'acq_func_kwargs' : { 'kappa' : 1 }}, metrics = [ 'accuracy' , 'f1_score' ], best_config_metric = 'f1_score' , outer_cv = StratifiedShuffleSplit ( n_splits = 3 , test_size = 0.2 ), inner_cv = StratifiedShuffleSplit ( n_splits = 4 , test_size = 0.2 ), verbosity = 0 , project_folder = './tmp' ) # first normalize all features my_pipe += PipelineElement ( 'StandardScaler' ) my_pipe += PipelineElement ( 'SVC' , hyperparameters = { 'C' : FloatRange ( 0.1 , 150 )}, probability = True ) my_pipe . fit ( X , y ) # Either, we continue working with the results directly now handler = my_pipe . results_handler #, or we load them again later. # from photonai.processing import ResultsHandler # handler = ResultsHandler().load_from_file(os.path.join(my_pipe.results.output_folder, \"photon_results_file.json\")) # A table with properties and performance of each outer # fold (and the overall run) is created with the following command. performance_table = handler . get_performance_table () with pd . option_context ( 'display.max_rows' , None , 'display.max_columns' , None ): print ( performance_table ) print ( \" \" ) # We now analyze the optimization influence on the result. config_evals = handler . get_config_evaluations () for i , j in enumerate ( config_evals [ 'f1_score' ]): print ( \"Standard deviation for fold {} : {} .\" . format ( str ( i ), str ( np . std ( j )))) print ( \" \" ) # To get an impression of the results, # it is possible to take a closer look at the test_predictions. best_config_preds = handler . get_test_predictions () y_pred = best_config_preds [ 'y_pred' ] y_pred_probabilities = best_config_preds [ 'probabilities' ] y_true = best_config_preds [ 'y_true' ] # While some elements have been misclassified, # we have a closer look to the elementwise probability. for i in range ( 2 , 6 ): attribute = \"correct\" if y_true [ i ] == y_pred [ i ] else \"incorrect\" print ( \"Test-element {} was {} predicted \" \"with an assignment probability of {} .\" . format ( str ( i ), attribute , str ( y_pred_probabilities [ i ])))","title":"Result Queries"},{"location":"features/result_handler/#how-to-query-photonais-result-logging","text":"There is a whole bunch of information stored in PHOTONAI's result logging tree. Here we showcase some convenience function to easily retrieve useful result details. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 import os import pandas as pd import numpy as np from sklearn.model_selection import StratifiedShuffleSplit from photonai.base import Hyperpipe , PipelineElement from photonai.optimization import FloatRange from sklearn.datasets import fetch_openml # blood-transfusion-service-center blood_transfusion = fetch_openml ( name = 'blood-transfusion-service-center' ) X = blood_transfusion . data . values y = blood_transfusion . target . values y = ( y == '2' ) . astype ( int ) my_pipe = Hyperpipe ( 'results_example' , optimizer = 'sk_opt' , optimizer_params = { 'n_configurations' : 15 , 'acq_func_kwargs' : { 'kappa' : 1 }}, metrics = [ 'accuracy' , 'f1_score' ], best_config_metric = 'f1_score' , outer_cv = StratifiedShuffleSplit ( n_splits = 3 , test_size = 0.2 ), inner_cv = StratifiedShuffleSplit ( n_splits = 4 , test_size = 0.2 ), verbosity = 0 , project_folder = './tmp' ) # first normalize all features my_pipe += PipelineElement ( 'StandardScaler' ) my_pipe += PipelineElement ( 'SVC' , hyperparameters = { 'C' : FloatRange ( 0.1 , 150 )}, probability = True ) my_pipe . fit ( X , y ) # Either, we continue working with the results directly now handler = my_pipe . results_handler #, or we load them again later. # from photonai.processing import ResultsHandler # handler = ResultsHandler().load_from_file(os.path.join(my_pipe.results.output_folder, \"photon_results_file.json\")) # A table with properties and performance of each outer # fold (and the overall run) is created with the following command. performance_table = handler . get_performance_table () with pd . option_context ( 'display.max_rows' , None , 'display.max_columns' , None ): print ( performance_table ) print ( \" \" ) # We now analyze the optimization influence on the result. config_evals = handler . get_config_evaluations () for i , j in enumerate ( config_evals [ 'f1_score' ]): print ( \"Standard deviation for fold {} : {} .\" . format ( str ( i ), str ( np . std ( j )))) print ( \" \" ) # To get an impression of the results, # it is possible to take a closer look at the test_predictions. best_config_preds = handler . get_test_predictions () y_pred = best_config_preds [ 'y_pred' ] y_pred_probabilities = best_config_preds [ 'probabilities' ] y_true = best_config_preds [ 'y_true' ] # While some elements have been misclassified, # we have a closer look to the elementwise probability. for i in range ( 2 , 6 ): attribute = \"correct\" if y_true [ i ] == y_pred [ i ] else \"incorrect\" print ( \"Test-element {} was {} predicted \" \"with an assignment probability of {} .\" . format ( str ( i ), attribute , str ( y_pred_probabilities [ i ])))","title":"How to query PHOTONAI's result logging"},{"location":"features/save_load/","text":"Reload and use a model Every successful trained pipeline saves the best model in the result folder as photon_best_model.photon. To collaborate with other people share your trained model saved in this file. Have a look at the following example script if you want to reload the trained Hyperpipe : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 from photonai.base import Hyperpipe from sklearn.datasets import load_breast_cancer X , _ = load_breast_cancer ( True ) # After optimization is finished, PHOTONAI saves the user's pipeline # fitted with the best hyperparameter configuration found # as \"photon_best_model.photon\" in the project's result folder. # this is done automatically, however the use may do so manually by calling # my_pipe.save_optimum_pipe('/home/photon_user/photon_test/optimum_pipe.photon') my_pipe = Hyperpipe . load_optimum_pipe ( \"full_path/to/photon_best_model.photon\" ) predictions = my_pipe . predict ( X ) # get permutation importances posthoc reloaded_hyperpipe = Hyperpipe . reload_hyperpipe ( \"full_path/to/results_folder/\" , X , y ) post_hoc_perm_importances = Hyperpipe . get_permutation_feature_importances ( n_repeats = 5 , random_state = 0 )","title":"Load and use a .photon model"},{"location":"getting_started/algorithm_index/","text":"Algorithms PHOTONAI offers easy access to established machine learning algorithms. The algorithms can be imported by adding a PipelineElement with a specific name, such as \"SVC\" for importing the SupportVectorClassifier from scikit-learn , as shown in the following examples. You can set all parameters of the imported class as usual: e.g. add gamma='auto' to the PipelineElement to set the support vector machine's gamma parameter to 'auto'. In addition, you can specify each parameter as a hyperparameter and define a value range or value list to find the optimal value, such as 'kernel': ['linear', 'rbf'] . To build a custom pipeline, have a look at PHOTONAIs pre-registered processing- and learning algorithms . You can access algorithms for all purposes from several open-source packages. In addition, PHOTONAI offers several utility classes as well, such as linear statistical feature selection or sample pairing algorithms. In addition you can specify hyperparameters as well as their value range in order to be optimized by the hyperparameter optimization strategy. Currently, PHOTONAI offers Grid-Search , Random Search and two frameworks for bayesian optimization. PCA 1 2 3 4 5 6 from photonai.base import PipelineElement PipelineElement ( 'PCA' , hyperparameters = { 'n_components' : IntegerRange ( 5 , 20 )}, test_disabled = True ) # to test if disabling the PipelineElement improves performance, # simply add the test_disabled=True parameter SVC 1 2 3 4 PipelineElement ( 'SVC' , hyperparameters = { 'kernel' : Categorical ([ 'rbf' , 'poly' ]), 'C' : FloatRange ( 0.5 , 2 )}, gamma = 'auto' ) Keras Neural Net 1 2 3 4 5 6 7 8 PipelineElement ( 'KerasDnnRegressor' , hyperparameters = { 'hidden_layer_sizes' : Categorical ([[ 10 , 8 , 4 ], [ 20 , 5 , 3 ]]), 'dropout_rate' : Categorical ([[ 0.5 , 0.2 , 0.1 ], 0.1 ])}, activations = 'relu' , epochs = 5 , batch_size = 32 )","title":"Access established ml-packages"},{"location":"getting_started/algorithm_index/#algorithms","text":"PHOTONAI offers easy access to established machine learning algorithms. The algorithms can be imported by adding a PipelineElement with a specific name, such as \"SVC\" for importing the SupportVectorClassifier from scikit-learn , as shown in the following examples. You can set all parameters of the imported class as usual: e.g. add gamma='auto' to the PipelineElement to set the support vector machine's gamma parameter to 'auto'. In addition, you can specify each parameter as a hyperparameter and define a value range or value list to find the optimal value, such as 'kernel': ['linear', 'rbf'] . To build a custom pipeline, have a look at PHOTONAIs pre-registered processing- and learning algorithms . You can access algorithms for all purposes from several open-source packages. In addition, PHOTONAI offers several utility classes as well, such as linear statistical feature selection or sample pairing algorithms. In addition you can specify hyperparameters as well as their value range in order to be optimized by the hyperparameter optimization strategy. Currently, PHOTONAI offers Grid-Search , Random Search and two frameworks for bayesian optimization.","title":"Algorithms"},{"location":"getting_started/algorithm_index/#pca","text":"1 2 3 4 5 6 from photonai.base import PipelineElement PipelineElement ( 'PCA' , hyperparameters = { 'n_components' : IntegerRange ( 5 , 20 )}, test_disabled = True ) # to test if disabling the PipelineElement improves performance, # simply add the test_disabled=True parameter","title":"PCA"},{"location":"getting_started/algorithm_index/#svc","text":"1 2 3 4 PipelineElement ( 'SVC' , hyperparameters = { 'kernel' : Categorical ([ 'rbf' , 'poly' ]), 'C' : FloatRange ( 0.5 , 2 )}, gamma = 'auto' )","title":"SVC"},{"location":"getting_started/algorithm_index/#keras-neural-net","text":"1 2 3 4 5 6 7 8 PipelineElement ( 'KerasDnnRegressor' , hyperparameters = { 'hidden_layer_sizes' : Categorical ([[ 10 , 8 , 4 ], [ 20 , 5 , 3 ]]), 'dropout_rate' : Categorical ([[ 0.5 , 0.2 , 0.1 ], 0.1 ])}, activations = 'relu' , epochs = 5 , batch_size = 32 )","title":"Keras Neural Net"},{"location":"getting_started/classification/","text":"Classification Classification is one of the central machine learning tasks. With PHOTONAI, classification pipelines can be created and designed easily. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 from sklearn.datasets import load_breast_cancer from sklearn.model_selection import KFold from photonai.base import Hyperpipe , PipelineElement from photonai.optimization import FloatRange , Categorical , IntegerRange my_pipe = Hyperpipe ( 'basic_svm_pipe' , inner_cv = KFold ( n_splits = 5 ), outer_cv = KFold ( n_splits = 3 ), optimizer = 'sk_opt' , optimizer_params = { 'n_configurations' : 15 }, metrics = [ 'accuracy' , 'precision' , 'recall' , 'balanced_accuracy' ], best_config_metric = 'accuracy' , project_folder = './tmp' ) my_pipe . add ( PipelineElement ( 'StandardScaler' )) my_pipe += PipelineElement ( 'PCA' , hyperparameters = { 'n_components' : IntegerRange ( 10 , 30 )}, test_disabled = True ) my_pipe += PipelineElement ( 'SVC' , hyperparameters = { 'kernel' : Categorical ([ 'rbf' , 'linear' ]), 'C' : FloatRange ( 1 , 6 )}, gamma = 'scale' ) X , y = load_breast_cancer ( return_X_y = True ) my_pipe . fit ( X , y )","title":"Classification"},{"location":"getting_started/custom_algorithm/","text":"Add a custom algorithm In order to integrate a custom algorithm in PHOTONAI, all you need to do is provide a class adhering to the popular scikit-learn object API . In the following we will demonstrate an example to integrate a custom transformer to the Hyperpipe . First, implement your data processing logic like this. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # we use BaseEstimator as to prepare the transformer for hyperparameter optimization # we inherit the get_params and set_params methods from sklearn.base import BaseEstimator class CustomTransformer ( BaseEstimator ): def __init__ ( self , param1 = 0 , param2 = None ): # it is important that you name your params the same in the constructor # stub as well as in your class variables! self . param1 = param1 self . param2 = param2 def fit ( self , data , targets = None , ** kwargs ): \"\"\" Adjust the underlying model or method to the data. Returns ------- IMPORTANT: must return self! \"\"\" return self def transform ( self , data , targets = None , ** kwargs ): \"\"\" Apply the method's logic to the data. \"\"\" return data Afterwards, register your element with the photon registry like this. Custom elements must only be registered once. 1 2 3 4 5 6 7 8 9 10 11 from photonai.base import PhotonRegistry custom_element_root_folder = \"./\" registry = PhotonRegistry ( custom_elements_folder = custom_element_root_folder ) registry . register ( photon_name = 'MyCustomTransformer' , class_str = 'custom_transformer.CustomTransformer' , element_type = 'Transformer' ) # show information about the element registry . info ( \"MyCustomTransformer\" ) Afterwards, you can use your custom element in the pipeline like this. Importantly, the custom_elements_folder must be activated for each use as the folder's content, and therefore the custom class implementation might otherwise not be accessible by the python script. 1 2 3 4 5 6 7 8 9 10 11 from photonai.base import PhotonRegistry , Hyperpipe , PipelineElement custom_element_root_folder = \"./\" registry = PhotonRegistry ( custom_elements_folder = custom_element_root_folder ) # This add the custom algorithm folder to the python path in order to import and instantiate the algorithm registry . activate () # then use it my_pipe = Hyperpipe ( \"...\" ) my_pipe += PipelineElement ( 'MyCustomTransformer' , hyperparameters = { 'param1' : [ 1 , 2 , 3 ]})","title":"Include custom algorithm"},{"location":"getting_started/hpos/","text":"Hyperparameter Optimization PHOTONAI offers easy access to several established hyperparameter optimization strategies. Grid Search An exhaustive searching through a manually specified subset of the hyperparameter space. The grid is defined by a finite list for each hyperparameter. 1 2 pipe = Hyperpipe ( \"...\" , optimizer = 'grid_search' ) Random Grid Search Random sampling of a manually specified subset of the hyperparameter space. The grid is defined by a finite list for each hyperparameter. Then, a specified number of random configurations from this grid is tested 1 2 3 4 pipe = Hyperpipe ( \"...\" , optimizer = 'random_grid_search' , optimizer_params = { 'n_configurations' : 30 , 'limit_in_minutes' : 10 }) Random Search A grid-free selection of configurations based on the hyperparameter space. In the case of numerical parameters, decisions are made only on the basis of the interval limits. The creation of configurations is limited by time or a maximum number of runs. 1 2 3 4 pipe = Hyperpipe ( \"...\" , optimizer = 'random_search' , optimizer_params = { 'n_configurations' : 30 , 'limit_in_minutes' : 20 }) If the optimizer_params contain a time and numerical limit, both limits are considered by aborting if either of the limits is met. The default limit for Random Search is n_configurations=10 . Scikit-Optimize Scikit-Optimize, or skopt, is a simple and efficient library to minimize (very) expensive and noisy black-box functions. It implements several methods for sequential model-based optimization. skopt aims to be accessible and easy to use in many contexts. Scikit-optimize usage and implementation details available here . A detailed parameter documentation here. 1 2 3 4 5 6 7 pipe = Hyperpipe ( \"...\" , optimizer = 'sk_opt' , optimizer_params = { 'n_configurations' : 55 , 'n_initial_points' : 15 , 'initial_point_generator' : \"sobol\" , 'acq_func' : 'LCB' , 'acq_func_kwargs' : { 'kappa' : 1.96 }}) Nevergrad Nevergrad is a gradient-free optimization platform. Thus, this package is suitable for optimizing over the hyperparamter space. As a great advantage, evolutionary algorithms are implemented here in addition to Bayesian techniques. Nevergrad usage and implementation details available here . 1 2 3 4 5 6 import nevergrad as ng # list of all available nevergrad optimizer print ( list ( ng . optimizers . registry . values ())) my_pipe = Hyperpipe ( \"...\" , optimizer = 'nevergrad' , optimizer_params = { 'facade' : 'NGO' , 'n_configurations' : 30 }) Smac SMAC (sequential model-based algorithm configuration) is a versatile tool for optimizing algorithm parameters. The main core consists of Bayesian Optimization in combination with an aggressive racing mechanism to efficiently decide which of two configurations performs better. SMAC usage and implementation details available here . 1 2 3 4 5 6 my_pipe = Hyperpipe ( \"...\" , optimizer = 'smac' , optimizer_params = { \"facade\" : \"SMAC4BO\" , \"wallclock_limit\" : 60.0 * 10 , # seconds \"ta_run_limit\" : 100 } # limit of configurations ) Switch Optimizer This optimizer is special, as it uses the strategies above to optimizes the same dataflow for different learning algorithms in a switch (\"OR\") element at the end of the pipeline. For example you can use bayesian optimization for each learning algorithm and select that each of the algorithms gets 25 configurations to be tested. This is different to a global optimization, in which, after an initial exploration phase, computational resources are dedicated to the best performing learning algorithm only. By equally distributing computational ressources to each learning algorithms, better comparability is achieved in-between the algorithms. This can according to the use case be desirable. 1 2 3 pipe = Hyperpipe ( \"...\" , optimizer = \"switch\" , optimizer_params = { 'name' : 'sk_opt' , 'n_configurations' : 25 })","title":"Hyperparameter Optimization"},{"location":"getting_started/output/","text":"PHOTONAI Output After executing the script a result folder is created. In there you find six files with different information about your pipeline and the results. photon_summary.txt A text file including a summary of the results. best_config_predictions.csv This file saves the test set predictions for the best configuration of each outer fold. photon_result_file.json You can visualize this file with our Explorer . Visualized information: Best Hyperparameter Configuration Performance Fold information Tested Configuration Optimization Progress photon_best_model.photon This file stores the best model. You can share or reload it later. photon_output.log Saves the console output from every fold, including the time, the current testing configurations and the results. hyperpipe_config.json Here is the initial setup for your analysis, so you can recreate it later.","title":"Inspect and visualize results"},{"location":"getting_started/photonai/","text":"Reasons to Use PHOTONAI It allows researchers to build, optimize and evaluate machine learning pipelines with few lines of code. It automates the training, optimization and test workflow according to user-defined parameters. It offers convenient access to established machine learning toolboxes such as sklearn. It facilitates machine learning applications for researchers with little programming experience. Users can select and change hyperparameter optimization strategies with simple keywords. It is built on clean interfaces and therefore fully customizable. It is easily extendable with custom algorithms, e.g. for handling biomedical data modalities. It acts as a unifying framework to help researchers share and reuse code across projects. It offers both simple and parallel pipeline streams for comparing algorithms, combining features and building ensembles. It extends existing pipeline implementations to enable the developer, e.g. to change the dataset (data augmentation within the training and testing cross validation splits at runtime. It enables rapid prototyping in contexts which require iterative evaluation of novel machine learning models. and many others... Class diagram Basic structure The PHOTONAI framework is built to accelerate and simplify the design of machine learning pipelines and automatize the training, testing and hyperparameter optimization process. The most important class is the Hyperpipe , as it is used to parametrize and control both the pipeline and the training and testing workflow. The Pipeline streams data through a sequence of PipelineElements , the latter of which represent either established or custom algorithm implementations ( BaseElement ). PipelineElements can share a position within the data stream via an And-Operation ( Stack ), an Or-Operation ( Switch ) or represent a parallel sub-pipeline ( Branch ).","title":"PHOTONAI Framework"},{"location":"getting_started/photonai/#reasons-to-use-photonai","text":"It allows researchers to build, optimize and evaluate machine learning pipelines with few lines of code. It automates the training, optimization and test workflow according to user-defined parameters. It offers convenient access to established machine learning toolboxes such as sklearn. It facilitates machine learning applications for researchers with little programming experience. Users can select and change hyperparameter optimization strategies with simple keywords. It is built on clean interfaces and therefore fully customizable. It is easily extendable with custom algorithms, e.g. for handling biomedical data modalities. It acts as a unifying framework to help researchers share and reuse code across projects. It offers both simple and parallel pipeline streams for comparing algorithms, combining features and building ensembles. It extends existing pipeline implementations to enable the developer, e.g. to change the dataset (data augmentation within the training and testing cross validation splits at runtime. It enables rapid prototyping in contexts which require iterative evaluation of novel machine learning models. and many others...","title":"Reasons to Use PHOTONAI"},{"location":"getting_started/photonai/#class-diagram","text":"","title":"Class diagram"},{"location":"getting_started/photonai/#basic-structure","text":"The PHOTONAI framework is built to accelerate and simplify the design of machine learning pipelines and automatize the training, testing and hyperparameter optimization process. The most important class is the Hyperpipe , as it is used to parametrize and control both the pipeline and the training and testing workflow. The Pipeline streams data through a sequence of PipelineElements , the latter of which represent either established or custom algorithm implementations ( BaseElement ). PipelineElements can share a position within the data stream via an And-Operation ( Stack ), an Or-Operation ( Switch ) or represent a parallel sub-pipeline ( Branch ).","title":"Basic structure"},{"location":"getting_started/regression/","text":"Regression In contrast to a classification task, a regression model is based on continuous target values. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 from sklearn.datasets import load_boston from sklearn.model_selection import KFold from photonai.base import Hyperpipe , PipelineElement from photonai.optimization import IntegerRange , FloatRange my_pipe = Hyperpipe ( 'basic_regression_pipe' , optimizer = 'random_search' , optimizer_params = { 'n_configurations' : 25 }, metrics = [ 'mean_squared_error' , 'mean_absolute_error' , 'explained_variance' ], best_config_metric = 'mean_squared_error' , outer_cv = KFold ( n_splits = 3 , shuffle = True ), inner_cv = KFold ( n_splits = 3 , shuffle = True ), verbosity = 1 , project_folder = './tmp/' ) my_pipe += PipelineElement ( 'SimpleImputer' ) my_pipe += PipelineElement ( 'StandardScaler' ) my_pipe += PipelineElement ( 'LassoFeatureSelection' , hyperparameters = { 'percentile' : [ 0.1 , 0.2 , 0.3 ], 'alpha' : FloatRange ( 0.5 , 5 )}) my_pipe += PipelineElement ( 'RandomForestRegressor' , hyperparameters = { 'n_estimators' : IntegerRange ( 10 , 50 )}) # load data and train X , y = load_boston ( return_X_y = True ) my_pipe . fit ( X , y )","title":"A first regression example"},{"location":"photon_elements/classifier_ensemble/","text":"An ensemble is a combination of multiple base estimators. For a short introduction to ensemble methods, see Sklearn Ensemble Methods . In PHOTONAI, an estimator ensemble can be easily created by adding any number of estimators to a Stack . Afterwards, simply add a meta estimator that receives the predictions of your stack. This can be any estimator or simply a averaging or voting strategy. In this example, we used the PhotonVotingClassifier to create a final prediction. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 from sklearn.datasets import load_breast_cancer from sklearn.model_selection import StratifiedKFold from photonai.base import Hyperpipe , PipelineElement , Stack my_pipe = Hyperpipe ( name = 'ensemble_pipe' , optimizer = 'random_grid_search' , metrics = [ 'balanced_accuracy' ], best_config_metric = 'balanced_accuracy' , outer_cv = StratifiedKFold ( n_splits = 2 , shuffle = True , random_state = 42 ), inner_cv = StratifiedKFold ( n_splits = 2 , shuffle = True , random_state = 42 ), project_folder = './tmp/' ) my_pipe += PipelineElement ( 'StandardScaler' ) # setup estimator stack est_stack = Stack ( name = 'classifier_stack' ) clf_list = [ 'RandomForestClassifier' , 'LinearSVC' , 'NuSVC' , \"SVC\" , \"MLPClassifier\" , \"KNeighborsClassifier\" , \"Lasso\" , \"PassiveAggressiveClassifier\" , \"LogisticRegression\" , \"Perceptron\" , \"RidgeClassifier\" , \"SGDClassifier\" , \"GaussianProcessClassifier\" , \"AdaBoostClassifier\" , \"BaggingClassifier\" , \"GradientBoostingClassifier\" ] for clf in clf_list : est_stack += PipelineElement ( clf ) my_pipe += est_stack my_pipe += PipelineElement ( 'PhotonVotingClassifier' ) X , y = load_breast_cancer ( return_X_y = True ) my_pipe . fit ( X , y )","title":"Classifier Ensemble"},{"location":"photon_elements/feature_subset_pipelines/","text":"In PHOTONAI, you can create individual data streams very easily. If, for example, you like to apply different preprocessing steps to distinct subsets of your features , you can create multiple branches within your ML pipeline that will hold any kind of preprocessing. Similarly, you could train different classifiers on different feature subsets. To add a branch to your pipeline, you can simply create a PHOTONAI Branch and then add any number of elements to it. If you only add transformer elements to your branch, the transformed data will be passed to the next element after your branch (or stacked in case of a PHOTONAI Stack). If, however, you add a final estimator to your branch, the prediction of this estimator will be passed to the next element. You could now add your created branch to a Hyperpipe, however, creating branches only really makes sense when having multiple ones and adding those to either a Stack or Switch . Otherwise, why create a branch in the first place? Importantly, a branch will always receive all of your features if you don't add a PHOTONAI DataFilter . A DataFilter can be added as first element of a branch to make sure only a specific subset of the features will be passed to the remaining elements of the branch. It only takes a parameter called indices that specifies the data columns that are ultimately passed to the next element. In this example, we create three branches to process three feature subsets of the breast cancer dataset separately. For all three branches, we add an SVC to predict the classification label. This way, PHOTONAI can find the optimal SVC hyperparameter for the three data modalities. All predictions are then stacked and passed to a final Switch that will decide between a Random Forest or another SVC. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 from sklearn.datasets import load_breast_cancer from sklearn.model_selection import KFold from photonai.base import Hyperpipe , PipelineElement , Stack , Branch , Switch , DataFilter from photonai.optimization import FloatRange , IntegerRange # LOAD DATA FROM SKLEARN X , y = load_breast_cancer ( return_X_y = True ) my_pipe = Hyperpipe ( 'data_integration' , optimizer = 'random_grid_search' , optimizer_params = { 'n_configurations' : 20 }, metrics = [ 'accuracy' , 'precision' , 'recall' ], best_config_metric = 'f1_score' , outer_cv = KFold ( n_splits = 3 ), inner_cv = KFold ( n_splits = 3 ), verbosity = 0 , project_folder = './tmp/' ) my_pipe += PipelineElement ( 'SimpleImputer' ) my_pipe += PipelineElement ( 'StandardScaler' , {}, with_mean = True ) # Use only \"mean\" features: [mean_radius, mean_texture, mean_perimeter, mean_area, mean_smoothness, mean_compactness, # mean_concavity, mean_concave_points, mean_symmetry, mean_fractal_dimension mean_branch = Branch ( 'MeanFeature' ) mean_branch += DataFilter ( indices = range ( 0 , 10 )) mean_branch += PipelineElement ( 'SVC' , { 'C' : FloatRange ( 0.1 , 200 )}, kernel = 'linear' ) # Use only \"error\" features error_branch = Branch ( 'ErrorFeature' ) error_branch += DataFilter ( indices = range ( 10 , 20 )) error_branch += PipelineElement ( 'SVC' , { 'C' : FloatRange ( 0.1 , 200 )}, kernel = 'linear' ) # use only \"worst\" features: [worst_radius, worst_texture, ..., worst_fractal_dimension] worst_branch = Branch ( 'WorstFeature' ) worst_branch += DataFilter ( indices = range ( 20 , 30 )) worst_branch += PipelineElement ( 'SVC' , { 'C' : FloatRange ( 0.1 , 200 )}, kernel = 'linear' ) my_pipe += Stack ( 'SourceStack' , [ mean_branch , error_branch , worst_branch ]) my_pipe += Switch ( 'EstimatorSwitch' , [ PipelineElement ( 'RandomForestClassifier' , { 'n_estimators' : IntegerRange ( 2 , 5 )}), PipelineElement ( 'SVC' )]) my_pipe . fit ( X , y )","title":"Feature Subset Pipelines"},{"location":"photon_elements/stack/","text":"Stack You want to do stacking if more than one algorithm shall be applied, which equals to an AND-Operation. The PHOTONAI Stack delivers the data to all of the entailed PipelineElements and the transformations or predictions are afterwards horizontally concatenated. In this way you can preprocess data in different ways and collect the resulting information to create a new feature matrix. Additionally, you can train several learning algorithms with the same data in an ensemble-like fashion and concatenate their predictions to a prediction matrix on which you can apply further processing like voting strategies. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 from sklearn.datasets import load_breast_cancer from sklearn.model_selection import KFold from photonai.base import Hyperpipe , PipelineElement , Stack from photonai.optimization import FloatRange , IntegerRange X , y = load_breast_cancer ( return_X_y = True ) my_pipe = Hyperpipe ( 'basic_stack_pipe' , optimizer = 'sk_opt' , optimizer_params = { 'n_configurations' : 25 }, metrics = [ 'accuracy' , 'precision' , 'recall' ], best_config_metric = 'accuracy' , outer_cv = KFold ( n_splits = 3 ), inner_cv = KFold ( n_splits = 3 ), verbosity = 0 , project_folder = './tmp/' ) my_pipe += PipelineElement ( 'StandardScaler' ) tree = PipelineElement ( 'DecisionTreeClassifier' , hyperparameters = { 'min_samples_split' : IntegerRange ( 2 , 4 )}, criterion = 'gini' ) svc = PipelineElement ( 'LinearSVC' , hyperparameters = { 'C' : FloatRange ( 0.5 , 25 )}) # for a stack that includes estimators you can choose whether predict or predict_proba is called for all estimators # in case only some implement predict_proba, predict is called for the remaining estimators my_pipe += Stack ( 'final_stack' , [ tree , svc ], use_probabilities = True ) my_pipe += PipelineElement ( 'LinearSVC' ) my_pipe . fit ( X , y )","title":"Stack (AND)"},{"location":"photon_elements/stack/#stack","text":"You want to do stacking if more than one algorithm shall be applied, which equals to an AND-Operation. The PHOTONAI Stack delivers the data to all of the entailed PipelineElements and the transformations or predictions are afterwards horizontally concatenated. In this way you can preprocess data in different ways and collect the resulting information to create a new feature matrix. Additionally, you can train several learning algorithms with the same data in an ensemble-like fashion and concatenate their predictions to a prediction matrix on which you can apply further processing like voting strategies. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 from sklearn.datasets import load_breast_cancer from sklearn.model_selection import KFold from photonai.base import Hyperpipe , PipelineElement , Stack from photonai.optimization import FloatRange , IntegerRange X , y = load_breast_cancer ( return_X_y = True ) my_pipe = Hyperpipe ( 'basic_stack_pipe' , optimizer = 'sk_opt' , optimizer_params = { 'n_configurations' : 25 }, metrics = [ 'accuracy' , 'precision' , 'recall' ], best_config_metric = 'accuracy' , outer_cv = KFold ( n_splits = 3 ), inner_cv = KFold ( n_splits = 3 ), verbosity = 0 , project_folder = './tmp/' ) my_pipe += PipelineElement ( 'StandardScaler' ) tree = PipelineElement ( 'DecisionTreeClassifier' , hyperparameters = { 'min_samples_split' : IntegerRange ( 2 , 4 )}, criterion = 'gini' ) svc = PipelineElement ( 'LinearSVC' , hyperparameters = { 'C' : FloatRange ( 0.5 , 25 )}) # for a stack that includes estimators you can choose whether predict or predict_proba is called for all estimators # in case only some implement predict_proba, predict is called for the remaining estimators my_pipe += Stack ( 'final_stack' , [ tree , svc ], use_probabilities = True ) my_pipe += PipelineElement ( 'LinearSVC' ) my_pipe . fit ( X , y )","title":"Stack"},{"location":"photon_elements/subpipelines/","text":"Subpipelines If the user wants to parallelize a complete sequence of transformations, that is not only singular PipelineElements but an ordered number of PipelineElements, the class PHOTONAI Branch offers a way to create parallel subpipelines. The branch in turn, can be used in combination with the AND- and OR- Elements in order to design complex pipeline architectures. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 from sklearn.datasets import load_breast_cancer from sklearn.model_selection import KFold from photonai.base import Hyperpipe , PipelineElement , Stack , Branch from photonai.optimization import IntegerRange , Categorical , FloatRange X , y = load_breast_cancer ( return_X_y = True ) my_pipe = Hyperpipe ( 'basic_stacking' , optimizer = 'grid_search' , metrics = [ 'accuracy' , 'precision' , 'recall' ], best_config_metric = 'f1_score' , outer_cv = KFold ( n_splits = 3 ), inner_cv = KFold ( n_splits = 10 ), verbosity = 1 , project_folder = './tmp/' ) # BRANCH WITH QUANTILTRANSFORMER AND DECISIONTREECLASSIFIER tree_qua_branch = Branch ( 'tree_branch' ) tree_qua_branch += PipelineElement ( 'QuantileTransformer' , n_quantiles = 100 ) tree_qua_branch += PipelineElement ( 'DecisionTreeClassifier' , { 'min_samples_split' : IntegerRange ( 2 , 4 )}, criterion = 'gini' ) # BRANCH WITH MinMaxScaler AND DecisionTreeClassifier svm_mima_branch = Branch ( 'svm_branch' ) svm_mima_branch += PipelineElement ( 'MinMaxScaler' ) svm_mima_branch += PipelineElement ( 'SVC' , { 'kernel' : Categorical ([ 'rbf' , 'linear' ]), 'C' : FloatRange ( 0.01 , 2.0 , num = 10 )}, gamma = 'auto' ) # BRANCH WITH StandardScaler AND KNeighborsClassifier knn_sta_branch = Branch ( 'neighbour_branch' ) knn_sta_branch += PipelineElement ( 'StandardScaler' ) knn_sta_branch += PipelineElement ( 'KNeighborsClassifier' ) # voting = True to mean the result of every branch my_pipe += Stack ( 'final_stack' , [ tree_qua_branch , svm_mima_branch , knn_sta_branch ]) my_pipe += PipelineElement ( 'LogisticRegression' , solver = 'lbfgs' ) my_pipe . fit ( X , y )","title":"Subpipelines"},{"location":"photon_elements/switch/","text":"Switch The PipelineSwitch element acts like an OR-Operator and decides which element performs best. Currently, you can only optimize the PipelineSwitch using Grid Search , Random Grid Search and Smac3 . In this example, we add two different transformer elements and two different estimators, and PHOTONAI will evaluate the best choices including the respective hyperparameters. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 from sklearn.datasets import load_breast_cancer from sklearn.model_selection import KFold from photonai.base import Hyperpipe , PipelineElement , Switch from photonai.optimization import IntegerRange # GET DATA X , y = load_breast_cancer ( return_X_y = True ) # CREATE HYPERPIPE my_pipe = Hyperpipe ( 'basic_switch_pipe' , optimizer = 'random_grid_search' , optimizer_params = { 'n_configurations' : 15 }, metrics = [ 'accuracy' , 'precision' , 'recall' ], best_config_metric = 'accuracy' , outer_cv = KFold ( n_splits = 3 ), inner_cv = KFold ( n_splits = 5 ), verbosity = 1 , project_folder = './tmp/' ) # Transformer Switch my_pipe += Switch ( 'StandardizationSwitch' , [ PipelineElement ( 'StandardScaler' ), PipelineElement ( 'MinMaxScaler' )]) # Estimator Switch svm = PipelineElement ( 'SVC' , hyperparameters = { 'kernel' : [ 'rbf' , 'linear' ]}) tree = PipelineElement ( 'DecisionTreeClassifier' , hyperparameters = { 'min_samples_split' : IntegerRange ( 2 , 5 ), 'min_samples_leaf' : IntegerRange ( 1 , 5 ), 'criterion' : [ 'gini' , 'entropy' ]}) my_pipe += Switch ( 'EstimatorSwitch' , [ svm , tree ]) my_pipe . fit ( X , y )","title":"Switch (OR)"},{"location":"photon_elements/switch/#switch","text":"The PipelineSwitch element acts like an OR-Operator and decides which element performs best. Currently, you can only optimize the PipelineSwitch using Grid Search , Random Grid Search and Smac3 . In this example, we add two different transformer elements and two different estimators, and PHOTONAI will evaluate the best choices including the respective hyperparameters. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 from sklearn.datasets import load_breast_cancer from sklearn.model_selection import KFold from photonai.base import Hyperpipe , PipelineElement , Switch from photonai.optimization import IntegerRange # GET DATA X , y = load_breast_cancer ( return_X_y = True ) # CREATE HYPERPIPE my_pipe = Hyperpipe ( 'basic_switch_pipe' , optimizer = 'random_grid_search' , optimizer_params = { 'n_configurations' : 15 }, metrics = [ 'accuracy' , 'precision' , 'recall' ], best_config_metric = 'accuracy' , outer_cv = KFold ( n_splits = 3 ), inner_cv = KFold ( n_splits = 5 ), verbosity = 1 , project_folder = './tmp/' ) # Transformer Switch my_pipe += Switch ( 'StandardizationSwitch' , [ PipelineElement ( 'StandardScaler' ), PipelineElement ( 'MinMaxScaler' )]) # Estimator Switch svm = PipelineElement ( 'SVC' , hyperparameters = { 'kernel' : [ 'rbf' , 'linear' ]}) tree = PipelineElement ( 'DecisionTreeClassifier' , hyperparameters = { 'min_samples_split' : IntegerRange ( 2 , 5 ), 'min_samples_leaf' : IntegerRange ( 1 , 5 ), 'criterion' : [ 'gini' , 'entropy' ]}) my_pipe += Switch ( 'EstimatorSwitch' , [ svm , tree ]) my_pipe . fit ( X , y )","title":"Switch"}]}